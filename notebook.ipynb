{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzqxHIh4OCdW",
        "colab_type": "text"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBHSznCZxpNB",
        "colab_type": "text"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eQ6O12jxMFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.5.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAYXtIdpx0Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "# from torchvision.models import resnet18\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YFLG4yRVwi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "username = ''\n",
        "password = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iWc_oCotu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download packages from repository\n",
        "\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePsy3BLSiXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: put in model package once finished\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"CIFAR-100 dataset handler.\n",
        "    \n",
        "    Args:\n",
        "        root (string): Root directory of the dataset where directory\n",
        "            cifar-100-python exists.\n",
        "        split (string, optional): If 'train', creates dataset from training\n",
        "            set, otherwise creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in a\n",
        "            PIL image and returns a transformed version.\n",
        "    \"\"\"\n",
        "\n",
        "    base_folder = 'cifar-100-python'\n",
        "\n",
        "    train_filename = 'train'\n",
        "    test_filename = 'test'\n",
        "    meta_filename = 'meta'\n",
        "\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            filename = self.train_filename\n",
        "        else:\n",
        "            filename = self.test_filename\n",
        "        \n",
        "        # @todo: add integrity checks\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            self.data = entry['data']\n",
        "            self.labels = entry['fine_labels']\n",
        "        \n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_filename)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Access an element through its index.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, label) where label is index of the target class.\n",
        "        \"\"\"\n",
        "\n",
        "        image, label = self.data[index], self.labels[index]\n",
        "\n",
        "        image = Image.fromarray(image) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the length of the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def get_class(self, label):\n",
        "        \"\"\"Return the indices of data belonging to the specified label.\"\"\"\n",
        "        return np.where(self.labels==label)[0]\n",
        "\n",
        "    def map_labels(self, label_map):\n",
        "        \"\"\"Change dataset labels with a label map.\n",
        "        \n",
        "        Args:\n",
        "            label_map (dict): dictionary mapping all original CIFAR100 labels\n",
        "                to custom labels.\n",
        "\n",
        "                e.g., {0: custom_label_0, ..., 99: custom_label_99}\n",
        "        \"\"\"\n",
        "\n",
        "        self.label_map = label_map\n",
        "\n",
        "        self.labels = np.vectorize(lambda x: self.label_map[x])(self.labels)\n",
        "\n",
        "        # @todo: also change the order of self.label_names\n",
        "\n",
        "    def class_splits(self, steps=10, random_state=None):\n",
        "        \"\"\"Split the classes in several sets of equal length and return them.\"\"\"\n",
        "\n",
        "        rs = np.random.RandomState(random_state)\n",
        "\n",
        "        idx = np.arange(len(self.label_names))\n",
        "        rs.shuffle(idx)\n",
        "\n",
        "        splits = np.split(idx, steps)\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def train_val_split(self, class_splits, val_size=0.5, random_state=None):\n",
        "        \"\"\"Perform a train and validation split based on given class splits.\n",
        "\n",
        "        Args:\n",
        "            class_splits (list): class split returned by self.class_splits\n",
        "            val_size (int, float or None): size of the validation set.                \n",
        "\n",
        "        Returns:\n",
        "            tuple: (train_indices, val_indices) where each element in the tuple\n",
        "                is a list of lists.\n",
        "\n",
        "                train_indices is a list of len(class_splits) lists. Each inner\n",
        "                list contains the training indices belonging to a class split.\n",
        "                val_indices, analogously, contains the validation indices\n",
        "                belonging to a class split.\n",
        "\n",
        "                e.g., [[0, 1, 2, 3, ..., 99],             <- first class split\n",
        "                       [100, 101, 12, 103, ..., 199],     <- second class split\n",
        "                       [200, 201, 22, 203, ..., 299],\n",
        "                       ...\n",
        "                       [900, 901, 902, 903, ..., 999]]    <- last class split\n",
        "        \"\"\"\n",
        "\n",
        "        train_indices = []\n",
        "        val_indices = []\n",
        "\n",
        "        for i, split in enumerate(class_splits):\n",
        "            train_indices.append([])\n",
        "            val_indices.append([])\n",
        "\n",
        "            for c in split:\n",
        "                # For each class, split the data into train and test\n",
        "                idx = self.get_class(c)\n",
        "                train_idx, val_idx = train_test_split(idx.tolist(), test_size=val_size, random_state=random_state)\n",
        "\n",
        "                train_indices[i].extend(train_idx)\n",
        "                val_indices[i].extend(val_idx)\n",
        "\n",
        "        return train_indices, val_indices\n",
        "\n",
        "    def test_split(self, class_splits):\n",
        "        \"\"\"Perform a train validation split on the dataset.\n",
        "\n",
        "        Args:\n",
        "            class_splits (int): class split returned by self.class_splits\n",
        "            val_size (int, float or None): size of the validation set.                \n",
        "\n",
        "        Returns:\n",
        "            test_indices (list): A list of lists. Analogous to train_test_split.\n",
        "        \"\"\"\n",
        "\n",
        "        test_indices = []\n",
        "\n",
        "        for i, split in enumerate(class_splits):\n",
        "            test_indices.append([])\n",
        "\n",
        "            for c in split:\n",
        "                idx = self.get_class(c)\n",
        "                test_indices[i].extend(idx)\n",
        "\n",
        "        return test_indices\n",
        "\n",
        "    def debug_labels(self):\n",
        "        print(self.labels[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUfYUe_QfVbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Resnet for CIFAR\n",
        "\n",
        "!git clone https://github.com/akamaster/pytorch_resnet_cifar10.git\n",
        "!mv -v 'pytorch_resnet_cifar10' 'Resnet'\n",
        "\n",
        "from Resnet.resnet import resnet32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j12pgffMR6Qv",
        "colab_type": "text"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwE0x8gkSisn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "RANDOM_STATE = 42       # For reproducibility of results\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "VAL_SIZE = 0.5          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 256\n",
        "LR = 1e-3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 5e-5\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method (at least 3 to have a fair benchmark)\n",
        "\n",
        "NUM_EPOCHS = 30         # Total number of training epochs\n",
        "STEP_SIZE = 20          # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1             # Multiplicative factor for learning rate step-down\n",
        "\n",
        "# Logging\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDDdumxRwbdQ",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ka2RwY0VS8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xf 'cifar-100-python.tar.gz'\n",
        "!mv 'cifar-100-python' $DATA_DIR/cifar-100-python\n",
        "!rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJVKOqLjVUhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmQEsuniWjIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dataset\n",
        "\n",
        "# train_dataloader, val_dataloader and test_dataloader have the same structure.\n",
        "# Each one is a list of length NUM_RUNS. Each element of each list has length\n",
        "# CLASS_BATCH_SIZE, and contains the DataLoader instances.\n",
        "# e.g., train_dataloader[i][j] is the DataLoader corresponding to the j-th class\n",
        "# batch on the i-th run\n",
        "train_dataloaders = []\n",
        "val_dataloaders = []\n",
        "test_dataloaders = []\n",
        "\n",
        "# Map original label numbers to ascending order numbers\n",
        "# e.g., [1, 4, 7, 11, 25, ...] to\n",
        "#       {1: 0, 4: 1, 7: 2, 11: 3, 25: 4...}\n",
        "#\n",
        "# label_maps[i]: access the label map of the i-th run\n",
        "label_maps = []\n",
        "\n",
        "for run_i in range(NUM_RUNS): # To have a fair benchmark, we run every method on at least three different random splits.\n",
        "    train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "    test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "    class_splits = train_dataset.class_splits(steps=CLASS_BATCH_SIZE, random_state=RANDOM_STATE+run_i)\n",
        "\n",
        "    label_maps.append({})\n",
        "    for split_i in range(len(class_splits)):\n",
        "        label_maps[run_i].update({class_splits[split_i][i]: j for i, j in zip(range(0, 10), range(split_i*10, (split_i+1)*10))})   \n",
        "\n",
        "    train_dataset.map_labels(label_maps[run_i])\n",
        "    test_dataset.map_labels(label_maps[run_i])\n",
        "\n",
        "    train_indices, val_indices = train_dataset.train_val_split(class_splits, val_size=VAL_SIZE, random_state=RANDOM_STATE+run_i)\n",
        "    test_indices = test_dataset.test_split(class_splits)\n",
        "\n",
        "    train_dataloaders.append([])\n",
        "    val_dataloaders.append([])\n",
        "    test_dataloaders.append([])\n",
        "\n",
        "    for split_i in range(len(class_splits)): \n",
        "        train_subset = Subset(train_dataset, train_indices[split_i])\n",
        "        val_subset = Subset(train_dataset, val_indices[split_i])\n",
        "        test_subset = Subset(test_dataset, test_indices[split_i])\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "        val_dataloaders[run_i].append(DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "        test_dataloaders[run_i].append(DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "\n",
        "# The test set should include all the classes seen in current *and previous* training steps\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in reversed(range(1, len(class_splits))):\n",
        "        test_dataloaders[run_i][split_i] = ConcatDataset([test_dataloaders[run_i][i] for i in range(split_i+1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqOQIuaBso79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function to show an image grid\n",
        "\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNwcf1fpsvm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sanity check: visualize a batch of images\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_dataloaders[0][0])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# show images\n",
        "matplotlib_imshow(img_grid, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEX3IV1VUXOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: put in model package once finished\n",
        "\n",
        "class Manager():\n",
        "\n",
        "    def __init__(self, device, net, criterion, train_dataloader, val_dataloader, test_dataloader):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fc layer\"\"\"\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "        self.net.n_classes += n\n",
        "\n",
        "    def do_batch(self, optimizer, batch, labels):\n",
        "        \"\"\"Runs model for one batch.\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero-ing the gradients\n",
        "        outputs = self.net(batch)\n",
        "\n",
        "        loss = self.criterion(outputs, labels)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects = torch.sum(\n",
        "            preds == labels.data).data.item()  # number corrects\n",
        "\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step()  # update weights based on accumulated gradients\n",
        "\n",
        "        return (loss, running_corrects)\n",
        "\n",
        "    def do_epoch(self, optimizer, scheduler, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\"\"\"\n",
        "\n",
        "        self.net.train()  # Set network in training mode\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        batch_idx = 0\n",
        "        for images, labels in tqdm(self.train_dataloader, desc='Epoch: %d ' % (current_epoch)):\n",
        "            loss, corrects = self.do_batch(optimizer, images, labels)\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average Scores\n",
        "        train_loss = running_train_loss / batch_idx  # average over all batches\n",
        "        train_accuracy = running_corrects / \\\n",
        "            len(self.train_dataloader)  # average over all samples\n",
        "\n",
        "        return (train_loss, train_accuracy)\n",
        "\n",
        "    def validate(self):\n",
        "\n",
        "        self.net.train(False)\n",
        "\n",
        "        running_test_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        batch_idx = 0\n",
        "        for images, labels in self.val_dataloader:\n",
        "\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.net(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            run_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calcuate Scores\n",
        "        val_loss = running_test_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(len(self.val_dataloader))\n",
        "\n",
        "        return (val_loss, val_accuracy)\n",
        "\n",
        "    def train(self, optimizer, scheduler, num_epochs):\n",
        "\n",
        "        self.net.to(self.device)\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        train_loss_history = {}\n",
        "        train_accuracy_history = {}\n",
        "        val_loss_history = {}\n",
        "        val_accuracy_history = {}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            train_loss_history[epoch+1], train_accuracy_history[epoch+1] = self.do_epoch(\n",
        "                optimizer, scheduler, epoch+1)  # Epochs start counting form 1\n",
        "            # Validate at each epoch\n",
        "            val_loss_history[epoch +\n",
        "                             1], val_accuracy_history[epoch+1] = self.validate()\n",
        "\n",
        "        return (train_loss_history, train_accuracy_history, val_loss_history, val_accuracy_history)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        self.net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "        running_corrects = 0\n",
        "        for images, labels in tqdm(self.test_dataloader):\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.net(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        # Calculate Accuracy\n",
        "        accuracy = running_corrects / \\\n",
        "            float(len(self.test_dataloader))  # len test dataloader\n",
        "\n",
        "        print('Test Accuracy: {}'.format(accuracy))\n",
        "\n",
        "        return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwGDklYJa-N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot Train vs Validation loss and Train vs Validation Accuracy\n",
        "\n",
        "def plot_scores(train_loss, validation_loss, train_accuracy, validation_accuracy, save_directory):\n",
        "\n",
        "    # axes[0] = train loss\n",
        "    # axes[1] = train vs validation accuracy\n",
        "    fig, axes = plt.subplots(1, 2, figsize=[15, 5])\n",
        "\n",
        "    axes[0].plot(list(train_loss.keys()), list(train_loss.values()),\n",
        "                 color='#2E84D5', linewidth=2.5, label='Train Loss')\n",
        "    axes[0].plot(list(validation_loss.keys()), list(validation_loss.values()),\n",
        "                 color='#FF9232', linewidth=2.5, label='Validation Loss')\n",
        "    axes[0].set_title(\"Train Loss\")\n",
        "    axes[0].set_xlabel(\"epoch\")\n",
        "    axes[0].set_ylabel(\"loss\")\n",
        "\n",
        "    axes[1].plot(list(train_accuracy.keys()), list(train_accuracy.values()),\n",
        "                 color='#2E84D5', linewidth=2.5, label='Train Accuracy')\n",
        "    axes[1].plot(list(validation_accuracy.keys()), list(validation_accuracy.values()),\n",
        "                 color='#FF9232', linewidth=2.5, label='Validation Accuracy')\n",
        "    axes[1].set_title(\"Val vs Train Accuracy\")\n",
        "    axes[1].set_xlabel(\"epoch\")\n",
        "    axes[1].set_ylabel(\"accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    axes[0].legend()\n",
        "    axes[1].legend()\n",
        "    axes[0].grid(True)\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    fig.savefig(save_directory)\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJqnljCV5gJ5",
        "colab_type": "text"
      },
      "source": [
        "**FINE TUNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzVPf2KxKci3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders, val_dataloaders, test_dataloaders):\n",
        "    train_loss_history.append([])\n",
        "    train_accuracy_history.append([])\n",
        "    val_loss_history.append([])\n",
        "    val_accuracy_history.append([])\n",
        "    test_accuracy_history.append([])\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss\n",
        "\n",
        "    # In this case we optimize over all the parameters of Resnet\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "    for train_split, val_split, test_split in zip(train_dataloader, val_dataloader, test_dataloader):\n",
        "        # Define Manager Object\n",
        "        manager = Manager(DEVICE, net, criterion,\n",
        "                          train_split, val_split, test_split)\n",
        "\n",
        "        scores = manager.train(optimizer, scheduler,\n",
        "                               NUM_EPOCHS)  # train the model\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        train_loss_history[-1].append(scores[0])\n",
        "        train_accuracy_history[-1].append(scores[1])\n",
        "        val_loss_history[-1].append(scores[2])\n",
        "        val_accuracy_history[-1].append(scores[3])\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy = manager.test()\n",
        "\n",
        "        test_accuracy_history[-1].append(test_accuracy)\n",
        "\n",
        "        manager.increment_classes(n=10)  # add 10 nodes to last FC layer"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
