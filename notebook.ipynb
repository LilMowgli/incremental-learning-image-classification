{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.5.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo-vv8dotZEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and import ResNet implementation for CIFAR10/100\n",
        "!git clone https://github.com/akamaster/pytorch_resnet_cifar10.git\n",
        "!mv -v 'pytorch_resnet_cifar10' 'Resnet'\n",
        "\n",
        "from Resnet.resnet import resnet32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "colab": {}
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = ''\n",
        "password = ''\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "colab": {}
      },
      "source": [
        "from data.cifar100 import CIFAR100\n",
        "from utils import plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "RANDOM_STATE = 42       # For reproducibility of results\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 128        # Icarl uses lr = 128. It slow down the Training, that is already enough time consuming for us\n",
        "LR = 0.1                # Icarl sets lr = 2. Since they use BinaryCrossEntropy loss it is feasible,\n",
        "                        # in our case it would diverge\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-5     # From Icarl\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method (at least 3 to have a fair benchmark)\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [30, 45]   # From Icarl\n",
        "GAMMA = 0.1             # From Icarl\n",
        "\n",
        "# Logging\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDDdumxRwbdQ"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Ka2RwY0VS8-",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xf 'cifar-100-python.tar.gz'\n",
        "!mv 'cifar-100-python' $DATA_DIR/cifar-100-python\n",
        "!rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hJVKOqLjVUhp",
        "colab": {}
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OmQEsuniWjIN",
        "colab": {}
      },
      "source": [
        "# train_dataloader, val_dataloader and test_dataloader have the same structure.\n",
        "# Each one is a list of length NUM_RUNS. Each element of each list has length\n",
        "# CLASS_BATCH_SIZE, and contains the DataLoader instances.\n",
        "# e.g., train_dataloader[i][j] is the DataLoader corresponding to the j-th class\n",
        "# batch on the i-th run\n",
        "train_dataloaders = []\n",
        "val_dataloaders = []\n",
        "test_subsets = []\n",
        "\n",
        "# Map original label numbers to ascending order numbers\n",
        "# e.g., [1, 4, 7, 11, 25, ...] to\n",
        "#       {1: 0, 4: 1, 7: 2, 11: 3, 25: 4...}\n",
        "#\n",
        "# label_maps[i]: access the label map of the i-th run\n",
        "label_maps = []\n",
        "\n",
        "for run_i in range(NUM_RUNS): # To have a fair benchmark, we run every method on at least three different random splits.\n",
        "    train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "    test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "    class_splits = train_dataset.class_splits(steps=CLASS_BATCH_SIZE, random_state=RANDOM_STATE+run_i)\n",
        "\n",
        "    label_maps.append({})\n",
        "    for split_i in range(len(class_splits)):\n",
        "        label_maps[run_i].update({class_splits[split_i][i]: j for i, j in zip(range(0, 10), range(split_i*10, (split_i+1)*10))})   \n",
        "\n",
        "    train_dataset.map_labels(label_maps[run_i])\n",
        "    test_dataset.map_labels(label_maps[run_i])\n",
        "\n",
        "    class_splits = [list(range(split_i*10, (split_i+1)*10)) for split_i in range(len(class_splits))]\n",
        "\n",
        "    train_indices, val_indices = train_dataset.train_val_split(class_splits, val_size=VAL_SIZE, random_state=RANDOM_STATE+run_i)\n",
        "    test_indices = test_dataset.test_split(class_splits)\n",
        "\n",
        "    train_dataloaders.append([])\n",
        "    val_dataloaders.append([])\n",
        "    test_subsets.append([])\n",
        "\n",
        "    for split_i in range(len(class_splits)): \n",
        "        train_subset = Subset(train_dataset, train_indices[split_i])\n",
        "        val_subset = Subset(train_dataset, val_indices[split_i])\n",
        "        test_subset = Subset(test_dataset, test_indices[split_i])\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "        val_dataloaders[run_i].append(DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "        test_subsets[run_i].append(test_subset)\n",
        "\n",
        "# The test set should include all the classes seen in current *and previous* training steps\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in reversed(range(0, len(class_splits))):\n",
        "        test_subsets[run_i][split_i] = DataLoader(ConcatDataset([test_subsets[run_i][i] for i in range(split_i+1)]),\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "test_dataloaders = test_subsets # @todo: fix this mess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNwcf1fpsvm_",
        "colab": {}
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = iter(train_dataloaders[0][0])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "plot.image_grid(images, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEX3IV1VUXOy",
        "colab": {}
      },
      "source": [
        "# @todo: put in model package once finished\n",
        "# @todo: function to save best model during validation\n",
        "\n",
        "class Manager():\n",
        "\n",
        "    def __init__(self, device, net, criterion, train_dataloader, val_dataloader, test_dataloader):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fc layer\"\"\"\n",
        "        in_features = self.net.linear.in_features  # size of each input sample\n",
        "        out_features = self.net.linear.out_features  # size of each output sample\n",
        "        weight = self.net.linear.weight.data\n",
        "\n",
        "        self.net.linear = nn.Linear(in_features, out_features+n)\n",
        "        self.net.linear.weight.data[:out_features] = weight\n",
        "\n",
        "    def do_batch(self, optimizer, batch, labels):\n",
        "        \"\"\"Runs model for one batch.\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero-ing the gradients\n",
        "        outputs = self.net(batch)\n",
        "\n",
        "        loss = self.criterion(outputs, labels)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects = torch.sum(\n",
        "            preds == labels.data).data.item()  # number corrects\n",
        "\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step()  # update weights based on accumulated gradients\n",
        "\n",
        "        return (loss, running_corrects)\n",
        "\n",
        "    def do_epoch(self, optimizer, scheduler, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\"\"\"\n",
        "\n",
        "        self.net.train()  # Set network in training mode\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "        for images, labels in tqdm(self.train_dataloader, desc='Epoch: %d ' % (current_epoch)):\n",
        "\n",
        "            loss, corrects = self.do_batch(optimizer, images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average Scores\n",
        "        train_loss = running_train_loss / batch_idx  # average over all batches\n",
        "        train_accuracy = running_corrects / \\\n",
        "            float(total)  # average over all samples\n",
        "\n",
        "        print('\\nTrain Loss {}, Train Accuracy {}'\\\n",
        "              .format(train_loss, train_accuracy))\n",
        "\n",
        "        return (train_loss, train_accuracy)\n",
        "\n",
        "    def validate(self):\n",
        "\n",
        "        self.net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "        for images, labels in self.val_dataloader:\n",
        "\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.net(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calcuate Scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print('\\nValidation Loss {}, Validation Accuracy {}'\\\n",
        "              .format(val_loss, val_accuracy))\n",
        "\n",
        "        return (val_loss, val_accuracy)\n",
        "\n",
        "    def train(self, optimizer, scheduler, num_epochs):\n",
        "\n",
        "        self.net.to(self.device)\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            train_loss, train_accuracy = self.do_epoch(\n",
        "                optimizer, scheduler, epoch+1)  # Epochs start counting form 1\n",
        "        \n",
        "        # Validate after each batch of classes\n",
        "        val_loss, val_accuracy = self.validate()            \n",
        "\n",
        "        return (train_loss, train_accuracy,\n",
        "                val_loss, val_accuracy)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        self.net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in tqdm(self.test_dataloader):\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.net(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "            (all_preds.to(self.device, torch.LongTensor), preds.to(self.device))\n",
        "            ,dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate Accuracy\n",
        "        accuracy = running_corrects / \\\n",
        "            float(total)  \n",
        "\n",
        "        print('Test Accuracy: {}'.format(accuracy))\n",
        "\n",
        "        return (accuracy, all_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJqnljCV5gJ5"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VzVPf2KxKci3",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders,\n",
        "                                                             val_dataloaders, test_dataloaders):\n",
        "    train_loss_history.append({})\n",
        "    train_accuracy_history.append({})\n",
        "    val_loss_history.append({})\n",
        "    val_accuracy_history.append({})\n",
        "    test_accuracy_history.append({})\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss\n",
        "\n",
        "    # In this case we optimize over all the parameters of Resnet\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                               milestones=MILESTONES, gamma=GAMMA)\n",
        "        \n",
        "\n",
        "    i = 0\n",
        "    for train_split, val_split, test_split in zip(train_dataloader,\n",
        "                                                  val_dataloader, test_dataloader):\n",
        "\n",
        "        current_split = \"Split %i\"%(i)\n",
        "        print(current_split)\n",
        "\n",
        "        # Define Manager Object\n",
        "        manager = Manager(DEVICE, net, criterion,\n",
        "                          train_split, val_split, test_split)\n",
        "\n",
        "        scores = manager.train(optimizer, scheduler,\n",
        "                               NUM_EPOCHS)  # train the model\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        train_loss_history[-1][current_split] = scores[0]\n",
        "        train_accuracy_history[-1][current_split] = scores[1]\n",
        "        val_loss_history[-1][current_split] = scores[2]\n",
        "        val_accuracy_history[-1][current_split] = scores[3]\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy, all_preds = manager.test()\n",
        "\n",
        "        test_accuracy_history[-1][current_split] = test_accuracy\n",
        "\n",
        "        manager.increment_classes(n=10)  # add 10 nodes to last FC layer\n",
        "\n",
        "        i+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWdj0wvu996S",
        "colab": {}
      },
      "source": [
        "targets = test_dataset.labels\n",
        "preds = all_preds.to('cpu').numpy()\n",
        "\n",
        "plot.heatmap_cm(test_dataset, all_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ESzk5gF2c_c",
        "colab": {}
      },
      "source": [
        "def mean_std_scores(train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "  '''Note: we decide to lose score informations on epochs, in favor of better comparability\n",
        "  of the scores over the different class splits\n",
        "  '''\n",
        "\n",
        "  keys = train_loss_history[0].keys()\n",
        "\n",
        "  avg_train_loss = {k:[] for k in keys}\n",
        "  avg_train_accuracy = {k:[] for k in keys}\n",
        "  avg_val_loss = {k:[] for k in keys}\n",
        "  avg_val_accuracy = {k:[] for k in keys}\n",
        "  avg_test_accuracy = {k:[] for k in keys}\n",
        "  \n",
        "  train_loss = []\n",
        "  train_accuracy = []\n",
        "  val_loss = []\n",
        "  val_accuracy = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  for key in keys:\n",
        "    for run in range(NUM_RUNS):\n",
        "      avg_train_loss[key].append(train_loss_history[run][key])\n",
        "      avg_train_accuracy[key].append(train_accuracy_history[run][key])\n",
        "      avg_val_loss[key].append(val_loss_history[run][key])\n",
        "      avg_val_accuracy[key].append(val_accuracy_history[run][key])\n",
        "      avg_test_accuracy[key].append(test_accuracy_history[run][key])\n",
        "\n",
        "    train_loss.append([np.array(avg_train_loss[key]).mean(), np.array(avg_train_loss[key]).std()])\n",
        "    train_accuracy.append([np.array(avg_train_accuracy[key]).mean(), np.array(avg_train_accuracy[key]).std()])\n",
        "    val_loss.append([np.array(avg_val_loss[key]).mean(), np.array(avg_val_loss[key]).std()])\n",
        "    val_accuracy.append([np.array(avg_val_accuracy[key]).mean(), np.array(avg_val_accuracy[key]).std()])\n",
        "    test_accuracy.append([np.array(avg_test_accuracy[key]).mean(), np.array(avg_test_accuracy[key]).std()])\n",
        "\n",
        "  train_loss = np.array(train_loss)\n",
        "  train_accuracy = np.array(train_accuracy)\n",
        "  val_loss = np.array(val_loss)\n",
        "  val_accuracy = np.array(val_accuracy)\n",
        "  test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "  return(train_loss, train_accuracy, val_loss, val_accuracy, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dKCztHSw7lHB",
        "colab": {}
      },
      "source": [
        "train_loss, train_accuracy, val_loss, val_accuracy,\\\n",
        "test_accuracy = mean_std_scores(train_loss_history, train_accuracy_history,\n",
        "                                   val_loss_history, val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TAdCtXDEa5d",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss, train_accuracy, val_loss, val_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E1APlRtTpmkK",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlbZp6TJzuLZ",
        "colab": {}
      },
      "source": [
        "# @todo: create utils package for functions\n",
        "\n",
        "import ast\n",
        "\n",
        "def load_json_scores(root):\n",
        "\n",
        "  with open('{root}train_accuracy_history.json') as f:\n",
        "      train_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open('{root}train_loss.json') as f:\n",
        "      train_loss_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open('{root}val_accuracy_history.json') as f:\n",
        "      val_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open('{root}val_loss_history.json') as f:\n",
        "      val_loss_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open('{root}test_accuracy_history.json') as f:\n",
        "      test_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  return(train_loss_history, train_accuracy_history, val_loss_history,\n",
        "         val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdI9oGAN3heI",
        "colab": {}
      },
      "source": [
        "# @todo: create utils package for functions\n",
        "\n",
        "import json\n",
        "\n",
        "def save_json_scores(root, train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "\n",
        "with open(os.path.join(root, 'train_loss_history.json'), 'w') as fout:\n",
        "    json.dump(train_loss_history, fout)\n",
        "\n",
        "with open(os.path.join(root, 'train_accuracy_history.json'), 'w') as fout:\n",
        "    json.dump(train_accuracy_history, fout)\n",
        "\n",
        "with open(os.path.join(root, 'val_loss_history.json'), 'w') as fout:\n",
        "    json.dump(val_loss_history, fout)\n",
        "\n",
        "with open(os.path.join(root, 'val_accuracy_history.json'), 'w') as fout:\n",
        "    json.dump(val_accuracy_history, fout)\n",
        "\n",
        "with open(os.path.join(root, 'test_accuracy_history.json'), 'w') as fout:\n",
        "    json.dump(test_accuracy_history, fout)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}