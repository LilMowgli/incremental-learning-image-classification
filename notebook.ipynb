{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf66f34d57fb4f9692d8284119449282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5641eba324714766b122c68b155fa610",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c98a9b5df4a4334a5b6a87e0bcfa71e",
              "IPY_MODEL_556589a23f254e2992141f8b584a2867"
            ]
          }
        },
        "5641eba324714766b122c68b155fa610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c98a9b5df4a4334a5b6a87e0bcfa71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_328fa6ce7f3b4c0fad6048b60405f468",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1df558b4ad24fcf9ad921c1074b55b0"
          }
        },
        "556589a23f254e2992141f8b584a2867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47d82c1498e74e3a98826f155f20c7ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 52019562.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d188a0d1fdb40769ac5b98ed2356721"
          }
        },
        "328fa6ce7f3b4c0fad6048b60405f468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1df558b4ad24fcf9ad921c1074b55b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47d82c1498e74e3a98826f155f20c7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d188a0d1fdb40769ac5b98ed2356721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b4a7afd98814e8a81d21b38c4dcf4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c71d2fbe5fb54a79b924facaf24eaca7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1cbd6170e1a041dcb9c1ec2967aed9a5",
              "IPY_MODEL_c637ed09611543f495f596a3483691df"
            ]
          }
        },
        "c71d2fbe5fb54a79b924facaf24eaca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cbd6170e1a041dcb9c1ec2967aed9a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1e3ffc483f2c4d4aaf94885d4bb2e986",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5bc53cca36d14d45ad5912fab4bec9f8"
          }
        },
        "c637ed09611543f495f596a3483691df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a90df8adc7644baa441ddb23ab50e21",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 48807261.30it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5561f3b256e64a4db1b091e57b68359a"
          }
        },
        "1e3ffc483f2c4d4aaf94885d4bb2e986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5bc53cca36d14d45ad5912fab4bec9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a90df8adc7644baa441ddb23ab50e21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5561f3b256e64a4db1b091e57b68359a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "outputId": "8d833588-1690-4823-b7ec-0cbecd1018b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = ''\n",
        "password = ''\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'incremental-learning-image-classification'...\n",
            "remote: Enumerating objects: 104, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 335 (delta 49), reused 43 (delta 14), pack-reused 231\u001b[K\n",
            "Receiving objects: 100% (335/335), 2.27 MiB | 17.57 MiB/s, done.\n",
            "Resolving deltas: 100% (162/162), done.\n",
            "renamed 'incremental-learning-image-classification/data' -> './data'\n",
            "renamed 'incremental-learning-image-classification/model' -> './model'\n",
            "renamed 'incremental-learning-image-classification/notebook.ipynb' -> './notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/README.md' -> './README.md'\n",
            "renamed 'incremental-learning-image-classification/report' -> './report'\n",
            "renamed 'incremental-learning-image-classification/utils' -> './utils'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "outputId": "060a96de-8309-4c8e-a88d-99c913352ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from utils import plot"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "RANDOM_STATE = 420      # For reproducibility of results                        \n",
        "                        # Note: different random states give very different\n",
        "                        # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "NUM_BATCHES = 10\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 2                  # Initial learning rate\n",
        "                        # iCaRL sets LR = 2. Since they use BinaryCrossEntropy loss it is feasible,\n",
        "                        # in our case it would diverge as we use CrossEntropy loss.\n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HDDdumxRwbdQ"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "skknIP5Jwspm",
        "colab": {}
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5wcci5vi5PIG",
        "colab": {}
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "\n",
        "    test_subsets = []\n",
        "\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "\n",
        "        # Download dataset only at first instantiation\n",
        "        if(run_i+split_i == 0):\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download,\n",
        "                                 random_state=RANDOM_STATE+run_i, transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False,\n",
        "                                random_state=RANDOM_STATE+run_i, transform=test_transform)\n",
        "\n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i])\n",
        "        test_dataset.set_classes_batch(\n",
        "            [test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(\n",
        "            VAL_SIZE, RANDOM_STATE)\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices),\n",
        "                                                   batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "\n",
        "        val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices),\n",
        "                                                 batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "\n",
        "        # Dataset with all seen class\n",
        "        test_dataloaders[run_i].append(DataLoader(test_dataset,\n",
        "                                                  batch_size=BATCH_SIZE, shuffle=True, num_workers=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNwcf1fpsvm_",
        "colab": {}
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = iter(test_dataloaders[0][5])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "plot.image_grid(images, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJqnljCV5gJ5"
      },
      "source": [
        "## Fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yya_CxODY4sd",
        "colab": {}
      },
      "source": [
        "# @todo try xavier initialization "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JqA6VD_VxTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders,\n",
        "                                                             val_dataloaders, test_dataloaders):\n",
        "  \n",
        "    \n",
        "    train_loss_history.append({})\n",
        "    train_accuracy_history.append({})\n",
        "    val_loss_history.append({})\n",
        "    val_accuracy_history.append({})\n",
        "    test_accuracy_history.append({})\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss()  # Define the loss      \n",
        "    \n",
        "    i = 0\n",
        "    for train_split, val_split, test_split in zip(train_dataloader,\n",
        "                                                  val_dataloader, test_dataloader):\n",
        "      \n",
        "        \n",
        "        current_split = \"Split %i\"%(i)\n",
        "        print(current_split)\n",
        "\n",
        "        parameters_to_optimize = net.parameters()\n",
        "        optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                               milestones=MILESTONES, gamma=GAMMA)\n",
        "\n",
        "        # Define Manager Object\n",
        "        manager = Manager(DEVICE, net, criterion, optimizer, scheduler,\n",
        "                          train_split, val_split, test_split)\n",
        "\n",
        "        scores = manager.train(NUM_EPOCHS)  # train the model\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        train_loss_history[-1][current_split] = scores[0]\n",
        "        train_accuracy_history[-1][current_split] = scores[1]\n",
        "        val_loss_history[-1][current_split] = scores[2]\n",
        "        val_accuracy_history[-1][current_split] = scores[3]\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy, all_preds = manager.test()\n",
        "\n",
        "        test_accuracy_history[-1][current_split] = test_accuracy\n",
        "\n",
        "        # Uncomment if default resnet has 10 node at last FC layer\n",
        "        manager.increment_classes(n=10)  # add 10 nodes to last FC layer\n",
        "\n",
        "        i+=1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JWdj0wvu996S",
        "colab": {}
      },
      "source": [
        "# Confusion matrix over last run test predictions\n",
        "targets = test_dataset.targets\n",
        "preds = all_preds.to('cpu').numpy()\n",
        "\n",
        "plot.heatmap_cm(targets, preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ESzk5gF2c_c",
        "colab": {}
      },
      "source": [
        "def mean_std_scores(train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "  '''\n",
        "      Average the scores of runs different splits\n",
        "  '''\n",
        "  # keys = 'Split i-esim'\n",
        "  keys = train_loss_history[0].keys()\n",
        "\n",
        "  # Containers for average scores\n",
        "  avg_train_loss = {k:[] for k in keys}\n",
        "  avg_train_accuracy = {k:[] for k in keys}\n",
        "  avg_val_loss = {k:[] for k in keys}\n",
        "  avg_val_accuracy = {k:[] for k in keys}\n",
        "  avg_test_accuracy = {k:[] for k in keys}\n",
        "  \n",
        "  train_loss = []\n",
        "  train_accuracy = []\n",
        "  val_loss = []\n",
        "  val_accuracy = []\n",
        "  test_accuracy = []\n",
        "\n",
        "  for key in keys:\n",
        "    for run in range(NUM_RUNS):\n",
        "\n",
        "      # Append all i-th scores (split i-esim) for the different runs\n",
        "      avg_train_loss[key].append(train_loss_history[run][key])\n",
        "      avg_train_accuracy[key].append(train_accuracy_history[run][key])\n",
        "      avg_val_loss[key].append(val_loss_history[run][key])\n",
        "      avg_val_accuracy[key].append(val_accuracy_history[run][key])\n",
        "      avg_test_accuracy[key].append(test_accuracy_history[run][key])\n",
        "\n",
        "    # Define (mean, std) of the i-th score for each split\n",
        "    train_loss.append([np.array(avg_train_loss[key]).mean(), np.array(avg_train_loss[key]).std()])\n",
        "    train_accuracy.append([np.array(avg_train_accuracy[key]).mean(), np.array(avg_train_accuracy[key]).std()])\n",
        "    val_loss.append([np.array(avg_val_loss[key]).mean(), np.array(avg_val_loss[key]).std()])\n",
        "    val_accuracy.append([np.array(avg_val_accuracy[key]).mean(), np.array(avg_val_accuracy[key]).std()])\n",
        "    test_accuracy.append([np.array(avg_test_accuracy[key]).mean(), np.array(avg_test_accuracy[key]).std()])\n",
        "\n",
        "  train_loss = np.array(train_loss)\n",
        "  train_accuracy = np.array(train_accuracy)\n",
        "  val_loss = np.array(val_loss)\n",
        "  val_accuracy = np.array(val_accuracy)\n",
        "  test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "  # Return averaged scores\n",
        "  return(train_loss, train_accuracy, val_loss, val_accuracy, test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dKCztHSw7lHB",
        "colab": {}
      },
      "source": [
        "# Get the average scores\n",
        "train_loss, train_accuracy, val_loss, val_accuracy,\\\n",
        "test_accuracy = mean_std_scores(train_loss_history, train_accuracy_history,\n",
        "                                   val_loss_history, val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7TAdCtXDEa5d",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss, train_accuracy, val_loss, val_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E1APlRtTpmkK",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlbZp6TJzuLZ",
        "colab": {}
      },
      "source": [
        "# @todo: create utils package for functions\n",
        "\n",
        "import ast\n",
        "\n",
        "def load_json_scores(root):\n",
        "\n",
        "  with open(os.path.join(root, 'train_accuracy_history.json')) as f:\n",
        "      train_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'train_loss_history.json')) as f:\n",
        "      train_loss_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'val_accuracy_history.json')) as f:\n",
        "      val_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'val_loss_history.json')) as f:\n",
        "      val_loss_history = ast.literal_eval(f.read())\n",
        "\n",
        "  with open(os.path.join(root, 'test_accuracy_history.json')) as f:\n",
        "      test_accuracy_history = ast.literal_eval(f.read())\n",
        "\n",
        "  return(train_loss_history, train_accuracy_history, val_loss_history,\n",
        "         val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tdI9oGAN3heI",
        "colab": {}
      },
      "source": [
        "# @todo: create utils package for functions\n",
        "import json\n",
        "\n",
        "def save_json_scores(root, train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "\n",
        "    with open(os.path.join(root, 'train_loss_history.json'), 'w') as fout:\n",
        "        json.dump(train_loss_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'train_accuracy_history.json'), 'w') as fout:\n",
        "        json.dump(train_accuracy_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'val_loss_history.json'), 'w') as fout:\n",
        "        json.dump(val_loss_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'val_accuracy_history.json'), 'w') as fout:\n",
        "        json.dump(val_accuracy_history, fout)\n",
        "\n",
        "    with open(os.path.join(root, 'test_accuracy_history.json'), 'w') as fout:\n",
        "        json.dump(test_accuracy_history, fout)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S_ZTJSSRd_Ea",
        "colab": {}
      },
      "source": [
        "save_json_scores('scores', train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QnBlpXBme3L-",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "!zip -r scores.zip scores\n",
        "files.download(\"scores.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJ_Z48QmQ2C",
        "colab_type": "text"
      },
      "source": [
        "## Learning Without Forgetting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERL_PF-cm1N_",
        "colab_type": "text"
      },
      "source": [
        "### Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JHBfXPTXm16d",
        "colab": {}
      },
      "source": [
        "# Training settings for Learning Without Forgetting\n",
        "RANDOM_STATE = 420\n",
        "BATCH_SIZE = 128\n",
        "LR = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHqtSdwzm16h"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "373M_sOAm16i",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "97Gi3Sp8m16k",
        "outputId": "6d87798a-78c7-4411-e99f-401cce48a388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "bf66f34d57fb4f9692d8284119449282",
            "5641eba324714766b122c68b155fa610",
            "0c98a9b5df4a4334a5b6a87e0bcfa71e",
            "556589a23f254e2992141f8b584a2867",
            "328fa6ce7f3b4c0fad6048b60405f468",
            "f1df558b4ad24fcf9ad921c1074b55b0",
            "47d82c1498e74e3a98826f155f20c7ce",
            "0d188a0d1fdb40769ac5b98ed2356721"
          ]
        }
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "\n",
        "  test_subsets = []\n",
        "\n",
        "  for split_i in range(CLASS_BATCH_SIZE):\n",
        "\n",
        "    # Download dataset only at first instantiation\n",
        "    if(run_i+split_i == 0):\n",
        "      download = True\n",
        "    else:\n",
        "      download = False\n",
        "\n",
        "    # Create CIFAR100 dataset\n",
        "    train_dataset = Cifar100(DATA_DIR, train = True, download = download, random_state = RANDOM_STATE+run_i, transform=train_transform)\n",
        "    test_dataset = Cifar100(DATA_DIR, train = False, download = False, random_state = RANDOM_STATE+run_i, transform=test_transform)\n",
        "   \n",
        "    # Subspace of CIFAR100 of 10 classes\n",
        "    train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "    test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "    # Define train and validation indices\n",
        "    train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATE)\n",
        "    \n",
        "    train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices), \n",
        "                               batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "    \n",
        "    val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices), \n",
        "                                batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "    \n",
        "    # Dataset with all seen class\n",
        "    test_dataloaders[run_i].append(DataLoader(test_dataset, \n",
        "                               batch_size=BATCH_SIZE, shuffle=True, num_workers=4))           "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf66f34d57fb4f9692d8284119449282",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ksaz2qZ5m16n",
        "colab": {}
      },
      "source": [
        "# Sanity check: visualize a batch of images\n",
        "dataiter = iter(test_dataloaders[0][5])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "plot.image_grid(images, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw6a_xAumXQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from copy import deepcopy\n",
        "\n",
        "'''BCE formulation:\n",
        " let x = logits, z = labels. The logistic loss is\n",
        "\n",
        "  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
        "'''\n",
        "\n",
        "   \n",
        "CLASS_BATCH_SIZE = 10\n",
        "\n",
        "\n",
        "class LWF():\n",
        "  def __init__(self, device, net, old_net, criterion, optimizer, scheduler,\n",
        "               train_dataloader, val_dataloader, test_dataloader, num_classes=10):\n",
        "    \n",
        "    self.device = device\n",
        "\n",
        "    self.net = net\n",
        "    self.best_net = self.net\n",
        "    self.old_net = old_net # None for first ten classes\n",
        "\n",
        "    self.criterion = BCEWithLogitsLoss() # Classifier criterion \n",
        "    self.optimizer = optimizer\n",
        "    self.scheduler = scheduler\n",
        "\n",
        "    self.train_dataloader = train_dataloader\n",
        "    self.val_dataloader = val_dataloader\n",
        "    self.test_dataloader = test_dataloader\n",
        "\n",
        "    self.num_classes = num_classes # can be incremented ouitside methods in the main, or inside methods\n",
        "    self.order = np.arange(100)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def warm_up():\n",
        "    pass\n",
        "\n",
        "  def increment_classes(self, n=10):\n",
        "    \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "    in_features = self.net.fc.in_features  # size of each input sample\n",
        "    out_features = self.net.fc.out_features  # size of each output sample\n",
        "    weight = self.net.fc.weight.data\n",
        "\n",
        "    self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "    self.net.fc.weight.data[:out_features] = weight\n",
        "\n",
        "  def to_onehot(self, targets): \n",
        "    '''\n",
        "    Args:\n",
        "    targets : dataloader.dataset.targets of the new task images\n",
        "    '''\n",
        "    one_hot_targets = torch.eye(self.num_classes)[targets]\n",
        "\n",
        "    return one_hot_targets.to(self.device)\n",
        "\n",
        "  def do_first_batch(self, batch, labels):\n",
        "\n",
        "    batch = batch.to(self.device)\n",
        "    labels = labels.to(self.device) # new classes labels\n",
        "\n",
        "    # Zero-ing the gradients\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    # One hot encoding of new task labels \n",
        "    one_hot_labels = self.to_onehot(labels) # Size = [128, 10]\n",
        "\n",
        "    # New net forward pass\n",
        "    outputs = self.net(batch)  \n",
        "    \n",
        "    loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Accuracy over NEW IMAGES, not over all images\n",
        "    running_corrects = \\\n",
        "        torch.sum(preds == labels.data).data.item() # PuÃ² essere che debba usare targets e non labels\n",
        "\n",
        "    # Backward pass: computes gradients\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss, running_corrects\n",
        "\n",
        "\n",
        "  def do_batch(self, batch, labels):\n",
        "\n",
        "    batch = batch.to(self.device)\n",
        "    labels = labels.to(self.device) # new classes labels\n",
        "\n",
        "    # Zero-ing the gradients\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    # One hot encoding of new task labels \n",
        "    one_hot_labels = self.to_onehot(labels) # Size = [128, n_classes] will be sliced as [:, :self.num_classes-10]\n",
        "    new_classes = (self.order[range(self.num_classes-10, self.num_classes)]).astype(np.int32)\n",
        "    one_hot_labels = torch.stack([one_hot_labels[:, i] for i in new_classes], axis=1)\n",
        "\n",
        "    # Old net forward pass\n",
        "    old_outputs = self.sigmoid(self.old_net(batch)) # Size = [128, 100]\n",
        "    old_classes = (self.order[range(self.num_classes-10)]).astype(np.int32)\n",
        "    old_outputs = torch.stack([old_outputs[:, i] for i in old_classes], axis =1)\n",
        "    \n",
        "    # Combine new and old class targets\n",
        "    targets = torch.cat((old_outputs, one_hot_labels), 1)\n",
        "\n",
        "    # New net forward pass\n",
        "    outputs = self.net(batch) # Size = [128, 100] comparable with the define targets\n",
        "    out_classes = (self.order[range(self.num_classes)]).astype(np.int32)\n",
        "    outputs = torch.stack([outputs[:, i] for i in out_classes], axis=1)\n",
        "  \n",
        "    \n",
        "    loss = self.criterion(outputs, targets) # BCE Loss with sigmoids over outputs (over targets must be done manually)\n",
        "\n",
        "    # Get predictions\n",
        "    _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Accuracy over NEW IMAGES, not over all images\n",
        "    running_corrects = \\\n",
        "        torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "    # Backward pass: computes gradients\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()\n",
        "\n",
        "    return loss, running_corrects\n",
        "\n",
        "\n",
        "  def do_epoch(self, current_epoch):\n",
        "\n",
        "    self.net.train()\n",
        "\n",
        "    running_train_loss = 0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    batch_idx = 0\n",
        "\n",
        "    print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "    for images, labels in self.train_dataloader:\n",
        "\n",
        "      if self.num_classes == CLASS_BATCH_SIZE:\n",
        "        loss, corrects = self.do_first_batch(images, labels)\n",
        "      else:\n",
        "        loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "      running_train_loss += loss.item()\n",
        "      running_corrects += corrects\n",
        "      total += labels.size(0)\n",
        "      batch_idx += 1\n",
        "\n",
        "    self.scheduler.step()\n",
        "\n",
        "    # Calculate average scores\n",
        "    train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "    train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "    print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "    return (train_loss, train_accuracy)\n",
        "\n",
        "\n",
        "  def train(self, num_epochs):\n",
        "    \"\"\"Train the network for a specified number of epochs, and save\n",
        "    the best performing model on the validation set.\n",
        "    \n",
        "    Args:\n",
        "        num_epochs (int): number of epochs for training the network.\n",
        "    Returns:\n",
        "        train_loss: loss computed on the last epoch\n",
        "        train_accuracy: accuracy computed on the last epoch\n",
        "        val_loss: average loss on the validation set of the last epoch\n",
        "        val_accuracy: accuracy on the validation set of the last epoch\n",
        "    \"\"\"\n",
        "\n",
        "    # @todo: is the return behaviour intended? (scores of the last epoch)\n",
        "\n",
        "    self.net = self.net.to(self.device)\n",
        "    if self.old_net != None:\n",
        "      self.old_net = self.old_net.to(self.device)\n",
        "      self.old_net.train(False)\n",
        "\n",
        "    cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "    self.best_loss = float(\"inf\")\n",
        "    self.best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Run an epoch (start counting form 1)\n",
        "        train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "    \n",
        "        # Validate after each epoch \n",
        "        val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "        # Best validation model\n",
        "        if val_loss < self.best_loss:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_net = deepcopy(self.net)\n",
        "            self.best_epoch = epoch\n",
        "            print(\"Best model updated\")\n",
        "\n",
        "        print(\"\")\n",
        "\n",
        "    return (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy)\n",
        "\n",
        "\n",
        "  def validate(self):\n",
        "    \"\"\"Validate the model.\n",
        "    \n",
        "    Returns:\n",
        "        val_loss: average loss function computed on the network outputs\n",
        "            of the validation set (val_dataloader).\n",
        "        val_accuracy: accuracy computed on the validation set.\n",
        "    \"\"\"\n",
        "\n",
        "    self.net.train(False)\n",
        "\n",
        "    running_val_loss = 0\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "    batch_idx = 0\n",
        "\n",
        "\n",
        "    for batch, labels in self.val_dataloader:\n",
        "      batch = batch.to(self.device)\n",
        "      labels = labels.to(self.device)\n",
        "      total += labels.size(0)\n",
        "\n",
        "      # One hot encoding of new task labels \n",
        "      one_hot_labels = self.to_onehot(labels) # Size = [128, 100] will be sliced as [:, :self.num_classes-10]\n",
        "      new_classes = (self.order[range(self.num_classes-10, self.num_classes)]).astype(np.int32)\n",
        "      one_hot_labels = torch.stack([one_hot_labels[:, i] for i in new_classes], axis=1)\n",
        "\n",
        "      if self.num_classes > 10:\n",
        "        # Old net forward pass\n",
        "        old_outputs = self.sigmoid(self.old_net(batch)) # Size = [128, 100]\n",
        "        old_classes = (self.order[range(self.num_classes-10)]).astype(np.int32)\n",
        "        old_outputs = torch.stack([old_outputs[:, i] for i in old_classes], axis =1)\n",
        "\n",
        "        # Combine new and old class targets\n",
        "        targets = torch.cat((old_outputs, one_hot_labels), 1)\n",
        "\n",
        "      else:\n",
        "        targets = one_hot_labels\n",
        "\n",
        "      # New net forward pass\n",
        "      outputs = self.net(batch) # Size = [128, 100] comparable with the define targets\n",
        "      out_classes = (self.order[range(self.num_classes)]).astype(np.int32)\n",
        "      outputs = torch.stack([outputs[:, i] for i in out_classes], axis=1)\n",
        "\n",
        "      \n",
        "      loss = self.criterion(outputs, targets) # BCE Loss with sigmoids over outputs (over targets must be done manually)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update the number of correctly classified validation samples\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "      running_val_loss += loss.item()\n",
        "\n",
        "      batch_idx += 1\n",
        "\n",
        "    # Calcuate scores\n",
        "    val_loss = running_val_loss / batch_idx\n",
        "    val_accuracy = running_corrects / float(total)\n",
        "\n",
        "    print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "    return (val_loss, val_accuracy)\n",
        "\n",
        "\n",
        "  def test(self):\n",
        "    \"\"\"Test the model.\n",
        "    Returns:\n",
        "        accuracy (float): accuracy of the model on the test set\n",
        "    \"\"\"\n",
        "\n",
        "    self.best_net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "    running_corrects = 0\n",
        "    total = 0\n",
        "\n",
        "    all_preds = torch.tensor([]) # to store all predictions\n",
        "    all_preds = all_preds.type(torch.LongTensor)\n",
        "    \n",
        "    for images, labels in self.test_dataloader:\n",
        "      images = images.to(self.device)\n",
        "      labels = labels.to(self.device)\n",
        "      total += labels.size(0)\n",
        "\n",
        "      # Forward Pass\n",
        "      outputs = self.best_net(images)\n",
        "\n",
        "      # Get predictions\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "      # Update Corrects\n",
        "      running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "      # Append batch predictions\n",
        "      all_preds = torch.cat(\n",
        "          (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "      )\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = running_corrects / float(total)  \n",
        "\n",
        "    print(f\"Test accuracy: {accuracy}\")\n",
        "\n",
        "    return (accuracy, all_preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlThDLCvXJwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders,\n",
        "                                                             val_dataloaders, test_dataloaders):\n",
        "  \n",
        "    \n",
        "    train_loss_history.append({})\n",
        "    train_accuracy_history.append({})\n",
        "    val_loss_history.append({})\n",
        "    val_accuracy_history.append({})\n",
        "    test_accuracy_history.append({})\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    \n",
        "    criterion = nn.BCEWithLogitsLoss()  # Define the loss\n",
        "        \n",
        "    \n",
        "    i = 0\n",
        "    for train_split, val_split, test_split in zip(train_dataloader,\n",
        "                                                  val_dataloader, test_dataloader):\n",
        "      \n",
        "      # Redefine optimizer at each split (pass by reference issue)\n",
        "      parameters_to_optimize = net.parameters()\n",
        "      optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                            momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "      scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                                milestones=MILESTONES, gamma=GAMMA)\n",
        "        \n",
        "      current_split = \"Split %i\"%(i)\n",
        "      print(current_split)\n",
        "\n",
        "      num_classes = CLASS_BATCH_SIZE*(i+1)\n",
        "\n",
        "      if num_classes == CLASS_BATCH_SIZE:\n",
        "        # Old Network = None\n",
        "        lwf = LWF(DEVICE, net, None, criterion, optimizer, scheduler,\n",
        "                          train_split, val_split, test_split, num_classes)\n",
        "      else:\n",
        "        lwf = LWF(DEVICE, net, old_net, criterion, optimizer, scheduler,\n",
        "                        train_split, val_split, test_split, num_classes)\n",
        "        \n",
        "\n",
        "      scores = lwf.train(NUM_EPOCHS)  # train the model\n",
        "\n",
        "      # score[i] = dictionary with key:epoch, value: score\n",
        "      train_loss_history[-1][current_split] = scores[0]\n",
        "      train_accuracy_history[-1][current_split] = scores[1]\n",
        "      val_loss_history[-1][current_split] = scores[2]\n",
        "      val_accuracy_history[-1][current_split] = scores[3]\n",
        "\n",
        "      # Test the model on classes seen until now\n",
        "      test_accuracy, all_preds = lwf.test()\n",
        "\n",
        "      test_accuracy_history[-1][current_split] = test_accuracy\n",
        "\n",
        "      # Uncomment if default resnet has 10 node at last FC layer\n",
        "      old_net = deepcopy(lwf.net)\n",
        "      lwf.increment_classes()\n",
        "\n",
        "      i =i+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B6CcDcDlMf_",
        "colab_type": "text"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4ZuxPA0lPPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: move to package once finished\n",
        "# @todo: provide an uncommented .py for easier editing of class for ablation studies\n",
        "\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "\n",
        "class iCaRL:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "\n",
        "    Args:\n",
        "        ...\n",
        "    \"\"\"\n",
        "\n",
        "    # Maximum number of exemplars\n",
        "    memory_size = 2000\n",
        "\n",
        "    # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "    # with num_classes the number of classes seen until now by the network.\n",
        "    exemplars = []\n",
        "\n",
        "    # Initialize the copy of the old network, used to compute outputs of the\n",
        "    # previous network for the distillation loss, to None. This is useful to\n",
        "    # correctly apply the first function when training the network for the first\n",
        "    # time.\n",
        "    old_net = None\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss() # @todo: should this be reduction='sum'?\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "\n",
        "    def classify(self, image):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            image: image to classify (three channels, RGB)\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        label = ...\n",
        "        \n",
        "        return label\n",
        "\n",
        "    def incremental_train(self, split, train_dataloader, val_dataloader):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            ...\n",
        "        Returns:\n",
        "            ...\n",
        "        \"\"\"\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        self.update_representation(train_dataloader, val_dataloader)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        # @todo\n",
        "\n",
        "    def update_representation(self, train_dataloader, val_dataloader):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            ...\n",
        "        \"\"\"\n",
        "\n",
        "        # Update dataloaders with new training (and validation) data\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "\n",
        "        # Send the copy to the chosen device and set it to evaluation mode\n",
        "        self.old_net = self.old_net.to(self.device)\n",
        "        self.old_net.train(False)\n",
        "\n",
        "        # Define the optimization algorithm\n",
        "        parameters_to_optimize = self.net.parameters()\n",
        "        self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                   lr=self.LR,\n",
        "                                   momentum=self.MOMENTUM,\n",
        "                                   weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        # @todo\n",
        "\n",
        "        # Increment the number of output nodes for the new network by 10\n",
        "        self.increment_classes(10)\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        self.train(self.NUM_EPOCHS)\n",
        "    \n",
        "    def construct_exemplar_set(self, m):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            m (int): target number of exemplars.\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class.\n",
        "            m (int): target number of exemplars.\n",
        "        \"\"\"\n",
        "\n",
        "        pass\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            num_epochs (int): number of epochs for training the network.\n",
        "        Returns:\n",
        "            train_loss: loss computed on the last epoch\n",
        "            train_accuracy: accuracy computed on the last epoch\n",
        "            val_loss: average loss on the validation set of the last epoch\n",
        "            val_accuracy: accuracy on the validation set of the last epoch\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the optimization algorithm\n",
        "        parameters_to_optimize = self.net.parameters()\n",
        "        self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                   lr=self.LR,\n",
        "                                   momentum=self.MOMENTUM,\n",
        "                                   weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Initialize exemplar set\n",
        "        # @todo\n",
        "\n",
        "        # @todo: is the return behaviour intended? (scores of the last epoch)\n",
        "\n",
        "        self.net.to(self.device)\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_accuracy = 0 # @todo: should we use best_loss instead?\n",
        "        self.best_epoch = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Best validation model\n",
        "            if val_accuracy > self.best_accuracy:\n",
        "                self.best_accuracy = val_accuracy\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "        return (train_loss, train_accuracy,\n",
        "                val_loss, val_accuracy)\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train()  # Set network in training mode\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return (train_loss, train_accuracy)\n",
        "\n",
        "    def do_batch(self, batch, labels):\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).# Number of classes seen until now, including new classes\n",
        "        num_classes = self.output_neurons_count() \n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "        if self.old_net is None: # Network is training for the first time\n",
        "            # We only apply the classification loss\n",
        "            targets = one_hot_labels\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "        else:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10]\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "            # New net forward pass\n",
        "            # Size: batch size (rows) by number of classes seen until now, \n",
        "            # including new ones (columns)\n",
        "            outputs = self.net(batch)\n",
        "\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = \\\n",
        "            torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels) # Size = [128, 10]\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calcuate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return (val_loss, val_accuracy)\n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Test the model.\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.best_net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.best_net(images)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy: {accuracy}\")\n",
        "\n",
        "        return (accuracy, all_preds)\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def to_onehot(self, targets): \n",
        "        '''\n",
        "        Args:\n",
        "        targets : dataloader.dataset.targets of the new task images\n",
        "        '''\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mf04UjEgmPG",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9Oq44dxgmPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "9b4a7afd98814e8a81d21b38c4dcf4cb",
            "c71d2fbe5fb54a79b924facaf24eaca7",
            "1cbd6170e1a041dcb9c1ec2967aed9a5",
            "c637ed09611543f495f596a3483691df",
            "1e3ffc483f2c4d4aaf94885d4bb2e986",
            "5bc53cca36d14d45ad5912fab4bec9f8",
            "9a90df8adc7644baa441ddb23ab50e21",
            "5561f3b256e64a4db1b091e57b68359a"
          ]
        },
        "outputId": "eb5efb46-fa83-497f-8456-9f70b91bbda7"
      },
      "source": [
        "train_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "val_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "test_dataloaders = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "\n",
        "    test_subsets = []\n",
        "\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "        # Download dataset only at first instantiation\n",
        "        if(run_i+split_i == 0):\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train = True, download = download, random_state = RANDOM_STATE+run_i, transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train = False, download = False, random_state = RANDOM_STATE+run_i, transform=test_transform)\n",
        "    \n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATE)\n",
        "        \n",
        "        train_dataloaders[run_i].append(DataLoader(Subset(train_dataset, train_indices), \n",
        "                                batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "        \n",
        "        val_dataloaders[run_i].append(DataLoader(Subset(train_dataset, val_indices), \n",
        "                                    batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True))\n",
        "        \n",
        "        # Dataset with all seen class\n",
        "        test_dataloaders[run_i].append(DataLoader(test_dataset, \n",
        "                                batch_size=BATCH_SIZE, shuffle=True, num_workers=4))           "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b4a7afd98814e8a81d21b38c4dcf4cb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCpRMBKgvDB",
        "colab_type": "text"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRU--zYjmZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrIjsrtSXtMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize logs\n",
        "logs = []\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "\n",
        "        if split_i == 0: # The model learns the first 10 classes\n",
        "            icarl.set_dataloaders(train_dataloader=train_dataloaders[run_i][split_i],\n",
        "                                  val_dataloader=val_dataloaders[run_i][split_i],\n",
        "                                  test_dataloader=test_dataloaders[run_i][split_i])\n",
        "            icarl.train(NUM_EPOCHS)\n",
        "    \n",
        "        else:\n",
        "            icarl.set_dataloaders(test_dataloader=test_dataloaders[run_i][split_i])\n",
        "            icarl.incremental_train(split_i, train_dataloaders[run_i][split_i], val_dataloaders[run_i][split_i])\n",
        "        \n",
        "        icarl.test()\n",
        "\n",
        "        # @todo: implement logging and plots"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}