{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzqxHIh4OCdW",
        "colab_type": "text"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBHSznCZxpNB",
        "colab_type": "text"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eQ6O12jxMFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "8d850dc1-1d97-46b4-b21d-45a55e52d58b"
      },
      "source": [
        "!pip3 install 'torch==1.5.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.6/dist-packages (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.0) (1.18.4)\n",
            "Collecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.4)\n",
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 23kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n",
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/6a/30d21c886293cca3755b8e55de34137a5068b77eba1c0644d3632080516b/Pillow-SIMD-7.0.0.post3.tar.gz (630kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 2.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl size=1110393 sha256=fd5d727ebc69d00c9941ed7408ca0eb57d95e2f8606e6e3c30e0ef8c5a344a56\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/ac/4f/4cdf8febba528e5f1b09fc58d5181e1c12ed1e8655dcd583b8\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-7.0.0.post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAYXtIdpx0Yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YFLG4yRVwi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "username = ''\n",
        "password = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09iWc_oCotu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "75db7d4d-da58-48bb-fb3f-7a42fba1340a"
      },
      "source": [
        "# Download packages from repository\n",
        "\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'incremental-learning-image-classification'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 83 (delta 39), reused 35 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n",
            "renamed 'incremental-learning-image-classification/notebook.ipynb' -> './notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/README.md' -> './README.md'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePsy3BLSiXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: put in model package once finished\n",
        "\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class CIFAR100(VisionDataset):\n",
        "    \"\"\"CIFAR-100 dataset handler.\n",
        "    \n",
        "    Args:\n",
        "        root (string): Root directory of the dataset where directory\n",
        "            cifar-100-python exists.\n",
        "        split (string, optional): If 'train', creates dataset from training\n",
        "            set, otherwise creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in a\n",
        "            PIL image and returns a transformed version.\n",
        "    \"\"\"\n",
        "\n",
        "    base_folder = 'cifar-100-python'\n",
        "\n",
        "    train_filename = 'train'\n",
        "    test_filename = 'test'\n",
        "    meta_filename = 'meta'\n",
        "\n",
        "    def __init__(self, root, split='train', transform=None):\n",
        "        super(CIFAR100, self).__init__(root, transform=transform)\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        if split == 'train':\n",
        "            filename = self.train_filename\n",
        "        else:\n",
        "            filename = self.test_filename\n",
        "        \n",
        "        # @todo: add integrity checks\n",
        "        data_path = os.path.join(self.root, self.base_folder, filename)\n",
        "\n",
        "        with open(data_path, 'rb') as f:\n",
        "            entry = pickle.load(f, encoding='latin1')\n",
        "            self.data = entry['data']\n",
        "            self.labels = entry['fine_labels']\n",
        "        \n",
        "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
        "        self.data = self.data.transpose((0, 2, 3, 1))  # Convert to HWC\n",
        "        \n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "        meta_path = os.path.join(self.root, self.base_folder, self.meta_filename)\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f, encoding='latin1')\n",
        "            self.label_names = meta['fine_label_names']\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Access an element through its index.\n",
        "\n",
        "        Args:\n",
        "            index (int): Index\n",
        "\n",
        "        Returns:\n",
        "            tuple: (image, label) where label is index of the target class.\n",
        "        \"\"\"\n",
        "\n",
        "        image, label = self.data[index], self.labels[index]\n",
        "\n",
        "        image = Image.fromarray(image) # Return a PIL image\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the length of the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def get_class(self, label):\n",
        "        \"\"\"Return the indices of data belonging to the specified label.\"\"\"\n",
        "        return np.where(self.labels==label)[0]\n",
        "\n",
        "    def map_labels(self, label_map):\n",
        "        \"\"\"Change dataset labels with a label map.\n",
        "        \n",
        "        Args:\n",
        "            label_map (dict): dictionary mapping all original CIFAR100 labels\n",
        "                to custom labels.\n",
        "\n",
        "                e.g., {0: custom_label_0, ..., 99: custom_label_99}\n",
        "        \"\"\"\n",
        "\n",
        "        self.label_map = label_map\n",
        "\n",
        "        self.labels = np.vectorize(lambda x: self.label_map[x])(self.labels)\n",
        "\n",
        "        # @todo: also change the order of self.label_names\n",
        "\n",
        "    def class_splits(self, steps=10, random_state=None):\n",
        "        \"\"\"Split the classes in several sets of equal length and return them.\"\"\"\n",
        "\n",
        "        rs = np.random.RandomState(random_state)\n",
        "\n",
        "        idx = np.arange(len(self.label_names))\n",
        "        rs.shuffle(idx)\n",
        "\n",
        "        splits = np.split(idx, steps)\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def train_val_split(self, class_splits, val_size=0.5, random_state=None):\n",
        "        \"\"\"Perform a train and validation split based on given class splits.\n",
        "\n",
        "        Args:\n",
        "            class_splits (list): class split returned by self.class_splits\n",
        "            val_size (int, float or None): size of the validation set.                \n",
        "\n",
        "        Returns:\n",
        "            tuple: (train_indices, val_indices) where each element in the tuple\n",
        "                is a list of lists.\n",
        "\n",
        "                train_indices is a list of len(class_splits) lists. Each inner\n",
        "                list contains the training indices belonging to a class split.\n",
        "                val_indices, analogously, contains the validation indices\n",
        "                belonging to a class split.\n",
        "\n",
        "                e.g., [[0, 1, 2, 3, ..., 99],             <- first class split\n",
        "                       [100, 101, 12, 103, ..., 199],     <- second class split\n",
        "                       [200, 201, 22, 203, ..., 299],\n",
        "                       ...\n",
        "                       [900, 901, 902, 903, ..., 999]]    <- last class split\n",
        "        \"\"\"\n",
        "\n",
        "        train_indices = []\n",
        "        val_indices = []\n",
        "\n",
        "        for i, split in enumerate(class_splits):\n",
        "            train_indices.append([])\n",
        "            val_indices.append([])\n",
        "\n",
        "            for c in split:\n",
        "                # For each class, split the data into train and test\n",
        "                idx = self.get_class(c)\n",
        "                train_idx, val_idx = train_test_split(idx.tolist(), test_size=val_size, random_state=random_state)\n",
        "\n",
        "                train_indices[i].extend(train_idx)\n",
        "                val_indices[i].extend(val_idx)\n",
        "\n",
        "        return train_indices, val_indices\n",
        "\n",
        "    def test_split(self, class_splits):\n",
        "        \"\"\"Perform a train validation split on the dataset.\n",
        "\n",
        "        Args:\n",
        "            class_splits (int): class split returned by self.class_splits\n",
        "            val_size (int, float or None): size of the validation set.                \n",
        "\n",
        "        Returns:\n",
        "            test_indices (list): A list of lists. Analogous to train_test_split.\n",
        "        \"\"\"\n",
        "\n",
        "        test_indices = []\n",
        "\n",
        "        for i, split in enumerate(class_splits):\n",
        "            test_indices.append([])\n",
        "\n",
        "            for c in split:\n",
        "                idx = self.get_class(c)\n",
        "                test_indices[i].extend(idx)\n",
        "\n",
        "        return test_indices\n",
        "\n",
        "    def debug_labels(self):\n",
        "        print(self.labels[:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUfYUe_QfVbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a8ddc810-5f20-4ad7-ffdd-c4769f6817e1"
      },
      "source": [
        "# Load Resnet for CIFAR\n",
        "\n",
        "!git clone https://github.com/akamaster/pytorch_resnet_cifar10.git\n",
        "!mv -v 'pytorch_resnet_cifar10' 'Resnet'\n",
        "\n",
        "from Resnet.resnet import resnet32"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_resnet_cifar10'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 81 (delta 0), reused 3 (delta 0), pack-reused 76\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n",
            "renamed 'pytorch_resnet_cifar10' -> 'Resnet'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j12pgffMR6Qv",
        "colab_type": "text"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwE0x8gkSisn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "RANDOM_STATE = 42       # For reproducibility of results\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 256        # Icarl uses lr = 128. It slow down the Training, that is already enough time consuming for us\n",
        "LR = 1                  # Icarl sets lr = 2. Since they use BinaryCrossEntropy loss it is feasible,\n",
        "                        # in our case it would diverge\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-5     # From Icarl\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method (at least 3 to have a fair benchmark)\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # From Icarl\n",
        "GAMMA = 0.25            # From Icarl\n",
        "# Logging\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDDdumxRwbdQ",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ka2RwY0VS8-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "e9c83eba-598f-4e7c-94ce-ba728d6cdcfb"
      },
      "source": [
        "# Download dataset\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xf 'cifar-100-python.tar.gz'\n",
        "!mv 'cifar-100-python' $DATA_DIR/cifar-100-python\n",
        "!rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-19 10:06:16--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  16.7MB/s    in 11s     \n",
            "\n",
            "2020-05-19 10:06:28 (14.8 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mv: cannot move 'cifar-100-python' to 'data/cifar-100-python': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJVKOqLjVUhp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmQEsuniWjIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import dataset\n",
        "\n",
        "# train_dataloader, val_dataloader and test_dataloader have the same structure.\n",
        "# Each one is a list of length NUM_RUNS. Each element of each list has length\n",
        "# CLASS_BATCH_SIZE, and contains the DataLoader instances.\n",
        "# e.g., train_dataloader[i][j] is the DataLoader corresponding to the j-th class\n",
        "# batch on the i-th run\n",
        "train_dataloaders = []\n",
        "val_dataloaders = []\n",
        "test_subsets = []\n",
        "\n",
        "# Map original label numbers to ascending order numbers\n",
        "# e.g., [1, 4, 7, 11, 25, ...] to\n",
        "#       {1: 0, 4: 1, 7: 2, 11: 3, 25: 4...}\n",
        "#\n",
        "# label_maps[i]: access the label map of the i-th run\n",
        "label_maps = []\n",
        "\n",
        "for run_i in range(NUM_RUNS): # To have a fair benchmark, we run every method on at least three different random splits.\n",
        "    train_dataset = CIFAR100('', split='train', transform=train_transform)\n",
        "    test_dataset = CIFAR100('', split='test', transform=eval_transform)\n",
        "\n",
        "    class_splits = train_dataset.class_splits(steps=CLASS_BATCH_SIZE, random_state=RANDOM_STATE+run_i)\n",
        "\n",
        "    label_maps.append({})\n",
        "    for split_i in range(len(class_splits)):\n",
        "        label_maps[run_i].update({class_splits[split_i][i]: j for i, j in zip(range(0, 10), range(split_i*10, (split_i+1)*10))})   \n",
        "\n",
        "    train_dataset.map_labels(label_maps[run_i])\n",
        "    test_dataset.map_labels(label_maps[run_i])\n",
        "\n",
        "    class_splits = [list(range(split_i*10, (split_i+1)*10)) for split_i in range(len(class_splits))]\n",
        "\n",
        "    train_indices, val_indices = train_dataset.train_val_split(class_splits, val_size=VAL_SIZE, random_state=RANDOM_STATE+run_i)\n",
        "    test_indices = test_dataset.test_split(class_splits)\n",
        "\n",
        "    train_dataloaders.append([])\n",
        "    val_dataloaders.append([])\n",
        "    test_subsets.append([])\n",
        "\n",
        "    for split_i in range(len(class_splits)): \n",
        "        train_subset = Subset(train_dataset, train_indices[split_i])\n",
        "        val_subset = Subset(train_dataset, val_indices[split_i])\n",
        "        test_subset = Subset(test_dataset, test_indices[split_i])\n",
        "\n",
        "        train_dataloaders[run_i].append(DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "        val_dataloaders[run_i].append(DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4))\n",
        "        test_subsets[run_i].append(test_subset)\n",
        "\n",
        "# The test set should include all the classes seen in current *and previous* training steps\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in reversed(range(0, len(class_splits))):\n",
        "        test_subsets[run_i][split_i] = DataLoader(ConcatDataset([test_subsets[run_i][i] for i in range(split_i+1)]), batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "test_dataloaders = test_subsets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqOQIuaBso79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to show an image grid\n",
        "\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNwcf1fpsvm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sanity check: visualize a batch of images\n",
        "dataiter = iter(train_dataloaders[0][0])\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# show images\n",
        "matplotlib_imshow(img_grid, one_channel=False)\n",
        "unique_labels = np.unique(labels, return_counts=True)\n",
        "unique_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEX3IV1VUXOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @todo: put in model package once finished\n",
        "# @todo: funciton to save best model during validation\n",
        "\n",
        "class Manager():\n",
        "\n",
        "    def __init__(self, device, net, criterion, train_dataloader, val_dataloader, test_dataloader):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "        self.criterion = criterion\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fc layer\"\"\"\n",
        "        in_features = self.net.linear.in_features  # size of each input sample\n",
        "        out_features = self.net.linear.out_features  # size of each output sample\n",
        "        weight = self.net.linear.weight.data\n",
        "\n",
        "        self.net.linear = nn.Linear(in_features, out_features+n)\n",
        "        self.net.linear.weight.data[:out_features] = weight\n",
        "\n",
        "    def do_batch(self, optimizer, batch, labels):\n",
        "        \"\"\"Runs model for one batch.\"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero-ing the gradients\n",
        "        outputs = self.net(batch)\n",
        "\n",
        "        loss = self.criterion(outputs, labels)\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects = torch.sum(\n",
        "            preds == labels.data).data.item()  # number corrects\n",
        "\n",
        "        loss.backward()  # backward pass: computes gradients\n",
        "        optimizer.step()  # update weights based on accumulated gradients\n",
        "\n",
        "        return (loss, running_corrects)\n",
        "\n",
        "    def do_epoch(self, optimizer, scheduler, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\"\"\"\n",
        "\n",
        "        self.net.train()  # Set network in training mode\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "        for images, labels in tqdm(self.train_dataloader, desc='Epoch: %d ' % (current_epoch)):\n",
        "\n",
        "            loss, corrects = self.do_batch(optimizer, images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Average Scores\n",
        "        train_loss = running_train_loss / batch_idx  # average over all batches\n",
        "        train_accuracy = running_corrects / \\\n",
        "            float(total)  # average over all samples\n",
        "\n",
        "        print('\\nTrain Loss {}, Train Accuracy {}'\\\n",
        "              .format(train_loss, train_accuracy))\n",
        "\n",
        "        return (train_loss, train_accuracy)\n",
        "\n",
        "    def validate(self):\n",
        "\n",
        "        self.net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "        for images, labels in self.val_dataloader:\n",
        "\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.net(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calcuate Scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        return (val_loss, val_accuracy)\n",
        "\n",
        "    def train(self, optimizer, scheduler, num_epochs):\n",
        "\n",
        "        self.net.to(self.device)\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        train_loss_history = []\n",
        "        train_accuracy_history = []\n",
        "        val_loss_history = []\n",
        "        val_accuracy_history = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "\n",
        "            train_loss, train_accuracy = self.do_epoch(\n",
        "                optimizer, scheduler, epoch+1)  # Epochs start counting form 1\n",
        "            # Validate at each epoch\n",
        "            val_loss, val_accuracy = self.validate()\n",
        "\n",
        "            train_loss_history.append(train_loss)\n",
        "            train_accuracy_history.append(train_accuracy)\n",
        "            val_loss_history.append(val_loss)\n",
        "            val_accuracy_history.append(val_accuracy)            \n",
        "\n",
        "        return (train_loss_history, train_accuracy_history,\n",
        "                val_loss_history, val_accuracy_history)\n",
        "\n",
        "    def test(self):\n",
        "\n",
        "        self.net.train(False)  # Set Network to evaluation mode\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        for images, labels in tqdm(self.test_dataloader):\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            outputs = self.net(images)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        # Calculate Accuracy\n",
        "        accuracy = running_corrects / \\\n",
        "            float(total)  \n",
        "\n",
        "        print('Test Accuracy: {}'.format(accuracy))\n",
        "\n",
        "        return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJqnljCV5gJ5",
        "colab_type": "text"
      },
      "source": [
        "**FINE TUNING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzVPf2KxKci3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "eb044ee1-59b0-456c-dda3-1e38d5be0430"
      },
      "source": [
        "train_loss_history = []\n",
        "train_accuracy_history = []\n",
        "val_loss_history = []\n",
        "val_accuracy_history = []\n",
        "test_accuracy_history = []\n",
        "\n",
        "# Iterate over runs\n",
        "for train_dataloader, val_dataloader, test_dataloader in zip(train_dataloaders, val_dataloaders, test_dataloaders):\n",
        "    train_loss_history.append({})\n",
        "    train_accuracy_history.append({})\n",
        "    val_loss_history.append({})\n",
        "    val_accuracy_history.append({})\n",
        "    test_accuracy_history.append({})\n",
        "\n",
        "    net = resnet32()  # Define the net\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss\n",
        "\n",
        "    # In this case we optimize over all the parameters of Resnet\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR,\n",
        "                          momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
        "                                               milestones=MILESTONES, gamma=GAMMA)\n",
        "        \n",
        "\n",
        "    i = 0\n",
        "    for train_split, val_split, test_split in zip(train_dataloader, val_dataloader, test_dataloader):\n",
        "\n",
        "        current_split = \"Split %i\"%(i)\n",
        "        print(current_split)\n",
        "\n",
        "        # Define Manager Object\n",
        "        manager = Manager(DEVICE, net, criterion,\n",
        "                          train_split, val_split, test_split)\n",
        "\n",
        "        scores = manager.train(optimizer, scheduler,\n",
        "                               NUM_EPOCHS)  # train the model\n",
        "\n",
        "        # score[i] = dictionary with key:epoch, value: score\n",
        "        train_loss_history[-1][current_split] = scores[0]\n",
        "        train_accuracy_history[-1][current_split] = scores[1]\n",
        "        val_loss_history[-1][current_split] = scores[2]\n",
        "        val_accuracy_history[-1][current_split] = scores[3]\n",
        "\n",
        "        # Test the model on classes seen until now\n",
        "        test_accuracy = manager.test()\n",
        "\n",
        "        test_accuracy_history[-1][current_split] = test_accuracy\n",
        "\n",
        "        manager.increment_classes(n=10)  # add 10 nodes to last FC layer\n",
        "\n",
        "        i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 : 100%|██████████| 18/18 [00:04<00:00,  3.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss 5.983281188540989, Train Accuracy 0.10133333333333333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 2 : 100%|██████████| 18/18 [00:04<00:00,  3.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Loss 2.3493687444263034, Train Accuracy 0.10155555555555555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 3 :  33%|███▎      | 6/18 [00:01<00:03,  3.36it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ESzk5gF2c_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_scores(train_loss_history, train_accuracy_history,\n",
        "                   val_loss_history, val_accuracy_history, test_accuracy_history):\n",
        "  '''Note: we decide to lose score informations on epochs, in favor of better comparability\n",
        "  of the scores over the different class splits\n",
        "  '''\n",
        "\n",
        "  keys = train_loss_history[0].keys() # define keys = class splits\n",
        "\n",
        "  # Initialize dictionaries to 0\n",
        "  avg_train_loss = {k:[0] for k in range(len(keys))}\n",
        "  avg_train_accuracy = {k:[0] for k in range(len(keys))}\n",
        "  avg_val_loss = {k:[0] for k in range(len(keys))}\n",
        "  avg_val_accuracy = {k:[0] for k in range(len(keys))}\n",
        "  avg_test_accuracy = {k:[0] for k in range(len(keys))}\n",
        "\n",
        "  # APpend in a list values for different runs sharing same key\n",
        "  for run in range(NUM_RUNS):\n",
        "    for i, key in enumerate(keys):\n",
        "      \n",
        "      avg_train_loss[i].append(train_loss_history[run][key][-1])\n",
        "      avg_train_accuracy[i].append(train_accuracy_history[run][key][-1])\n",
        "      avg_val_loss[i].append(val_loss_history[run][key][-1])\n",
        "      avg_val_accuracy[i].append(val_accuracy_history[run][key][-1])\n",
        "      avg_test_accuracy[i].append(test_accuracy_history[run][key])\n",
        "\n",
        "  # Average over all runs\n",
        "  avg_train_loss = [[np.array(avg_train_loss[key]).mean(),\n",
        "                         np.array(avg_train_loss[key]).std()] for key in range(len(keys))]\n",
        "  avg_train_accuracy = [[np.array(avg_train_accuracy[key]).mean(),\n",
        "                             np.array(avg_train_accuracy[key]).std()] for key in range(len(keys))]\n",
        "  avg_val_loss = [[np.array(avg_val_loss[key]).mean(),\n",
        "                       np.array(avg_val_loss[key]).std()] for key in range(len(keys))]\n",
        "  avg_val_accuracy = [[np.array(avg_val_accuracy[key]).mean(),\n",
        "                           np.array(avg_val_accuracy[key]).std()] for key in range(len(keys))]\n",
        "  avg_test_accuracy = [[np.array(avg_test_accuracy[key]).mean(),\n",
        "                            np.array(avg_test_accuracy[key]).std()] for key in range(len(keys))]\n",
        "\n",
        "  return(avg_train_loss, avg_train_accuracy, avg_val_loss, avg_val_accuracy, avg_test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R187Vuj7JFdC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKCztHSw7lHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_train_loss, avg_train_accuracy, avg_val_loss, avg_val_accuracy,\\\n",
        "avg_test_accuracy = average_scores(train_loss_history, train_accuracy_history,\n",
        "                                   val_loss_history, val_accuracy_history, test_accuracy_history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrsWsatEMnbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_val_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA9bBfJvBlLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot Train vs Validation loss and Train vs Validation Accuracy\n",
        "\n",
        "def plot_train_scores(train_loss, train_accuracy, validation_loss, validation_accuracy, save_directory):\n",
        "\n",
        "    # axes[0] = train loss\n",
        "    # axes[1] = train vs validation accuracy\n",
        "    fig, axes = plt.subplots(1, 2, figsize=[15, 5])\n",
        "\n",
        "    x = np.arange(10, 101, 10)\n",
        "    print(np.array(list(train_loss.values()))[:, 0])\n",
        "\n",
        "    axes[0].errorbar(x, np.array(list(train_loss.values()))[:, 0], np.array(list(train_loss.values()))[:,1],\n",
        "                 color='#2E84D5', linewidth=2.5, label='Train Loss')\n",
        "    axes[0].errorbar(x, np.array(list(validation_loss.values()))[:,0], np.array(list(validation_loss.values()))[:,1],\n",
        "                 color='#FF9232', linewidth=2.5, label='Validation Loss')\n",
        "    axes[0].set_title(\"Train Loss\")\n",
        "    axes[0].set_xlabel(\"Number of Classes\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "\n",
        "    axes[1].errorbar(x, np.array(list(train_accuracy.values()))[:,0], np.array(list(train_accuracy.values()))[:,1],\n",
        "                 color='#2E84D5', linewidth=2.5, label='Train Accuracy')\n",
        "    axes[1].errorbar(x, np.array(list(validation_accuracy.values()))[:,0], np.array(list(validation_accuracy.values()))[:,1],\n",
        "                 color='#FF9232', linewidth=2.5, label='Validation Accuracy')\n",
        "    axes[1].set_title(\"Val vs Train Accuracy\")\n",
        "    axes[1].set_xlabel(\"Number of Classes\")\n",
        "    axes[1].set_ylabel(\"Accuracy\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    axes[0].legend()\n",
        "    axes[1].legend()\n",
        "    axes[0].grid(True)\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    if save_directory != None:\n",
        "      fig.savefig(save_directory)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TAdCtXDEa5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_train_scores(avg_train_loss, avg_train_accuracy, avg_val_loss, avg_val_accuracy, None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}