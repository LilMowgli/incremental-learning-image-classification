{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "studies.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1f5dc477b4274ed1872b723cdec0f1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f4313089fa2a41c2a8bd5b67d1fa4c86",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_939ccb347e3f467f8b036937b113d90a",
              "IPY_MODEL_f8f94815d938481dba5a596462fa9aac"
            ]
          }
        },
        "f4313089fa2a41c2a8bd5b67d1fa4c86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "939ccb347e3f467f8b036937b113d90a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74e8154a152d42fea583b92dd264643e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06336c43a8be4fe3838ef94a4812dd72"
          }
        },
        "f8f94815d938481dba5a596462fa9aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3f6a278456ff4c4a8b16b17887a9d1cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 68282029.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b865df03f77546928f024ae652f7d5af"
          }
        },
        "74e8154a152d42fea583b92dd264643e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06336c43a8be4fe3838ef94a4812dd72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f6a278456ff4c4a8b16b17887a9d1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b865df03f77546928f024ae652f7d5af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification\n",
        "Ablation studies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {}
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "colab": {}
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = 'xolotl18'\n",
        "password = ''\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "colab": {}
      },
      "source": [
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from model.icarl import Exemplars\n",
        "from model.icarl import iCaRL\n",
        "from utils import plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "\n",
        "RANDOM_STATE = None\n",
        "\n",
        "RANDOM_STATES = [658, 423, 422]      # For reproducibility of results                        \n",
        "                                     # Note: different random states give very different\n",
        "                                     # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "NUM_BATCHES = 10\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 2                  # Initial learning rate\n",
        "                       \n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR",
        "colab_type": "text"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mf04UjEgmPG",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9Oq44dxgmPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "1f5dc477b4274ed1872b723cdec0f1ce",
            "f4313089fa2a41c2a8bd5b67d1fa4c86",
            "939ccb347e3f467f8b036937b113d90a",
            "f8f94815d938481dba5a596462fa9aac",
            "74e8154a152d42fea583b92dd264643e",
            "06336c43a8be4fe3838ef94a4812dd72",
            "3f6a278456ff4c4a8b16b17887a9d1cd",
            "b865df03f77546928f024ae652f7d5af"
          ]
        },
        "outputId": "a350c840-4239-47ac-df2c-a1e32f7c75fb"
      },
      "source": [
        "train_subsets = [[] for i in range(NUM_RUNS)]\n",
        "val_subsets = [[] for i in range(NUM_RUNS)]\n",
        "test_subsets = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "        if run_i+split_i == 0: # Download dataset only at first instantiation\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=RANDOM_STATES[run_i], transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=RANDOM_STATES[run_i], transform=test_transform)\n",
        "    \n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATES[run_i])\n",
        "\n",
        "        # Define subsets\n",
        "        train_subsets[run_i].append(Subset(train_dataset, train_indices))\n",
        "        val_subsets[run_i].append(Subset(train_dataset, val_indices))\n",
        "        test_subsets[run_i].append(test_dataset)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f5dc477b4274ed1872b723cdec0f1ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0oEuUQPo2Nb",
        "colab_type": "text"
      },
      "source": [
        "## Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9tu5eAf7mXF",
        "colab_type": "text"
      },
      "source": [
        "### K-NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fnolqQQ72Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class iCaRLwithKNN(iCaRL):\n",
        "    def classifier_fit(self, train_dataset, **clf_args):\n",
        "        \"\"\"Fit classifier on the union of training dataset and exemplars.\"\"\"\n",
        "\n",
        "        # Union of training dataset and exemplars\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Convert dataset to numpy format\n",
        "        # X contains training samples, y contains labels\n",
        "        X, y = self.dataset_to_numpy(train_dataset_with_exemplars)\n",
        "\n",
        "        # Extract features from the training dataset\n",
        "        X_features = self.extract_features(torch.tensor(X, dtype=torch.float))\n",
        "        for i in range(X_features.size(0)):\n",
        "            X_features[i] = X_features[i]/X_features[i].norm()\n",
        "        X_features = X_features.to('cpu').numpy()\n",
        "\n",
        "        self.clf = KNeighborsClassifier(clf_args['n_neighbors'])\n",
        "        self.clf.fit(X_features, y)\n",
        "\n",
        "    def classifier_predict(self, test_dataset):\n",
        "        \"\"\"Predict labels of test_dataset.\"\"\"\n",
        "\n",
        "        X_test, y_test = self.dataset_to_numpy(test_dataset)\n",
        "\n",
        "        # Extract features from the test set\n",
        "        X_test_features = self.extract_features(torch.tensor(X_test, dtype=torch.float))\n",
        "        for i in range(X_test_features.size(0)):\n",
        "            X_test_features[i] = X_test_features[i]/X_test_features[i].norm()\n",
        "        X_test_features = X_test_features.to('cpu').numpy()\n",
        "        \n",
        "        y_pred = self.clf.predict(X_test_features)\n",
        "\n",
        "        return y_test, y_pred\n",
        "\n",
        "    def dataset_to_numpy(self, dataset):\n",
        "        # Preallocate arrays\n",
        "        X = np.zeros((len(dataset), 3, 32, 32))\n",
        "        y = np.zeros(len(dataset), dtype=int)\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "        for idx, (image, labels) in enumerate(dataloader):\n",
        "            X[idx] = image[0].numpy()\n",
        "            y[idx] = labels.numpy()[0]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def test_knn(self, test_dataset, train_dataset):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.classifier_fit(train_dataset, n_neighbors=3)\n",
        "            y_truth, y_pred = self.classifier_predict(test_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_truth, y_pred)\n",
        "\n",
        "        print(f\"Test accuracy (iCaRL with KNN): {accuracy} \")\n",
        "\n",
        "        return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t3c4MCBV2jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB1mJCwwVwqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_knn = [[] for _ in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl_knn = iCaRLwithKNN(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl_knn.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        acc, _ = icarl_knn.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "        logs_icarl[run_i].append(acc)\n",
        "\n",
        "        acc = icarl_knn.test_knn(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "        logs_icarl_knn[run_i].append(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ufG1nvlnrIkP"
      },
      "source": [
        "### Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dswJ5wX2rIkS",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class iCaRLwithSVM(iCaRL):\n",
        "    def classifier_fit(self, train_dataset, **clf_args):\n",
        "        \"\"\"Fit classifier on the union of training dataset and exemplars.\"\"\"\n",
        "\n",
        "        # Union of training dataset and exemplars\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Convert dataset to numpy format\n",
        "        # X contains training samples, y contains labels\n",
        "        X, y = self.dataset_to_numpy(train_dataset_with_exemplars)\n",
        "\n",
        "        # Extract features from the training dataset\n",
        "        X_features = self.extract_features(torch.tensor(X, dtype=torch.float))\n",
        "        for i in range(X_features.size(0)):\n",
        "            X_features[i] = X_features[i]/X_features[i].norm()\n",
        "        X_features = X_features.to('cpu').numpy()\n",
        "\n",
        "        self.clf = SVC(C=clf_args['C'], kernel=clf_args['kernel'], gamma=clf_args['gamma'])\n",
        "        self.clf.fit(X_features, y)\n",
        "\n",
        "    def classifier_predict(self, test_dataset):\n",
        "        \"\"\"Predict labels of test_dataset.\"\"\"\n",
        "\n",
        "        X_test, y_test = self.dataset_to_numpy(test_dataset)\n",
        "\n",
        "        # Extract features from the test set\n",
        "        X_test_features = self.extract_features(torch.tensor(X_test, dtype=torch.float))\n",
        "        for i in range(X_test_features.size(0)):\n",
        "            X_test_features[i] = X_test_features[i]/X_test_features[i].norm()\n",
        "        X_test_features = X_test_features.to('cpu').numpy()\n",
        "        \n",
        "        y_pred = self.clf.predict(X_test_features)\n",
        "\n",
        "        return y_test, y_pred\n",
        "\n",
        "    def dataset_to_numpy(self, dataset):\n",
        "        # Preallocate arrays\n",
        "        X = np.zeros((len(dataset), 3, 32, 32))\n",
        "        y = np.zeros(len(dataset), dtype=int)\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "        for idx, (image, labels) in enumerate(dataloader):\n",
        "            X[idx] = image[0].numpy()\n",
        "            y[idx] = labels.numpy()[0]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def test_knn(self, test_dataset, train_dataset):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.classifier_fit(train_dataset, C=1, kernel=\"rbf\", gamma=\"auto\")\n",
        "            y_truth, y_pred = self.classifier_predict(test_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = accuracy_score(y_truth, y_pred)\n",
        "\n",
        "        print(f\"Test accuracy (iCaRL with SVC): {accuracy} \")\n",
        "\n",
        "        return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsmxdNMVrIkV",
        "colab": {}
      },
      "source": [
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Emgmal00rIkZ",
        "colab": {}
      },
      "source": [
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_svm = [[] for _ in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl_svm = iCaRLwithSVM(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl_svm.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        acc, _ = icarl_svm.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "        logs_icarl[run_i].append(acc)\n",
        "\n",
        "        acc = icarl_svm.test_knn(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "        logs_icarl_svm[run_i].append(acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUggs5zsz-eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(logs_icarl)\n",
        "print(logs_icarl_svm)from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzoVUe8wVWUY",
        "colab_type": "text"
      },
      "source": [
        "### All classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x86Z3l3OViOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX4SqCBKVjLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NhDWVFPVmAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class iCaRLwithCLFS(iCaRL):\n",
        "    def classifier_fit(self, train_dataset, split):\n",
        "        \"\"\"Fit classifier on the union of training dataset and exemplars.\"\"\"\n",
        "\n",
        "        # Union of training dataset and exemplars\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Convert dataset to numpy format\n",
        "        # X contains training samples, y contains labels\n",
        "        X, y = self.dataset_to_numpy(train_dataset_with_exemplars)\n",
        "\n",
        "        # Extract features from the training dataset\n",
        "        X_features = self.extract_features(torch.tensor(X, dtype=torch.float))\n",
        "        for i in range(X_features.size(0)):\n",
        "            X_features[i] = X_features[i]/X_features[i].norm()\n",
        "        X_features = X_features.to('cpu').numpy()\n",
        "\n",
        "        param_grid1 = { \n",
        "          'n_estimators': [100, 200, 500],\n",
        "          'max_features': ['sqrt', 'log2'],\n",
        "          'max_depth' : [5,6,7],\n",
        "          'criterion' :['gini', 'entropy']\n",
        "        }\n",
        "\n",
        "        param_grid2 = {\n",
        "            'n_neighbors': [3, 5, 7, 9],\n",
        "            'leaf_size': [10, 30, 100]\n",
        "        }\n",
        "        param_grid3 = {\n",
        "            'kernel' : ['linear', 'rbf', 'sigmoid'],\n",
        "            'C' : [0.01, 0.1, 1, 10],\n",
        "            'gamma' : [1, 0.1, 0.01, 0.001]\n",
        "        }\n",
        "\n",
        "\n",
        "        self.clf1 = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid1, cv=3, refit=True)\n",
        "        self.clf2 = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=param_grid2, cv=3, refit=True)\n",
        "        self.clf3 = GridSearchCV(estimator=SVC(), param_grid=param_grid3, cv=3, refit=True)\n",
        "        \n",
        "        self.clf1.fit(X_features, y)\n",
        "        self.clf2.fit(X_features, y)\n",
        "        self.clf3.fit(X_features, y)\n",
        "\n",
        "        with open('./params.txt', 'w') as writefile:\n",
        "          writefile.write(f\"split {split}\")\n",
        "          writefile.write(f\"rfc best parameters:\\n n_estimtors: {self.clf1.best_params_['n_estimators']}\\n max_features : {self.clf1.best_params_['max_features']}\\n max_depth : {self.clf1.best_params_['max_depth']}\\n criterion : {self.clf1.best_params_['criterion']}\")\n",
        "          writefile.write(f\"knn best parameters:\\n knn__n_neighbors : {self.clf2.best_params_['n_neighbors']}\\n leaf_size : {self.clf2.best_params_['leaf_size']}\")\n",
        "          writefile.write(f\"svc best parameters:\\n kernel : {self.clf3.best_params_['kernel']}\\n C : {self.clf2.best_params_['C']}\\n gamma : {self.clf3.best_params_['gamma']}\\n\")\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def classifier_predict(self, test_dataset):\n",
        "        \"\"\"Predict labels of test_dataset.\"\"\"\n",
        "\n",
        "        X_test, y_test = self.dataset_to_numpy(test_dataset)\n",
        "\n",
        "        # Extract features from the test set\n",
        "        X_test_features = self.extract_features(torch.tensor(X_test, dtype=torch.float))\n",
        "        for i in range(X_test_features.size(0)):\n",
        "            X_test_features[i] = X_test_features[i]/X_test_features[i].norm()\n",
        "        X_test_features = X_test_features.to('cpu').numpy()\n",
        "        \n",
        "        y_pred1 = self.clf1.predict(X_test_features)\n",
        "        y_pred2 = self.clf2.predict(X_test_features)\n",
        "        y_pred3 = self.clf3.predict(X_test_features)\n",
        "\n",
        "        return y_test, y_pred1, y_pred2, y_pred3\n",
        "\n",
        "    def dataset_to_numpy(self, dataset):\n",
        "        # Preallocate arrays\n",
        "        X = np.zeros((len(dataset), 3, 32, 32))\n",
        "        y = np.zeros(len(dataset), dtype=int)\n",
        "\n",
        "        dataloader = DataLoader(dataset, batch_size=1)\n",
        "\n",
        "        for idx, (image, labels) in enumerate(dataloader):\n",
        "            X[idx] = image[0].numpy()\n",
        "            y[idx] = labels.numpy()[0]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def test_classifiers(self, test_dataset, train_dataset, split):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.classifier_fit(train_dataset, split)\n",
        "            y_truth, y_pred1, y_pred2, y_pred3 = self.classifier_predict(test_dataset)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy1 = accuracy_score(y_truth, y_pred1)\n",
        "        accuracy2 = accuracy_score(y_truth, y_pred2)\n",
        "        accuracy3 = accuracy_score(y_truth, y_pred3)\n",
        "\n",
        "        print(f\"Test accuracy (iCaRL with RFC): {accuracy1} \")\n",
        "        print(f\"Test accuracy (iCaRL with KNN): {accuracy2} \")\n",
        "        print(f\"Test accuracy (iCaRL with SVC): {accuracy3} \")\n",
        "\n",
        "        return accuracy1, accuracy2, accuracy3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F1MQyiGV3bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_rfc = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_knn = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_svc = [[] for _ in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "\n",
        "    icarl_CLFS = iCaRLwithCLFS(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "    \n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl_CLFS.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        acc, _ = icarl_CLFS.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i])\n",
        "        logs_icarl[run_i].append(acc)\n",
        "\n",
        "        acc1, acc2, acc3 = icarl_CLFS.test_classifiers(test_subsets[run_i][split_i], train_subsets[run_i][split_i], split_i)\n",
        "        logs_icarl_rfc[run_i].append(acc1)\n",
        "        logs_icarl_knn[run_i].append(acc2)\n",
        "        logs_icarl_svc[run_i].append(acc3)\n",
        "\n",
        "        with open('./logs.txt', 'w') as writefile:\n",
        "          writefile.write(f\"split : {split_i}/n\")\n",
        "          writefile.write(f\"accuracy iCaRL : {acc}/n\")\n",
        "          writefile.write(f\"accuracy rfc : {acc1}/n\")\n",
        "          writefile.write(f\"accuracy knn : {acc2}/n\")\n",
        "          writefile.write(f\"accuracy svc : {acc3}/n/n\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download('params.txt')\n",
        "files.download('logs.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}