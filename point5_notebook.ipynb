{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4df08f2b6a0f483e8747c6bc1763a18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_79e4e7f7e05a4801b8015ca4dd9aae76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1aa86075b6e441e8f205694605748bc",
              "IPY_MODEL_29d60b50f04e411ba0ec0eaf98db3264"
            ]
          }
        },
        "79e4e7f7e05a4801b8015ca4dd9aae76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1aa86075b6e441e8f205694605748bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_32e79f80e75149a69a559255454404f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6259b2f1d1a547beb9da5fa5b332c031"
          }
        },
        "29d60b50f04e411ba0ec0eaf98db3264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_da1035e6bf634cbd80ab7afd8072fe19",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [10:02&lt;00:00, 280380.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbb31417938646db83351901d0d4b96f"
          }
        },
        "32e79f80e75149a69a559255454404f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6259b2f1d1a547beb9da5fa5b332c031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da1035e6bf634cbd80ab7afd8072fe19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbb31417938646db83351901d0d4b96f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "37497dc4-4876-48b7-fc13-f908c330f9ad"
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 753.4MB 23kB/s \n",
            "\u001b[31mERROR: torchvision 0.6.1+cu101 has requirement torch==1.5.1, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "  Found existing installation: torch 1.5.1+cu101\n",
            "    Uninstalling torch-1.5.1+cu101:\n",
            "      Successfully uninstalled torch-1.5.1+cu101\n",
            "Successfully installed torch-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.0MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
            "Installing collected packages: torchvision\n",
            "  Found existing installation: torchvision 0.6.1+cu101\n",
            "    Uninstalling torchvision-0.6.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.1+cu101\n",
            "Successfully installed torchvision-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchvision"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting Pillow-SIMD\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/6a/30d21c886293cca3755b8e55de34137a5068b77eba1c0644d3632080516b/Pillow-SIMD-7.0.0.post3.tar.gz (630kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 634kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-7.0.0.post3-cp36-cp36m-linux_x86_64.whl size=1110282 sha256=e9fa8a8d765f720aa48c6b46a4be4b10890cef5ab69e9ea394933518a7b06a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/ac/4f/4cdf8febba528e5f1b09fc58d5181e1c12ed1e8655dcd583b8\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-7.0.0.post3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "a25abaa0-ef20-4a69-caac-3e8726099f24"
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = 'LilMowgli'\n",
        "password = '_Kora3030_'\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'incremental-learning-image-classification'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 942 (delta 28), reused 5 (delta 3), pack-reused 889\u001b[K\n",
            "Receiving objects: 100% (942/942), 4.32 MiB | 3.88 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "renamed 'incremental-learning-image-classification/data' -> './data'\n",
            "renamed 'incremental-learning-image-classification/dist_targets_analisys_notebook.ipynb' -> './dist_targets_analisys_notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/icarlSVM.ipynb' -> './icarlSVM.ipynb'\n",
            "renamed 'incremental-learning-image-classification/joint_training.ipynb' -> './joint_training.ipynb'\n",
            "renamed 'incremental-learning-image-classification/losses' -> './losses'\n",
            "renamed 'incremental-learning-image-classification/lwf_confmat_targets.pkl' -> './lwf_confmat_targets.pkl'\n",
            "renamed 'incremental-learning-image-classification/model' -> './model'\n",
            "renamed 'incremental-learning-image-classification/notebook.ipynb' -> './notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/point5_notebook.ipynb' -> './point5_notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/README.md' -> './README.md'\n",
            "renamed 'incremental-learning-image-classification/report' -> './report'\n",
            "renamed 'incremental-learning-image-classification/results' -> './results'\n",
            "renamed 'incremental-learning-image-classification/studies-classifier.ipynb' -> './studies-classifier.ipynb'\n",
            "renamed 'incremental-learning-image-classification/utils' -> './utils'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "bd384d30-2b17-4646-e023-1c46b1eb9c09"
      },
      "source": [
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from model.icarl import Exemplars\n",
        "# from model.icarl import iCaRL\n",
        "from utils import plot"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3w4M1jBsp4",
        "colab_type": "text"
      },
      "source": [
        "## Save and Load Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvBFyKFyBs0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "if not os.path.isdir('./obj'):\n",
        "    !mkdir 'obj'\n",
        "\n",
        "def obj_save(obj, name):\n",
        "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "    time.sleep(5)\n",
        "\n",
        "    files.download('obj/'+ name + '.pkl') \n",
        "\n",
        "def obj_load(name):\n",
        "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "\n",
        "RANDOM_STATE = None\n",
        "\n",
        "RANDOM_STATES = [658, 423, 422]      # For reproducibility of results                        \n",
        "                                     # Note: different random states give very different\n",
        "                                     # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "NUM_BATCHES = 10\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 2                  # Initial learning rate\n",
        "                       \n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B6CcDcDlMf_",
        "colab_type": "text"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mf04UjEgmPG",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9Oq44dxgmPN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "4df08f2b6a0f483e8747c6bc1763a18a",
            "79e4e7f7e05a4801b8015ca4dd9aae76",
            "c1aa86075b6e441e8f205694605748bc",
            "29d60b50f04e411ba0ec0eaf98db3264",
            "32e79f80e75149a69a559255454404f8",
            "6259b2f1d1a547beb9da5fa5b332c031",
            "da1035e6bf634cbd80ab7afd8072fe19",
            "bbb31417938646db83351901d0d4b96f"
          ]
        },
        "outputId": "d445217f-1b32-43a1-f490-072a32e96dd6"
      },
      "source": [
        "train_subsets = [[] for i in range(NUM_RUNS)]\n",
        "val_subsets = [[] for i in range(NUM_RUNS)]\n",
        "test_subsets = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "        if run_i+split_i == 0: # Download dataset only at first instantiation\n",
        "            download = True\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=RANDOM_STATES[run_i], transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=RANDOM_STATES[run_i], transform=test_transform)\n",
        "    \n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATES[run_i])\n",
        "\n",
        "        # Define subsets\n",
        "        train_subsets[run_i].append(Subset(train_dataset, train_indices))\n",
        "        val_subsets[run_i].append(Subset(train_dataset, val_indices))\n",
        "        test_subsets[run_i].append(test_dataset)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4df08f2b6a0f483e8747c6bc1763a18a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5aJIio18xnO",
        "colab_type": "text"
      },
      "source": [
        "### iCaRL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blz4YxK78ha0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "PATH = 'net_zero'\n",
        "\n",
        "class Exemplars(torch.utils.data.Dataset):\n",
        "    def __init__(self, exemplars, transform=None):\n",
        "        # exemplars = [\n",
        "        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n",
        "        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n",
        "        #     ...\n",
        "        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n",
        "        # ]\n",
        "\n",
        "        self.dataset = []\n",
        "        self.targets = []\n",
        "\n",
        "        for y, exemplar_y in enumerate(exemplars):\n",
        "            self.dataset.extend(exemplar_y)\n",
        "            self.targets.extend([y] * len(exemplar_y))\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "class iCaRL:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform\n",
        "                 , loaded_exemplars = None, loaded_prototypes = None, loaded_feat_coeffs = None):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        # Set transformations\n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "        # with num_classes the number of classes seen until now by the network.\n",
        "        self.exemplars = []\n",
        "\n",
        "        # Initialize the copy of the old network, used to compute outputs of the\n",
        "        # previous network for the distillation loss, to None. This is useful to\n",
        "        # correctly apply the first function when training the network for the\n",
        "        # first time.\n",
        "        self.old_net = None\n",
        "\n",
        "        # in order to recycle first split scores \n",
        "        self.loaded_exemplars = loaded_exemplars\n",
        "        self.loaded_prototypes = loaded_prototypes\n",
        "        self.loaded_feat_coeffs = loaded_feat_coeffs\n",
        "\n",
        "        # For drift analisys and mitigation\n",
        "        \n",
        "        self.prototypes = dict() # prototypes representation to study features shift from one task to the other\n",
        "        self.rep_distance = dict() # dictionary with drift value for each label. Drift value is MSE\n",
        "        self.features_criterion = nn.SmoothL1Loss() # loss for drift prevention\n",
        "        self.alpha = 0.000005 # features loss coefficient\n",
        "\n",
        "\n",
        "        # Maximum number of exemplars\n",
        "        self.memory_size = 2000\n",
        "    \n",
        "        # Loss function\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # If True, test on the best model found (e.g., minimize loss). If False,\n",
        "        # test on the last model build (of the last epoch).\n",
        "        self.VALIDATE = False\n",
        "\n",
        "    def prototype_distance(self):\n",
        "      # measure prototype distance between last 2 learned reprsentations\n",
        "\n",
        "      # criterion = nn.MSELoss(reduction = 'mean')\n",
        "      criterion = nn.MSELoss(reduction = 'none')\n",
        "\n",
        "      num_common_classes = self.output_neurons_count()-10\n",
        "\n",
        "      current_rep = dict()\n",
        "\n",
        "      for label in range(num_common_classes): # consider only class known by both tasks\n",
        "\n",
        "        # for each label, it returns the mean distance between old prototypes and new\n",
        "        # the result is stored in self.rep_distance\n",
        "        # multiply by old net featrues value as weight. It says importance of a single feature\n",
        "        # for that class. We do not want a meaningless feature to be penalizing\n",
        "        old = self.prototypes[self.split-1][label, :]\n",
        "        tot_old = torch.sum(old)\n",
        "        new = self.prototypes[self.split][label, :]\n",
        "        coef_old = old*self.feature_neurons_count()/tot_old # weight of the features based on  their  importance\n",
        "        \n",
        "        current_rep[label] = torch.mean(criterion(old*coef_old,new*coef_old))\n",
        "        \n",
        "      self.rep_distance[self.split] = current_rep\n",
        "        \n",
        "\n",
        "    def classify(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "            self.prototypes[self.split] = deepcopy(self.cached_means)\n",
        "            self.features_coeffs = []\n",
        "            for el in self.prototypes[self.split]:\n",
        "              tot_label = torch.sum(el)\n",
        "              coef_label = self.feature_neurons_count()*el/tot_label\n",
        "              self.features_coeffs.append(coef_label)\n",
        "            \n",
        "            self.features_coeffs = torch.stack(self.features_coeffs)\n",
        "\n",
        "            # measure prototype distance from old class representaiton to new\n",
        "            # e.g. from representation  of split 8 to split 9\n",
        "            # distance is recorded for each class in common\n",
        "            # it measured with L2 metric\n",
        "\n",
        "\n",
        "            if self.split in [1, 9]: # at the moment only at split 1\n",
        "              self.prototype_distance()\n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    def extract_features(self, sample, batch=True, transform=None):\n",
        "        \"\"\"Extract features from single sample or from batch.\n",
        "        \n",
        "        Args:\n",
        "            sample (PIL image or torch.tensor): sample(s) from which to\n",
        "                extract features\n",
        "            batch (bool): if True, sample is a torch.tensor containing a batch\n",
        "                of images with dimensions (batch_size, 3, 32, 32)\n",
        "            transform: transformations to apply to the PIL image before\n",
        "                processing\n",
        "        Returns:\n",
        "            features: torch.tensor, 1-D of dimension 64 for single samples or\n",
        "                2-D of dimension (batch_size, 64) for batch\n",
        "        \"\"\"\n",
        "\n",
        "        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        if batch is False: # Treat sample as single PIL image\n",
        "            sample = transform(sample)\n",
        "            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336\n",
        "\n",
        "        sample = sample.to(self.device)\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            features = self.best_net.features(sample)\n",
        "        else:\n",
        "            features = self.net.features(sample)\n",
        "\n",
        "        if batch is False:\n",
        "            features = features[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def incremental_train(self, split, train_dataset, val_dataset):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            split (int): current split number, counting from zero\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        if split is not 0:\n",
        "            # Increment the number of output nodes for the new network by 10\n",
        "            self.increment_classes(10)\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        train_logs = self.update_representation(train_dataset, val_dataset)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Target total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m)\n",
        "        self.exemplars.extend(new_exemplars)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def update_representation(self, train_dataset, val_dataset):\n",
        "        \"\"\"Update the parameters of the network.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "        # Store split one results\n",
        "        # Allow avoiding repeating training on split 1 which is not meaningful in fine tuning\n",
        "        if self.split == 1 and self.loaded_exemplars is not None:\n",
        "          self.exemplars = self.loaded_exemplars\n",
        "          self.prototypes = self.loaded_prototypes\n",
        "          self.features_coeffs = self.loaded_feat_coeffs\n",
        "          self.old_net = deepcopy(net)\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        train_logs = self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "\n",
        "        if self.split == 0:\n",
        "          torch.save(net.state_dict(), PATH)\n",
        "          print(\"### Current net saved\")\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def construct_exemplar_set_rand(self, dataset, m):\n",
        "        \"\"\"Randomly sample m elements from a dataset without replacement.\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        for y in range(10):\n",
        "            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Randomly choose m samples from samples[y] without replacement\n",
        "            exemplars[y] = random.sample(samples[y], m)\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "\n",
        "        return exemplars\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"Procedure for removing exemplars from a given set.\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class\n",
        "            m (int): target number of exemplars\n",
        "        Returns:\n",
        "            exemplar_set: reduced exemplar set\n",
        "        \"\"\"\n",
        "\n",
        "        return exemplar_set[:m]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns: train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training. If\n",
        "            validation is enabled, return scores of the best epoch, otherwise\n",
        "            return scores of the last epoch.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define the optimization algorithm\n",
        "        parameters_to_optimize = self.net.parameters()\n",
        "        self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                   lr=self.LR,\n",
        "                                   momentum=self.MOMENTUM,\n",
        "                                   weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Create DataLoaders for training and validation\n",
        "        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # Send networks to chosen device\n",
        "        self.net = self.net.to(self.device)\n",
        "        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_train_accuracy = 0\n",
        "        \n",
        "        self.best_net = None\n",
        "        self.best_epoch = -1\n",
        "\n",
        "        for epoch in range(self.NUM_EPOCHS):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Validation criterion: best net is the one that minimizes the loss\n",
        "            # on the validation set.\n",
        "            if self.VALIDATE and val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_train_loss = train_loss\n",
        "                self.best_train_accuracy = train_accuracy\n",
        "\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            val_loss = self.best_val_loss\n",
        "            val_accuracy = self.best_val_accuracy\n",
        "            train_loss = self.best_train_loss\n",
        "            train_accuracy = self.best_train_accuracy\n",
        "\n",
        "            print(f\"Best model found at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set the current network in training mode\n",
        "        self.net.train()\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    def do_batch(self, batch, labels):\n",
        "        \"\"\"Train network for a batch. Loss is applied here.\n",
        "\n",
        "        Args:\n",
        "            batch: batch of data used for training the network\n",
        "            labels: targets of the batch\n",
        "        Returns:\n",
        "            loss: output of the criterion applied\n",
        "            running_corrects: number of correctly classified elements\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).\n",
        "        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "        if self.old_net is None:\n",
        "            # Network is training for the first time, so we only apply the\n",
        "            # classification loss.\n",
        "            targets = one_hot_labels\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "        else:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10]\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "            # Introduce loss contribute to mitigate features drift from old task  \n",
        "            #representation to new one\n",
        "            # As target we use the prototypes saved for each class  on previous task\n",
        "            new_representation = self.extract_features(batch) # [batch_size, 64]\n",
        "            mask = [] # mask for the features representation\n",
        "            label_mask = [] # mask for the correspndent labels\n",
        "            labels_index = []\n",
        "\n",
        "            # initialize features loss\n",
        "            features_loss = 0\n",
        "\n",
        "            # for loop to select only old classes labels\n",
        "            # since this is the implementation of a distillation loss\n",
        "            for label in labels.to('cpu').numpy():\n",
        "              \n",
        "              if label >= num_classes-10:\n",
        "                mask.append(torch.tensor([False]))\n",
        "                label_mask.append(torch.tensor(False))\n",
        "              else:\n",
        "                mask.append(torch.tensor([True]))\n",
        "                label_mask.append(torch.tensor(True))\n",
        "                labels_index.append(torch.tensor(label))\n",
        "\n",
        "            \n",
        "            tensor_labels_index = torch.stack(labels_index).to(self.device)\n",
        "            selected_labels = torch.masked_select(labels, torch.stack(label_mask).to(self.device)) # apply mask\n",
        "            selected_new_rep = torch.stack([torch.masked_select(new_representation,\n",
        "                                                    torch.stack(mask).to(self.device))[i*64 : 64*(i+1)] for i in range(selected_labels.size(0))]) # apply mask\n",
        "            \n",
        "            # filter prototypes by labels selected\n",
        "            features_targets = torch.index_select(self.prototypes[self.split-1], dim = 0, index = tensor_labels_index)\n",
        "            \n",
        "            # compute features coefficient\n",
        "            coef_targets = torch.index_select(self.features_coeffs,dim = 0, index = tensor_labels_index)\n",
        "\n",
        "            features_loss = self.features_criterion(coef_targets*selected_new_rep, coef_targets*features_targets)\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "            loss = self.criterion(outputs, targets) + features_loss\n",
        "\n",
        "\n",
        "        # print(\"CLF+DIST Loss: {}\".format(loss))\n",
        "\n",
        "      \n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels)\n",
        "\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calculate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    def test(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "\n",
        "    def test_without_classifier(self, test_dataset):\n",
        "        \"\"\"Test the model without classifier, using the outputs of the\n",
        "        network instead.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            with torch.no_grad():\n",
        "                if self.VALIDATE:\n",
        "                    outputs = self.best_net(images)\n",
        "                else:\n",
        "                    outputs = self.net(images)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (hybrid1): {accuracy}\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "        self.net.fc.bias.data[:out_features] = bias\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def feature_neurons_count(self):\n",
        "        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n",
        "\n",
        "        return self.net.fc.in_features\n",
        "    \n",
        "    def to_onehot(self, targets):\n",
        "        \"\"\"Convert targets to one-hot encoding (for BCE loss).\n",
        "\n",
        "        Args:\n",
        "            targets: dataloader.dataset.targets of the new task images\n",
        "        \"\"\"\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)\n",
        "\n",
        "    def network_params(self):\n",
        "        weight = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        return weight, bias\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dZF1ObfqChI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "New iCaRL structure: at split 8 and 9 we compute features\n",
        "for each known class: we show how at the end of the training procedure there is a shift\n",
        "between old net representation and new net representaiton\n",
        "and hopefully we obseve a larger odg  of the mean for new class features\n",
        "\n",
        "class representation will be done comparing 2 or few classes, half from old task half from new\n",
        "See if there is a bias in representaioon and if there is a shift in representation beween old and new net\n",
        "on same classes\n",
        "\n",
        "Finally, we can propose a mitigation of the bias and of the shift fromold to new net.\n",
        "If time is available we can try to implement it\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCpRMBKgvDB",
        "colab_type": "text"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMPSeJd2v2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters for prototype distance measures\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRU--zYjmZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 0.3\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [77, 123] \n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mcTQUN7VLPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2e12860-76af-45fe-9a79-1b050458797b"
      },
      "source": [
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_rand = [[] for _ in range(NUM_RUNS)] # @DEBUG\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys_split = [dict() for _ in range(NUM_RUNS)]\n",
        "\n",
        "'''\n",
        "Cambia questa merda, non Ã¨ solo il primo seed!\n",
        "'''\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        all_targets = torch.stack([label[0] for _, label in DataLoader(test_subsets[run_i][split_i])])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "            logs_icarl_rand[run_i].append({}) # @DEBUG\n",
        "\n",
        "            \n",
        "            print(\"Random:\") # @DEBUG\n",
        "            acc_rand, all_targets, all_preds = icarl.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i]) # @DEBUG\n",
        "\n",
        "            logs_icarl_rand[run_i][split_i]['accuracy'] = acc_rand # @DEBUG\n",
        "\n",
        "            # Store results of split one\n",
        "            # Allow avoiding to repeat training on split 0, which is not meaningful in fine tuning\n",
        "            if split_i == 0:\n",
        "              exemplars = icarl.exemplars\n",
        "              prototypes = icarl.prototypes\n",
        "              features_coeffs = icarl.features_coeffs\n",
        "              \n",
        "              obj_save(exemplars, 'exemplars')\n",
        "              obj_save(prototypes, 'prototypes')\n",
        "              obj_save(features_coeffs, 'features_coeffs')\n",
        "\n",
        "            \n",
        "\n",
        "            logs_icarl[run_i][split_i]['accuracy'] = acc_rand\n",
        "            logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc\n",
        "\n",
        "\n",
        "# Possibili modifiche\n",
        "# 1. Alzare LR e alpha e rimuovere distillation, spostando a tutti gli effetti la distillation loss un layer prima\n",
        "#    in corrispondenza del layer che usiamo per la classificazione\n",
        "# 2. Se non impara le nuove  classi: alzo Lr e contemporaneamente abbasso alpha fino a che non trovo combinazione\n",
        "#    perfetta\n",
        "# 3. Posso anche variare la loss, anche se  nel setup attuale e anche logicamente secondo i nostri obbiettivi\n",
        "#    questa mi sembra appropriata. (Nostri obbiettivi e migliorare anche di uno 0.1 per ceto, qunidi non dev'essere una roba invasiva)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## Split 0 of run 0 ##\n",
            "Length of exemplars set: 0\n",
            "Epoch: 1, LR: [0.3]\n",
            "Train loss: 0.3229983555419104, Train accuracy: 0.2450892857142857\n",
            "Validation loss: 0.3537013190133231, Validation accuracy: 0.29910714285714285\n",
            "### Current net saved\n",
            "Target number of exemplars per class: 200\n",
            "Target total number of exemplars: 2000\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... done\n",
            "Test accuracy (iCaRL): 0.366 (exemplars and training data)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_c0610298-aec2-4c40-817e-3738087e76db\", \"exemplars.pkl\", 6206886)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
            "  torch.save(self, b)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_37747571-5bda-49ce-82ed-c961e3f9eec9\", \"prototypes.pkl\", 2962)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3a0d6660-426d-412c-8c01-6a7f06f5df66\", \"features_coeffs.pkl\", 2957)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "## Split 1 of run 0 ##\n",
            "Length of exemplars set: 2000\n",
            "Epoch: 1, LR: [0.3]\n",
            "Train loss: nan, Train accuracy: 0.03094059405940594\n",
            "Validation loss: nan, Validation accuracy: 0.0\n",
            "Target number of exemplars per class: 100\n",
            "Target total number of exemplars: 2000\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 100 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... done\n",
            "Test accuracy (iCaRL): 0.05 (exemplars and training data)\n",
            "## Split 2 of run 0 ##\n",
            "Length of exemplars set: 2000\n",
            "Epoch: 1, LR: [0.3]\n",
            "Train loss: nan, Train accuracy: 0.01547029702970297\n",
            "Validation loss: nan, Validation accuracy: 0.0\n",
            "Target number of exemplars per class: 66\n",
            "Target total number of exemplars: 1980\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 66 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 66 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... done\n",
            "Test accuracy (iCaRL): 0.03333333333333333 (exemplars and training data)\n",
            "## Split 3 of run 0 ##\n",
            "Length of exemplars set: 1980\n",
            "Epoch: 1, LR: [0.3]\n",
            "Train loss: nan, Train accuracy: 0.01021039603960396\n",
            "Validation loss: nan, Validation accuracy: 0.0\n",
            "Target number of exemplars per class: 50\n",
            "Target total number of exemplars: 2000\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 50 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 50 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... done\n",
            "Test accuracy (iCaRL): 0.025 (exemplars and training data)\n",
            "## Split 4 of run 0 ##\n",
            "Length of exemplars set: 2000\n",
            "Epoch: 1, LR: [0.3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6fdc155c83d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"## Split {split_i} of run {run_i} ##\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micarl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincremental_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_subsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mall_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_subsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-164b1b81beae>\u001b[0m in \u001b[0;36mincremental_train\u001b[0;34m(self, split, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Improve network parameters upon receiving new classes. Effectively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# train a new network starting from the current network parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# Compute the number of exemplars per class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-164b1b81beae>\u001b[0m in \u001b[0;36mupdate_representation\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# Train the network on combined dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_with_exemplars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# @todo: include exemplars in validation set?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# Keep a copy of the current network in order to compute its outputs for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-164b1b81beae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;31m# Run an epoch (start counting form 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# Validate after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-164b1b81beae>\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(self, current_epoch)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mrunning_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-164b1b81beae>\u001b[0m in \u001b[0;36mdo_batch\u001b[0;34m(self, batch, labels)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;31m# Backward pass: computes gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         r\"\"\"Registers a backward hook.\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mhook\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mevery\u001b[0m \u001b[0mtime\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m def grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1gdy7n1C4u9",
        "colab_type": "text"
      },
      "source": [
        "#### Execution - Split 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krsIVKVgDDxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_rand = [[] for _ in range(NUM_RUNS)] # @DEBUG\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys_split = [dict() for _ in range(NUM_RUNS)]\n",
        "\n",
        "'''\n",
        "Cambia questa merda, non Ã¨ solo il primo seed!\n",
        "'''\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    net = resnet32()\n",
        "    net.load_state_dict(torch.load(PATH))\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform, \n",
        "                  exemplars, prototypes, features_coeffs)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        all_targets = torch.stack([label[0] for _, label in DataLoader(test_subsets[run_i][split_i])])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "            logs_icarl_rand[run_i].append({}) # @DEBUG\n",
        "\n",
        "            \n",
        "            print(\"Random:\") # @DEBUG\n",
        "            acc_rand, all_targets, all_preds = icarl.test(test_subsets[run_i][split_i], train_subsets[run_i][split_i]) # @DEBUG\n",
        "\n",
        "            logs_icarl_rand[run_i][split_i]['accuracy'] = acc_rand # @DEBUG\n",
        "\n",
        "            # Store results of split one\n",
        "            # Allow avoiding to repeat training on split 0, which is not meaningful in fine tuning\n",
        "            if split_i == 0:\n",
        "              exemplars = icarl.exemplars\n",
        "              prototypes = icarl.prototypes\n",
        "              features_coeffs = icarl.features_coeffs\n",
        "              \n",
        "              obj_save(exemplars, 'exemplars')\n",
        "              obj_save(prototypes, 'prototypes')\n",
        "              obj_save(features_coeffs, 'features_coeffs')\n",
        "\n",
        "            \n",
        "\n",
        "            logs_icarl[run_i][split_i]['accuracy'] = acc_rand\n",
        "            logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc\n",
        "\n",
        "\n",
        "# Possibili modifiche\n",
        "# 1. Alzare LR e alpha e rimuovere distillation, spostando a tutti gli effetti la distillation loss un layer prima\n",
        "#    in corrispondenza del layer che usiamo per la classificazione\n",
        "# 2. Se non impara le nuove  classi: alzo Lr e contemporaneamente abbasso alpha fino a che non trovo combinazione\n",
        "#    perfetta\n",
        "# 3. Posso anche variare la loss, anche se  nel setup attuale e anche logicamente secondo i nostri obbiettivi\n",
        "#    questa mi sembra appropriata. (Nostri obbiettivi e migliorare anche di uno 0.1 per ceto, qunidi non dev'essere una roba invasiva)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdBCSeXCrL4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e3de7054-18b1-4a6d-f142-39f3f31edab6"
      },
      "source": [
        "!unzip net_zero_folder -d net_zero_folder.zip"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open net_zero_folder, net_zero_folder.zip or net_zero_folder.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UdGY1g1bT5m",
        "colab_type": "text"
      },
      "source": [
        "### Representation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EyqOwDJYsde",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "f6a4ed22-1eef-4400-b3b4-875cabbecf7e"
      },
      "source": [
        "icarl.rep_distance  # MSE, alpha = 0.00001, lr = 0.2, num_epochs = 130, milestones = 77\n",
        "                    # 0.76"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {0: tensor(0.0257, device='cuda:0'),\n",
              "  1: tensor(0.1041, device='cuda:0'),\n",
              "  2: tensor(0.0391, device='cuda:0'),\n",
              "  3: tensor(0.0515, device='cuda:0'),\n",
              "  4: tensor(0.0381, device='cuda:0'),\n",
              "  5: tensor(0.0631, device='cuda:0'),\n",
              "  6: tensor(0.0464, device='cuda:0'),\n",
              "  7: tensor(0.0528, device='cuda:0'),\n",
              "  8: tensor(0.0690, device='cuda:0'),\n",
              "  9: tensor(0.0396, device='cuda:0')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Uu2qPKGrrtf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "cbb648e6-49d7-44fc-9599-1a277051b08d"
      },
      "source": [
        "icarl.rep_distance  # MSE, alpha = 0.00005, lr = 0.3, num_epochs = 130, milestones = 67, 88"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {0: tensor(0.0329, device='cuda:0'),\n",
              "  1: tensor(0.1025, device='cuda:0'),\n",
              "  2: tensor(0.0472, device='cuda:0'),\n",
              "  3: tensor(0.0311, device='cuda:0'),\n",
              "  4: tensor(0.0145, device='cuda:0'),\n",
              "  5: tensor(0.0478, device='cuda:0'),\n",
              "  6: tensor(0.0838, device='cuda:0'),\n",
              "  7: tensor(0.0383, device='cuda:0'),\n",
              "  8: tensor(0.0126, device='cuda:0'),\n",
              "  9: tensor(0.0073, device='cuda:0')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkawPkrgYmWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d828fdfa-6452-4351-855f-bbf0637d368c"
      },
      "source": [
        "icarl.rep_distance  # cosine embeddings, weighted but without alpha"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {0: tensor(0.0003, device='cuda:0'),\n",
              "  1: tensor(0.0001, device='cuda:0'),\n",
              "  2: tensor(0.0003, device='cuda:0'),\n",
              "  3: tensor(0.0003, device='cuda:0'),\n",
              "  4: tensor(7.2254e-05, device='cuda:0'),\n",
              "  5: tensor(0.0002, device='cuda:0'),\n",
              "  6: tensor(0.0006, device='cuda:0'),\n",
              "  7: tensor(0.0005, device='cuda:0'),\n",
              "  8: tensor(0.0002, device='cuda:0'),\n",
              "  9: tensor(0.0003, device='cuda:0')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LAhOmDuTBoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "33c77711-035a-4dcf-e622-a0369dcc7b37"
      },
      "source": [
        "icarl.rep_distance # weighted by importance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {0: tensor(0.1515, device='cuda:0'),\n",
              "  1: tensor(0.0844, device='cuda:0'),\n",
              "  2: tensor(0.1231, device='cuda:0'),\n",
              "  3: tensor(0.1016, device='cuda:0'),\n",
              "  4: tensor(0.1191, device='cuda:0'),\n",
              "  5: tensor(0.2006, device='cuda:0'),\n",
              "  6: tensor(0.1563, device='cuda:0'),\n",
              "  7: tensor(0.1776, device='cuda:0'),\n",
              "  8: tensor(0.0996, device='cuda:0'),\n",
              "  9: tensor(0.2898, device='cuda:0')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I2grMIIbdAE",
        "colab_type": "text"
      },
      "source": [
        "### Analisys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuDM6Ydokv05",
        "colab_type": "text"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEn9P6usbwYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = [[logs_icarl[run_i][i]['train_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs_icarl[run_i][i]['train_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs_icarl[run_i][i]['val_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs_icarl[run_i][i]['val_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs_icarl[run_i][i]['accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GmiU4BfhVht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAWPfif3j7pK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ezv8lUG7DsQw"
      },
      "source": [
        "### T-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFIVAeXb70A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm\n",
        "\n",
        "def tsne_plot(tsne_target, tsne_output, out_labels):\n",
        "  '''\n",
        "  Args: \n",
        "  tsne_target: numpy array with shape (10, 2) represetning the 2 values in low dimensional space\n",
        "               it represents prototypes features in TSNE low dim. space\n",
        "  tsne_output: numpy array with shape (num_images, 2) representing the 2 values in low dimensional space\n",
        "               it represents the selected features representation in the 2 values in low dimensional space\n",
        "  out_labels: numpy array with shape (num_images), represent the labels of tsne_output images.\n",
        "              Label is decoded with color\n",
        "  '''\n",
        "\n",
        "  colors = ['#E14116', '#ff8e00', '#FFC300', '#008e00', '#00c0c0', '#400098', \n",
        "          '#8e008e', '#0e5276', '#a75063', '#d3a9c3']\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  for j in range(10):\n",
        "    plt.scatter(x=tsne_0[j, 0], y=tsne_0[j, 1], color = colors[j], label = j, marker = '*', s = 50\n",
        "    )\n",
        "  for i in range(tsne_9.shape[0]):\n",
        "    plt.scatter(x=tsne_9[i, 0], y=tsne_9[i, 1], color = colors[rep_labels[i]], marker = '.', s = 10\n",
        "    )\n",
        "\n",
        "  plt.legend()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHDeARe7xEWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TSNE on first batch classes with net trained at step 9\n",
        "test_dataloader = DataLoader(test_subsets[0][0], batch_size=2000, shuffle=True, num_workers=4)\n",
        "\n",
        "rep_labels = []\n",
        "reps = []\n",
        "\n",
        "i = 0\n",
        "\n",
        "for imgs, labels in test_dataloader:\n",
        "  features = icarl.extract_features(imgs)\n",
        "  i += 1\n",
        "  rep_labels = labels\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDOjMRr6qY5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne_0 = TSNE(n_components=2, perplexity = 50, n_iter = 10000, random_state = 42).fit_transform(icarl.prototypes[0].to('cpu').numpy(), [i for i in range(10)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8NhsodOzdN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tsne_9 = TSNE(n_components=2, perplexity = 50, n_iter=10000, random_state = 42).fit_transform(features.to('cpu').detach().numpy(), rep_labels.to('cpu').detach().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0PHhoHuyzIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "4a623919-b917-4b41-ed32-a76bbc8ab374"
      },
      "source": [
        "colors = ['#E14116', '#ff8e00', '#FFC300', '#008e00', '#00c0c0', '#400098', \n",
        "          '#8e008e', '#0e5276', '#a75063', '#d3a9c3']\n",
        "plt.figure(figsize=(8, 8))\n",
        "for j in range(10):\n",
        "  plt.scatter(x=tsne_0[j, 0], y=tsne_0[j, 1], color = colors[j], label = j, marker = '*', s = 50\n",
        "  )\n",
        "for i in range(tsne_9.shape[0]):\n",
        "  plt.scatter(x=tsne_9[i, 0], y=tsne_9[i, 1], color = colors[rep_labels[i]], marker = '.', s = 10\n",
        "  )\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fe402d5e668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwmC1_071L0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "7f61528c-aedf-4600-9ba4-114068f31e42"
      },
      "source": [
        "obj_save(tsne_9,'tsne_9')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_284fe964-8d0d-4b3d-9aba-bde234e68c30\", \"tsne_9_bis.pkl\", 8155)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyLhCh6P1TYf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "09bf4e08-8a21-4907-a0de-88226ffed003"
      },
      "source": [
        "obj_save(tsne_0,'tsne_0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_80b3f883-2b70-4666-b1e9-a96f7650b04a\", \"tsne_0.pkl\", 231)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}