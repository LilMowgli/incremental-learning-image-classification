{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzqxHIh4OCdW"
      },
      "source": [
        "# Incremental learning on image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBHSznCZxpNB"
      },
      "source": [
        "## Libraries and packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4eQ6O12jxMFf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "31601267-5384-4443-b78a-df688619409b"
      },
      "source": [
        "!pip3 install 'torch==1.4.0'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision==0.5.0 in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch==1.4.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.4.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (7.0.0.post3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAYXtIdpx0Yy",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, Subset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "09iWc_oCotu2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "54b9e315-f824-4bf7-bacf-f9b4589077e5"
      },
      "source": [
        "# GitHub credentials for cloning private repository\n",
        "username = 'LilMowgli'\n",
        "password = '_Kora3030_'\n",
        "\n",
        "# Download packages from repository\n",
        "password = urllib.parse.quote(password)\n",
        "!git clone https://$username:$password@github.com/manuelemacchia/incremental-learning-image-classification.git\n",
        "password = ''\n",
        "\n",
        "!mv -v incremental-learning-image-classification/* .\n",
        "!rm -rf incremental-learning-image-classification README.md"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'incremental-learning-image-classification'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (200/200), done.\u001b[K\n",
            "remote: Compressing objects: 100% (179/179), done.\u001b[K\n",
            "remote: Total 874 (delta 124), reused 43 (delta 21), pack-reused 674\u001b[K\n",
            "Receiving objects: 100% (874/874), 3.77 MiB | 1.50 MiB/s, done.\n",
            "Resolving deltas: 100% (483/483), done.\n",
            "mv: cannot move 'incremental-learning-image-classification/data' to './data': Directory not empty\n",
            "renamed 'incremental-learning-image-classification/dist_targets_analisys_notebook.ipynb' -> './dist_targets_analisys_notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/icarlSVM.ipynb' -> './icarlSVM.ipynb'\n",
            "renamed 'incremental-learning-image-classification/joint_training.ipynb' -> './joint_training.ipynb'\n",
            "mv: cannot move 'incremental-learning-image-classification/losses' to './losses': Directory not empty\n",
            "mv: cannot move 'incremental-learning-image-classification/model' to './model': Directory not empty\n",
            "renamed 'incremental-learning-image-classification/notebook.ipynb' -> './notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/point5_notebook.ipynb' -> './point5_notebook.ipynb'\n",
            "renamed 'incremental-learning-image-classification/README.md' -> './README.md'\n",
            "mv: cannot move 'incremental-learning-image-classification/report' to './report': Directory not empty\n",
            "mv: cannot move 'incremental-learning-image-classification/results' to './results': Directory not empty\n",
            "renamed 'incremental-learning-image-classification/studies-classifier.ipynb' -> './studies-classifier.ipynb'\n",
            "mv: cannot move 'incremental-learning-image-classification/utils' to './utils': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QPLViftqtC3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "dfe63d37-1ec8-478e-cdf8-2dabe79fff2b"
      },
      "source": [
        "from data.cifar100 import Cifar100\n",
        "from model.resnet_cifar import resnet32\n",
        "from model.manager import Manager\n",
        "from model.icarl import Exemplars\n",
        "# from model.icarl import iCaRL\n",
        "from utils import plot"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j12pgffMR6Qv"
      },
      "source": [
        "## Arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JwE0x8gkSisn",
        "colab": {}
      },
      "source": [
        "# Directories\n",
        "DATA_DIR = 'data'       # Directory where the dataset will be downloaded\n",
        "\n",
        "# Settings\n",
        "DEVICE = 'cuda'\n",
        "\n",
        "# Dataset\n",
        "\n",
        "RANDOM_STATE = None\n",
        "\n",
        "RANDOM_STATES = [658, 423, 422]      # For reproducibility of results                        \n",
        "                                     # Note: different random states give very different\n",
        "                                     # splits and therefore very different results.\n",
        "\n",
        "NUM_CLASSES = 100       # Total number of classes\n",
        "NUM_BATCHES = 10\n",
        "CLASS_BATCH_SIZE = 10   # Size of batch of classes for incremental learning\n",
        "\n",
        "VAL_SIZE = 0.1          # Proportion of validation set with respect to training set (between 0 and 1)\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 64         # Batch size (iCaRL sets this to 128)\n",
        "LR = 2                  # Initial learning rate\n",
        "                       \n",
        "MOMENTUM = 0.9          # Momentum for stochastic gradient descent (SGD)\n",
        "WEIGHT_DECAY = 1e-5     # Weight decay from iCaRL\n",
        "\n",
        "NUM_RUNS = 3            # Number of runs of every method\n",
        "                        # Note: this should be at least 3 to have a fair benchmark\n",
        "\n",
        "NUM_EPOCHS = 70         # Total number of training epochs\n",
        "MILESTONES = [49, 63]   # Step down policy from iCaRL (MultiStepLR)\n",
        "                        # Decrease the learning rate by gamma at each milestone\n",
        "GAMMA = 0.2             # Gamma factor from iCaRL"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B6CcDcDlMf_",
        "colab_type": "text"
      },
      "source": [
        "## iCaRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF0ypxGognNR",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mf04UjEgmPG",
        "colab": {}
      },
      "source": [
        "# Transformations for Learning Without Forgetting\n",
        "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                    \n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y9Oq44dxgmPN",
        "colab": {}
      },
      "source": [
        "train_subsets = [[] for i in range(NUM_RUNS)]\n",
        "val_subsets = [[] for i in range(NUM_RUNS)]\n",
        "test_subsets = [[] for i in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS):\n",
        "    for split_i in range(CLASS_BATCH_SIZE):\n",
        "        if run_i+split_i == 0: # Download dataset only at first instantiation\n",
        "            download = False\n",
        "        else:\n",
        "            download = False\n",
        "\n",
        "        # Create CIFAR100 dataset\n",
        "        train_dataset = Cifar100(DATA_DIR, train=True, download=download, random_state=RANDOM_STATES[run_i], transform=train_transform)\n",
        "        test_dataset = Cifar100(DATA_DIR, train=False, download=False, random_state=RANDOM_STATES[run_i], transform=test_transform)\n",
        "    \n",
        "        # Subspace of CIFAR100 of 10 classes\n",
        "        train_dataset.set_classes_batch(train_dataset.batch_splits[split_i]) \n",
        "        test_dataset.set_classes_batch([test_dataset.batch_splits[i] for i in range(0, split_i+1)])\n",
        "\n",
        "        # Define train and validation indices\n",
        "        train_indices, val_indices = train_dataset.train_val_split(VAL_SIZE, RANDOM_STATES[run_i])\n",
        "\n",
        "        # Define subsets\n",
        "        train_subsets[run_i].append(Subset(train_dataset, train_indices))\n",
        "        val_subsets[run_i].append(Subset(train_dataset, val_indices))\n",
        "        test_subsets[run_i].append(test_dataset)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwyiPrAZw3L9",
        "colab_type": "text"
      },
      "source": [
        "### iCaRL Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h-Fy3D0xAqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "from math import floor\n",
        "from copy import deepcopy\n",
        "import random\n",
        "\n",
        "sigmoid = nn.Sigmoid() # Sigmoid function\n",
        "\n",
        "class Exemplars(torch.utils.data.Dataset):\n",
        "    def __init__(self, exemplars, transform=None):\n",
        "        # exemplars = [\n",
        "        #     [ex0_class0, ex1_class0, ex2_class0, ...],\n",
        "        #     [ex0_class1, ex1_class1, ex2_class1, ...],\n",
        "        #     ...\n",
        "        #     [ex0_classN, ex1_classN, ex2_classN, ...]\n",
        "        # ]\n",
        "\n",
        "        self.dataset = []\n",
        "        self.targets = []\n",
        "\n",
        "        for y, exemplar_y in enumerate(exemplars):\n",
        "            self.dataset.extend(exemplar_y)\n",
        "            self.targets.extend([y] * len(exemplar_y))\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = self.dataset[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "class iCaRL:\n",
        "    \"\"\"Implement iCaRL, a strategy for simultaneously learning classifiers and a\n",
        "    feature representation in the class-incremental setting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, net, lr, momentum, weight_decay, milestones, gamma, num_epochs, batch_size, train_transform, test_transform):\n",
        "        self.device = device\n",
        "        self.net = net\n",
        "\n",
        "        # Set hyper-parameters\n",
        "        self.LR = lr\n",
        "        self.MOMENTUM = momentum\n",
        "        self.WEIGHT_DECAY = weight_decay\n",
        "        self.MILESTONES = milestones\n",
        "        self.GAMMA = gamma\n",
        "        self.NUM_EPOCHS = num_epochs\n",
        "        self.BATCH_SIZE = batch_size\n",
        "        \n",
        "        # Set transformations\n",
        "        self.train_transform = train_transform\n",
        "        self.test_transform = test_transform\n",
        "\n",
        "        # List of exemplar sets. Each set contains memory_size/num_classes exemplars\n",
        "        # with num_classes the number of classes seen until now by the network.\n",
        "        self.exemplars = []\n",
        "\n",
        "        # @DEBUG\n",
        "        self.exemplars_rand = []\n",
        "        \n",
        "        # prototypes representation to study features shift from one task to the other\n",
        "        self.prototypes = dict()\n",
        "        self.rep_distance = dict()\n",
        "        self.features_criterion = nn.MSELoss()\n",
        "        self.alpha = 0.0001\n",
        "\n",
        "        # Initialize the copy of the old network, used to compute outputs of the\n",
        "        # previous network for the distillation loss, to None. This is useful to\n",
        "        # correctly apply the first function when training the network for the\n",
        "        # first time.\n",
        "        self.old_net = None\n",
        "\n",
        "        # Maximum number of exemplars\n",
        "        self.memory_size = 2000\n",
        "    \n",
        "        # Loss function\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # If True, test on the best model found (e.g., minimize loss). If False,\n",
        "        # test on the last model build (of the last epoch).\n",
        "        self.VALIDATE = False\n",
        "\n",
        "    def prototype_distance(self):\n",
        "      # measure prototype distance between last 2 learned reprsentations\n",
        "\n",
        "      # criterion = nn.MSELoss(reduction = 'mean')\n",
        "      criterion = nn.MSELoss(reduction = 'none')\n",
        "\n",
        "      num_common_classes = self.output_neurons_count()-10\n",
        "\n",
        "      current_rep = dict()\n",
        "\n",
        "      for label in range(num_common_classes): # consider only class known by both tasks\n",
        "\n",
        "        # for each label, it returns the mean distance between old prototypes and new\n",
        "        # the result is stored in self.rep_distance\n",
        "        # multiply by old net featrues value as weight. It says importance of a single feature\n",
        "        # for that class. We do not want a meaningless feature to be penalizing\n",
        "        old = self.prototypes[self.split-1][label, :]\n",
        "        tot_old = torch.sum(old)\n",
        "        new = self.prototypes[self.split][label, :]\n",
        "        coef_old = old*self.feature_neurons_count()/tot_old # weight of the features based on  their  importance\n",
        "        \n",
        "        current_rep[label] = torch.mean(criterion(old*coef_old,new*coef_old))\n",
        "        \n",
        "      self.rep_distance[self.split] = current_rep\n",
        "        \n",
        "\n",
        "\n",
        "    def classify(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "            self.prototypes[self.split] = self.cached_means\n",
        "            self.features_coeffs = []\n",
        "            for el in self.prototypes[self.split]:\n",
        "              tot_label = torch.sum(el)\n",
        "              coef_label = self.feature_neurons_count()*el/tot_label\n",
        "              self.features_coeffs.append(coef_label)\n",
        "            \n",
        "            self.features_coeffs = torch.stack(self.features_coeffs)\n",
        "\n",
        "            # measure prototype distance from old class representaiton to new\n",
        "            # e.g. from representation  of split 8 to split 9\n",
        "            # distance is recorded for each class in common\n",
        "            # it measured with L2 metric\n",
        "\n",
        "\n",
        "            if self.split in [1, 9]: # at the moment only at split 1\n",
        "              self.prototype_distance()\n",
        "\n",
        "           \n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    # @DEBUG\n",
        "    def classify_rand(self, batch, train_dataset=None):\n",
        "        \"\"\"Mean-of-exemplars classifier used to classify images into the set of\n",
        "        classes observed so far.\n",
        "\n",
        "        Args:\n",
        "            batch (torch.tensor): batch to classify\n",
        "        Returns:\n",
        "            label (int): class label assigned to the image\n",
        "        \"\"\"\n",
        "\n",
        "        batch_features = self.extract_features(batch) # (batch size, 64)\n",
        "        for i in range(batch_features.size(0)):\n",
        "            batch_features[i] = batch_features[i]/batch_features[i].norm() # Normalize sample feature representation\n",
        "        batch_features = batch_features.to(self.device)\n",
        "\n",
        "        if self.cached_means is None:\n",
        "            print(\"Computing mean of exemplars... \", end=\"\")\n",
        "\n",
        "            self.cached_means = []\n",
        "\n",
        "            # Number of known classes\n",
        "            num_classes = len(self.exemplars_rand)\n",
        "\n",
        "            # Compute the means of classes with all the data available,\n",
        "            # including training data which contains samples belonging to\n",
        "            # the latest 10 classes. This will remove noise from the mean\n",
        "            # estimate, improving the results.\n",
        "            if train_dataset is not None:\n",
        "                train_features_list = [[] for _ in range(10)]\n",
        "\n",
        "                for train_sample, label in train_dataset:\n",
        "                    features = self.extract_features(train_sample, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm()\n",
        "                    train_features_list[label % 10].append(features)\n",
        "\n",
        "            # Compute means of exemplars for all known classes\n",
        "            for y in range(num_classes):\n",
        "                if (train_dataset is not None) and (y in range(num_classes-10, num_classes)):\n",
        "                    features_list = train_features_list[y % 10]\n",
        "                else:\n",
        "                    features_list = []\n",
        "\n",
        "                for exemplar in self.exemplars_rand[y]:\n",
        "                    features = self.extract_features(exemplar, batch=False, transform=self.test_transform)\n",
        "                    features = features/features.norm() # Normalize the feature representation of the exemplar\n",
        "                    features_list.append(features)\n",
        "                \n",
        "                features_list = torch.stack(features_list)\n",
        "                class_means = features_list.mean(dim=0)\n",
        "                class_means = class_means/class_means.norm() # Normalize the class means\n",
        "\n",
        "                self.cached_means.append(class_means)\n",
        "            \n",
        "            self.cached_means = torch.stack(self.cached_means).to(self.device)\n",
        "            print(\"done\")\n",
        "\n",
        "            self.prototypes[self.split] = self.cached_means\n",
        "            self.features_coeffs = []\n",
        "            for el in self.prototypes[self.split]:\n",
        "              tot_label = torch.sum(el)\n",
        "              coef_label = self.feature_neurons_count()*el/tot_label\n",
        "              self.features_coeffs.append(coef_label)\n",
        "\n",
        "            self.features_coeffs = torch.stack(self.features_coeffs)\n",
        "            # measure prototype distance from old class representaiton to new\n",
        "            # e.g. from representation  of split 8 to split 9\n",
        "            # distance is recorded for each class in common\n",
        "            # it measured with L2 metric\n",
        "\n",
        "\n",
        "            if self.split in [1, 9]: # at the moment only at split 1\n",
        "              self.prototype_distance()\n",
        "\n",
        "\n",
        "        preds = []\n",
        "        for i in range(batch_features.size(0)):\n",
        "            f_arg = torch.norm(batch_features[i] - self.cached_means, dim=1)\n",
        "            preds.append(torch.argmin(f_arg))\n",
        "        \n",
        "        return torch.stack(preds)\n",
        "    \n",
        "    def extract_features(self, sample, batch=True, transform=None):\n",
        "        \"\"\"Extract features from single sample or from batch.\n",
        "        \n",
        "        Args:\n",
        "            sample (PIL image or torch.tensor): sample(s) from which to\n",
        "                extract features\n",
        "            batch (bool): if True, sample is a torch.tensor containing a batch\n",
        "                of images with dimensions (batch_size, 3, 32, 32)\n",
        "            transform: transformations to apply to the PIL image before\n",
        "                processing\n",
        "        Returns:\n",
        "            features: torch.tensor, 1-D of dimension 64 for single samples or\n",
        "                2-D of dimension (batch_size, 64) for batch\n",
        "        \"\"\"\n",
        "\n",
        "        assert not (batch is False and transform is None), \"if a PIL image is passed to extract_features, a transform must be defined\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        if batch is False: # Treat sample as single PIL image\n",
        "            sample = transform(sample)\n",
        "            sample = sample.unsqueeze(0) # https://stackoverflow.com/a/59566009/6486336\n",
        "\n",
        "        sample = sample.to(self.device)\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            features = self.best_net.features(sample)\n",
        "        else:\n",
        "            features = self.net.features(sample)\n",
        "\n",
        "        if batch is False:\n",
        "            features = features[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def incremental_train(self, split, train_dataset, val_dataset):\n",
        "        \"\"\"Adjust internal knowledge based on the additional information\n",
        "        available in the new observations.\n",
        "\n",
        "        Args:\n",
        "            split (int): current split number, counting from zero\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        if split is not 0:\n",
        "            # Increment the number of output nodes for the new network by 10\n",
        "            self.increment_classes(10)\n",
        "\n",
        "        self.split = split\n",
        "\n",
        "        # Improve network parameters upon receiving new classes. Effectively\n",
        "        # train a new network starting from the current network parameters.\n",
        "        train_logs = self.update_representation(train_dataset, val_dataset)\n",
        "\n",
        "        # Compute the number of exemplars per class\n",
        "        num_classes = self.output_neurons_count()\n",
        "        m = floor(self.memory_size / num_classes)\n",
        "\n",
        "        print(f\"Target number of exemplars per class: {m}\")\n",
        "        print(f\"Target total number of exemplars: {m*num_classes}\")\n",
        "\n",
        "        # Reduce pre-existing exemplar sets in order to fit new exemplars\n",
        "        for y in range(len(self.exemplars)):\n",
        "            self.exemplars[y] = self.reduce_exemplar_set(self.exemplars[y], m)\n",
        "\n",
        "        # @DEBUG\n",
        "        for y in range(len(self.exemplars_rand)):\n",
        "            self.exemplars_rand[y] = self.reduce_exemplar_set(self.exemplars_rand[y], m)\n",
        "\n",
        "        # Construct exemplar set for new classes\n",
        "        new_exemplars = self.construct_exemplar_set(train_dataset, m)\n",
        "        self.exemplars.extend(new_exemplars)\n",
        "\n",
        "        # @DEBUG\n",
        "        new_exemplars = self.construct_exemplar_set_rand(train_dataset, m)\n",
        "        self.exemplars_rand.extend(new_exemplars)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def update_representation(self, train_dataset, val_dataset):\n",
        "        \"\"\"Update the parameters of the network.\n",
        "\n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns:\n",
        "            train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training\n",
        "        \"\"\"\n",
        "\n",
        "        # Combine the new training data with existing exemplars.\n",
        "        print(f\"Length of exemplars set: {sum([len(self.exemplars[y]) for y in range(len(self.exemplars))])}\")\n",
        "        exemplars_dataset = Exemplars(self.exemplars, self.train_transform)\n",
        "        train_dataset_with_exemplars = ConcatDataset([exemplars_dataset, train_dataset])\n",
        "\n",
        "        # Train the network on combined dataset\n",
        "        train_logs = self.train(train_dataset_with_exemplars, val_dataset) # @todo: include exemplars in validation set?\n",
        "\n",
        "        # Keep a copy of the current network in order to compute its outputs for\n",
        "        # the distillation loss while the new network is being trained.\n",
        "        self.old_net = deepcopy(self.net)\n",
        "\n",
        "        return train_logs\n",
        "\n",
        "    def construct_exemplar_set(self, dataset, m):\n",
        "        \"\"\"...\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        # Initialize exemplar sets\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        # Iterate over classes\n",
        "        for y in range(10):\n",
        "            print(f\"Extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Transform samples to tensors and apply normalization\n",
        "            transformed_samples = torch.zeros((len(samples[y]), 3, 32, 32)).to(self.device)\n",
        "            for i in range(len(transformed_samples)):\n",
        "                transformed_samples[i] = self.test_transform(samples[y][i])\n",
        "\n",
        "            # Extract features from samples\n",
        "            samples_features = self.extract_features(transformed_samples).to(self.device)\n",
        "\n",
        "            # Compute the feature mean of the current class\n",
        "            features_mean = samples_features.mean(dim=0)\n",
        "\n",
        "            # Initializes indices vector, containing the index of each exemplar chosen\n",
        "            idx = []\n",
        "\n",
        "            # See iCaRL algorithm 4\n",
        "            for k in range(1, m+1): # k = 1, ..., m -- Choose m exemplars\n",
        "                if k == 1: # No exemplars chosen yet, sum to 0 vector\n",
        "                    f_sum = torch.zeros(64).to(self.device)\n",
        "                else: # Sum of features of all exemplars chosen until now (j = 1, ..., k-1)\n",
        "                    f_sum = samples_features[idx].sum(dim=0)\n",
        "\n",
        "                # Compute argument of argmin function\n",
        "                f_arg = torch.norm(features_mean - 1/k * samples_features + f_sum, dim=1)\n",
        "\n",
        "                # Mask exemplars that were already taken, as we do not want to store the\n",
        "                # same exemplar more than once\n",
        "                mask = np.zeros(len(f_arg), int)\n",
        "                mask[idx] = 1\n",
        "                f_arg_masked = ma.masked_array(f_arg.cpu().detach().numpy(), mask=mask)\n",
        "\n",
        "                # Compute the nearest available exemplar\n",
        "                exemplar_idx = np.argmin(f_arg_masked)\n",
        "\n",
        "                idx.append(exemplar_idx)\n",
        "            \n",
        "            # Save exemplars to exemplar set\n",
        "            for i in idx:\n",
        "                exemplars[y].append(samples[y][i])\n",
        "            \n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "            \n",
        "        return exemplars\n",
        "\n",
        "    def construct_exemplar_set_rand(self, dataset, m):\n",
        "        \"\"\"Randomly sample m elements from a dataset without replacement.\n",
        "\n",
        "        Args:\n",
        "            dataset: dataset containing a split (samples from 10 classes) from\n",
        "                which to take exemplars\n",
        "            m (int): target number of exemplars per class\n",
        "        Returns:\n",
        "            exemplars: list of samples extracted from the dataset\n",
        "        \"\"\"\n",
        "\n",
        "        dataset.dataset.disable_transform()\n",
        "\n",
        "        samples = [[] for _ in range(10)]\n",
        "        for image, label in dataset:\n",
        "            label = label % 10 # Map labels to 0-9 range\n",
        "            samples[label].append(image)\n",
        "\n",
        "        dataset.dataset.enable_transform()\n",
        "\n",
        "        exemplars = [[] for _ in range(10)]\n",
        "\n",
        "        for y in range(10):\n",
        "            print(f\"Randomly extracting exemplars from class {y} of current split... \", end=\"\")\n",
        "\n",
        "            # Randomly choose m samples from samples[y] without replacement\n",
        "            exemplars[y] = random.sample(samples[y], m)\n",
        "\n",
        "            print(f\"Extracted {len(exemplars[y])} exemplars.\")\n",
        "\n",
        "        return exemplars\n",
        "\n",
        "    def reduce_exemplar_set(self, exemplar_set, m):\n",
        "        \"\"\"Procedure for removing exemplars from a given set.\n",
        "\n",
        "        Args:\n",
        "            exemplar_set (set): set of exemplars belonging to a certain class\n",
        "            m (int): target number of exemplars\n",
        "        Returns:\n",
        "            exemplar_set: reduced exemplar set\n",
        "        \"\"\"\n",
        "\n",
        "        return exemplar_set[:m]\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        \"\"\"Train the network for a specified number of epochs, and save\n",
        "        the best performing model on the validation set.\n",
        "        \n",
        "        Args:\n",
        "            train_dataset: dataset for training the model\n",
        "            val_dataset: dataset for validating the model\n",
        "        Returns: train_logs: tuple of four metrics (train_loss, train_accuracy,\n",
        "            val_loss, val_accuracy) obtained during network training. If\n",
        "            validation is enabled, return scores of the best epoch, otherwise\n",
        "            return scores of the last epoch.\n",
        "        \"\"\"\n",
        "        if self.split == 0:\n",
        "          # Define the optimization algorithm\n",
        "          parameters_to_optimize = self.net.parameters()\n",
        "          self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                    lr=2,\n",
        "                                    momentum=self.MOMENTUM,\n",
        "                                    weight_decay=self.WEIGHT_DECAY)\n",
        "        else:\n",
        "          # Define the optimization algorithm\n",
        "          parameters_to_optimize = self.net.parameters()\n",
        "          self.optimizer = optim.SGD(parameters_to_optimize, \n",
        "                                    lr=self.LR,\n",
        "                                    momentum=self.MOMENTUM,\n",
        "                                    weight_decay=self.WEIGHT_DECAY)\n",
        "        \n",
        "        # Define the learning rate decaying policy\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer,\n",
        "                                                        milestones=self.MILESTONES,\n",
        "                                                        gamma=self.GAMMA)\n",
        "\n",
        "        # Create DataLoaders for training and validation\n",
        "        self.train_dataloader = DataLoader(train_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "        self.val_dataloader = DataLoader(val_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "        # Send networks to chosen device\n",
        "        self.net = self.net.to(self.device)\n",
        "        if self.old_net is not None: self.old_net = self.old_net.to(self.device)\n",
        "\n",
        "        cudnn.benchmark  # Calling this optimizes runtime\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_val_accuracy = 0\n",
        "        self.best_train_loss = float('inf')\n",
        "        self.best_train_accuracy = 0\n",
        "        \n",
        "        self.best_net = None\n",
        "        self.best_epoch = -1\n",
        "\n",
        "        for epoch in range(self.NUM_EPOCHS):\n",
        "            # Run an epoch (start counting form 1)\n",
        "            train_loss, train_accuracy = self.do_epoch(epoch+1)\n",
        "        \n",
        "            # Validate after each epoch \n",
        "            val_loss, val_accuracy = self.validate()    \n",
        "\n",
        "            # Validation criterion: best net is the one that minimizes the loss\n",
        "            # on the validation set.\n",
        "            if self.VALIDATE and val_loss < self.best_val_loss:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.best_val_accuracy = val_accuracy\n",
        "                self.best_train_loss = train_loss\n",
        "                self.best_train_accuracy = train_accuracy\n",
        "\n",
        "                self.best_net = deepcopy(self.net)\n",
        "                self.best_epoch = epoch\n",
        "                print(\"Best model updated\")\n",
        "\n",
        "        if self.VALIDATE:\n",
        "            val_loss = self.best_val_loss\n",
        "            val_accuracy = self.best_val_accuracy\n",
        "            train_loss = self.best_train_loss\n",
        "            train_accuracy = self.best_train_accuracy\n",
        "\n",
        "            print(f\"Best model found at epoch {self.best_epoch+1}\")\n",
        "\n",
        "        return train_loss, train_accuracy, val_loss, val_accuracy\n",
        "    \n",
        "    def do_epoch(self, current_epoch):\n",
        "        \"\"\"Trains model for one epoch.\n",
        "        \n",
        "        Args:\n",
        "            current_epoch (int): current epoch number (begins from 1)\n",
        "        Returns:\n",
        "            train_loss: average training loss over all batches of the\n",
        "                current epoch.\n",
        "            train_accuracy: training accuracy of the current epoch over\n",
        "                all samples.\n",
        "        \"\"\"\n",
        "\n",
        "        # Set the current network in training mode\n",
        "        self.net.train()\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_train_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        print(f\"Epoch: {current_epoch}, LR: {self.scheduler.get_last_lr()}\")\n",
        "\n",
        "        for images, labels in self.train_dataloader:\n",
        "            loss, corrects = self.do_batch(images, labels)\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_corrects += corrects\n",
        "            total += labels.size(0)\n",
        "            batch_idx += 1\n",
        "\n",
        "        self.scheduler.step()\n",
        "\n",
        "        # Calculate average scores\n",
        "        train_loss = running_train_loss / batch_idx # Average over all batches\n",
        "        train_accuracy = running_corrects / float(total) # Average over all samples\n",
        "\n",
        "        print(f\"Train loss: {train_loss}, Train accuracy: {train_accuracy}\")\n",
        "\n",
        "        return train_loss, train_accuracy\n",
        "\n",
        "    def do_batch(self, batch, labels):\n",
        "        \"\"\"Train network for a batch. Loss is applied here.\n",
        "\n",
        "        Args:\n",
        "            batch: batch of data used for training the network\n",
        "            labels: targets of the batch\n",
        "        Returns:\n",
        "            loss: output of the criterion applied\n",
        "            running_corrects: number of correctly classified elements\n",
        "        \"\"\"\n",
        "        batch = batch.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # Zero-ing the gradients\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # One-hot encoding of labels of the new training data (new classes)\n",
        "        # Size: batch size (rows) by number of classes seen until now (columns)\n",
        "        #\n",
        "        # e.g., suppose we have four images in a batch, and each incremental\n",
        "        #   step adds three new classes. At the second step, the one-hot\n",
        "        #   encoding may return the following tensor:\n",
        "        #\n",
        "        #       tensor([[0., 0., 0., 1., 0., 0.],   # image 0 (label 3)\n",
        "        #               [0., 0., 0., 0., 1., 0.],   # image 1 (label 4)\n",
        "        #               [0., 0., 0., 0., 0., 1.],   # image 2 (label 5)\n",
        "        #               [0., 0., 0., 0., 1., 0.]])  # image 3 (label 4)\n",
        "        #\n",
        "        #   The first three elements of each vector will always be 0, as the\n",
        "        #   new training batch does not contain images belonging to classes\n",
        "        #   already seen in previous steps.\n",
        "        #\n",
        "        #   The last three elements of each vector will contain the actual\n",
        "        #   information about the class of each image (one-hot encoding of the\n",
        "        #   label). Therefore, we slice the tensor and remove the columns \n",
        "        #   related to old classes (all zeros).\n",
        "        num_classes = self.output_neurons_count() # Number of classes seen until now, including new classes\n",
        "        one_hot_labels = self.to_onehot(labels)[:, num_classes-10:num_classes]\n",
        "\n",
        "\n",
        "        if self.old_net is None:\n",
        "            # Network is training for the first time, so we only apply the\n",
        "            # classification loss.\n",
        "            targets = one_hot_labels\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "\n",
        "        else:\n",
        "            # Old net forward pass. We compute the outputs of the old network\n",
        "            # and apply a sigmoid function. These are used in the distillation\n",
        "            # loss. We discard the output of the new neurons, as they are not\n",
        "            # considered in the distillation loss.\n",
        "            old_net_outputs = sigmoid(self.old_net(batch))[:, :num_classes-10]\n",
        "\n",
        "            # Concatenate the outputs of the old network and the one-hot encoded\n",
        "            # labels along dimension 1 (columns).\n",
        "            # \n",
        "            # Each row refers to an image in the training set, and contains:\n",
        "            # - the output of the old network for that image, used by the\n",
        "            #   distillation loss\n",
        "            # - the one-hot label of the image, used by the classification loss\n",
        "            targets = torch.cat((old_net_outputs, one_hot_labels), dim=1)\n",
        "\n",
        "\n",
        "            # Introduce loss contribute to mitigate features drift from old task  \n",
        "            #representation to new one\n",
        "            # As target we use the prototypes saved for each class  on previous task\n",
        "            new_representation = self.extract_features(batch) # [batch_size, 64]\n",
        "            mask = [] # mask for the features representation\n",
        "            label_mask = [] # mask for the correspndent labels\n",
        "            labels_index = []\n",
        "\n",
        "            # initialize features loss\n",
        "            features_loss = 0\n",
        "\n",
        "            # for loop to select only old classes labels\n",
        "            # since this is the implementation of a distillation loss\n",
        "            for label in labels.to('cpu').numpy():\n",
        "              \n",
        "              if label >= num_classes-10:\n",
        "                mask.append(torch.tensor([False]))\n",
        "                label_mask.append(torch.tensor(False))\n",
        "              else:\n",
        "                mask.append(torch.tensor([True]))\n",
        "                label_mask.append(torch.tensor(True))\n",
        "                labels_index.append(torch.tensor(label))\n",
        "\n",
        "            print(\"labels index: {}\".format(labels_index))\n",
        "            tensor_labels_index = torch.stack(labels_index).to(self.device)\n",
        "            selected_labels = torch.masked_select(labels, torch.stack(label_mask).to(self.device)) # apply mask\n",
        "            selected_new_rep = torch.stack([torch.masked_select(new_representation,\n",
        "                                                    torch.stack(mask).to(self.device))[i*64 : 64*(i+1)] for i in range(selected_labels.size(0))]) # apply mask\n",
        "            print(\"Targets wished size: {}\".format(selected_labels.size()))\n",
        "            # filter prototypes by labels selected\n",
        "            features_targets = torch.index_select(self.prototypes[self.split-1], dim = 0, index = tensor_labels_index)\n",
        "            print(\"Targets actual size: {}\".format(features_targets.size()))\n",
        "\n",
        "\n",
        "            # compute features coefficient\n",
        "            coef_targets = torch.index_select(self.features_coeffs,dim = 0, index = tensor_labels_index)\n",
        "            print(\"Target coeffs size: {}\".format(coef_targets.size()))\n",
        "\n",
        "            features_loss = self.features_criterion(coef_targets*selected_new_rep, coef_targets*features_targets)\n",
        "\n",
        "\n",
        "            '''Penso sia concettualmente sbagliato quello che faccio qua sotto: anziche fare somma sulle features e media, faccio\n",
        "            somme delle medie, e poi medio su tutto\n",
        "            '''\n",
        "            # compute loss on selected exemplars. Cosine embeddings on l2 normalized features, as done in \n",
        "            # Learning a unified classifier incrementally via rebalancing\n",
        "            # counter = 0\n",
        "            # for label, new_rep in zip(selected_labels, selected_new_rep):\n",
        "            #   counter += 1\n",
        "            #   index = label.to('cpu').numpy()\n",
        "              \n",
        "            #   target = self.prototypes[self.split-1][index].unsqueeze(0)\n",
        "            #   out = new_rep.unsqueeze(0)\n",
        "           \n",
        "            #   target_coef = self.features_coeffs[index]\n",
        "\n",
        "            #   features_loss += self.alpha*self.features_criterion(target_coef*target.to(self.device), target_coef*out.to(self.device)) \n",
        "            # print(\"SUM: {}\".format(features_loss))\n",
        "            # features_loss = features_loss/counter\n",
        "            # print(\"MEAN: {}\".format(features_loss))\n",
        "            # print(\"Counter %i\" %counter)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = self.net(batch)\n",
        "            loss = self.criterion(outputs, targets) + features_loss\n",
        "\n",
        "\n",
        "        print(\"CLF+DIST Loss: {}\".format(loss))\n",
        "\n",
        "        # Get predictions\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Accuracy over NEW IMAGES, not over all images\n",
        "        running_corrects = torch.sum(preds == labels.data).data.item() \n",
        "\n",
        "        # Backward pass: computes gradients\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss, running_corrects\n",
        "\n",
        "    def validate(self):\n",
        "        \"\"\"Validate the model.\n",
        "        \n",
        "        Returns:\n",
        "            val_loss: average loss function computed on the network outputs\n",
        "                of the validation set (val_dataloader).\n",
        "            val_accuracy: accuracy computed on the validation set.\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)\n",
        "\n",
        "        running_val_loss = 0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        batch_idx = 0\n",
        "\n",
        "        for images, labels in self.val_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # One hot encoding of new task labels \n",
        "            one_hot_labels = self.to_onehot(labels)\n",
        "\n",
        "            # New net forward pass\n",
        "            outputs = self.net(images)  \n",
        "            loss = self.criterion(outputs, one_hot_labels) # BCE Loss with sigmoids over outputs\n",
        "\n",
        "            running_val_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update the number of correctly classified validation samples\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            batch_idx += 1\n",
        "\n",
        "        # Calculate scores\n",
        "        val_loss = running_val_loss / batch_idx\n",
        "        val_accuracy = running_corrects / float(total)\n",
        "\n",
        "        print(f\"Validation loss: {val_loss}, Validation accuracy: {val_accuracy}\")\n",
        "\n",
        "        return val_loss, val_accuracy\n",
        "\n",
        "    def test(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "\n",
        "    # @DEBUG\n",
        "    def test_rand(self, test_dataset, train_dataset=None):\n",
        "        \"\"\"Test the model.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "            train_dataset: training set used to train the last split, if\n",
        "                available\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False)  # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        # To store all predictions\n",
        "        all_preds = torch.tensor([])\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        all_targets = torch.tensor([])\n",
        "        all_targets = all_targets.type(torch.LongTensor)\n",
        "    \n",
        "\n",
        "        # Clear mean of exemplars cache\n",
        "        self.cached_means = None\n",
        "        \n",
        "        # Disable transformations for train_dataset, if available, as we will\n",
        "        # need original PIL images from which to extract features.\n",
        "        if train_dataset is not None: train_dataset.dataset.disable_transform()\n",
        "\n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                preds = self.classify_rand(images, train_dataset)\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "            all_targets = torch.cat(\n",
        "                (all_targets.to(self.device), labels.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        if train_dataset is not None: train_dataset.dataset.enable_transform()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (iCaRL): {accuracy} \", end=\"\")\n",
        "\n",
        "        if train_dataset is None:\n",
        "            print(\"(only exemplars)\")\n",
        "        else:\n",
        "            print(\"(exemplars and training data)\")\n",
        "\n",
        "        return accuracy, all_targets, all_preds\n",
        "\n",
        "    def test_without_classifier(self, test_dataset):\n",
        "        \"\"\"Test the model without classifier, using the outputs of the\n",
        "        network instead.\n",
        "\n",
        "        Args:\n",
        "            test_dataset: dataset on which to test the network\n",
        "        Returns:\n",
        "            accuracy (float): accuracy of the model on the test set\n",
        "        \"\"\"\n",
        "\n",
        "        self.net.train(False)\n",
        "        if self.best_net is not None: self.best_net.train(False) # Set Network to evaluation mode\n",
        "        if self.old_net is not None: self.old_net.train(False)\n",
        "\n",
        "        self.test_dataloader = DataLoader(test_dataset, batch_size=self.BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        all_preds = torch.tensor([]) # to store all predictions\n",
        "        all_preds = all_preds.type(torch.LongTensor)\n",
        "        \n",
        "        for images, labels in self.test_dataloader:\n",
        "            images = images.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Forward Pass\n",
        "            with torch.no_grad():\n",
        "                if self.VALIDATE:\n",
        "                    outputs = self.best_net(images)\n",
        "                else:\n",
        "                    outputs = self.net(images)\n",
        "\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update Corrects\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Append batch predictions\n",
        "            all_preds = torch.cat(\n",
        "                (all_preds.to(self.device), preds.to(self.device)), dim=0\n",
        "            )\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = running_corrects / float(total)  \n",
        "\n",
        "        print(f\"Test accuracy (hybrid1): {accuracy}\")\n",
        "\n",
        "        return accuracy, all_preds\n",
        "    \n",
        "    def increment_classes(self, n=10):\n",
        "        \"\"\"Add n classes in the final fully connected layer.\"\"\"\n",
        "\n",
        "        in_features = self.net.fc.in_features  # size of each input sample\n",
        "        out_features = self.net.fc.out_features  # size of each output sample\n",
        "        weight = self.net.fc.weight.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features+n)\n",
        "        self.net.fc.weight.data[:out_features] = weight\n",
        "    \n",
        "    def output_neurons_count(self):\n",
        "        \"\"\"Return the number of output neurons of the current network.\"\"\"\n",
        "\n",
        "        return self.net.fc.out_features\n",
        "    \n",
        "    def feature_neurons_count(self):\n",
        "        \"\"\"Return the number of neurons of the last layer of the feature extractor.\"\"\"\n",
        "\n",
        "        return self.net.fc.in_features\n",
        "    \n",
        "    def to_onehot(self, targets):\n",
        "        \"\"\"Convert targets to one-hot encoding (for BCE loss).\n",
        "\n",
        "        Args:\n",
        "            targets: dataloader.dataset.targets of the new task images\n",
        "        \"\"\"\n",
        "        num_classes = self.net.fc.out_features\n",
        "        one_hot_targets = torch.eye(num_classes)[targets]\n",
        "\n",
        "        return one_hot_targets.to(self.device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dZF1ObfqChI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "New iCaRL structure: at split 8 and 9 we compute features\n",
        "for each known class: we show how at the end of the training procedure there is a shift\n",
        "between old net representation and new net representaiton\n",
        "and hopefully we obseve a larger odg  of the mean for new class features\n",
        "\n",
        "class representation will be done comparing 2 or few classes, half from old task half from new\n",
        "See if there is a bias in representaioon and if there is a shift in representation beween old and new net\n",
        "on same classes\n",
        "\n",
        "Finally, we can propose a mitigation of the bias and of the shift fromold to new net.\n",
        "If time is available we can try to implement it\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmCpRMBKgvDB",
        "colab_type": "text"
      },
      "source": [
        "### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbMPSeJd2v2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters for prototype distance measures\n",
        "LR = 2\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nRU--zYjmZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# iCaRL hyperparameters\n",
        "LR = 1\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.00001\n",
        "MILESTONES = [49, 63]\n",
        "GAMMA = 0.2\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mcTQUN7VLPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dba8e5b-ab54-45be-8738-cbe0358c746c"
      },
      "source": [
        "# Define what tests to run\n",
        "TEST_ICARL = True # Run test with iCaRL (exemplars + train dataset)\n",
        "TEST_HYBRID1 = False # Run test with hybrid1\n",
        "\n",
        "# Initialize logs\n",
        "logs_icarl = [[] for _ in range(NUM_RUNS)]\n",
        "logs_icarl_rand = [[] for _ in range(NUM_RUNS)] # @DEBUG\n",
        "logs_hybrid1 = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys = [[] for _ in range(NUM_RUNS)]\n",
        "logs_analisys_split = [dict() for _ in range(NUM_RUNS)]\n",
        "\n",
        "for run_i in range(NUM_RUNS-2):\n",
        "    net = resnet32()\n",
        "    icarl = iCaRL(DEVICE, net, LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, NUM_EPOCHS, BATCH_SIZE, train_transform, test_transform)\n",
        "\n",
        "    for split_i in range(10):\n",
        "        print(f\"## Split {split_i} of run {run_i} ##\")\n",
        "        \n",
        "        train_logs = icarl.incremental_train(split_i, train_subsets[run_i][split_i], val_subsets[run_i][split_i])\n",
        "\n",
        "        all_targets = torch.stack([label[0] for _, label in DataLoader(test_subsets[run_i][split_i])])\n",
        "\n",
        "        if TEST_ICARL:\n",
        "            logs_icarl[run_i].append({})\n",
        "            logs_icarl_rand[run_i].append({}) # @DEBUG\n",
        "\n",
        "            \n",
        "            print(\"Random:\") # @DEBUG\n",
        "            acc_rand, all_targets, all_preds = icarl.test_rand(test_subsets[run_i][split_i], train_subsets[run_i][split_i]) # @DEBUG\n",
        "\n",
        "            logs_icarl_rand[run_i][split_i]['accuracy'] = acc_rand # @DEBUG\n",
        "\n",
        "\n",
        "            logs_icarl[run_i][split_i]['accuracy'] = acc_rand\n",
        "            logs_icarl[run_i][split_i]['conf_mat'] = confusion_matrix(all_targets.to('cpu'), all_preds.to('cpu'))\n",
        "\n",
        "            logs_icarl[run_i][split_i]['train_loss'] = train_logs[0]\n",
        "            logs_icarl[run_i][split_i]['train_accuracy'] = train_logs[1]\n",
        "\n",
        "        if TEST_HYBRID1:\n",
        "            logs_hybrid1[run_i].append({})\n",
        "\n",
        "            acc, all_preds = icarl.test_without_classifier(test_subsets[run_i][split_i])\n",
        "\n",
        "            logs_hybrid1[run_i][split_i]['accuracy'] = acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "## Split 0 of run 0 ##\n",
            "Length of exemplars set: 0\n",
            "Epoch: 1, LR: [2]\n",
            "CLF+DIST Loss: 0.9932001233100891\n",
            "CLF+DIST Loss: 0.8768850564956665\n",
            "CLF+DIST Loss: 1.0285345315933228\n",
            "CLF+DIST Loss: 0.8414387702941895\n",
            "CLF+DIST Loss: 0.6582624316215515\n",
            "CLF+DIST Loss: 0.42708054184913635\n",
            "CLF+DIST Loss: 0.4536280632019043\n",
            "CLF+DIST Loss: 0.39145728945732117\n",
            "CLF+DIST Loss: 0.3770957589149475\n",
            "CLF+DIST Loss: 0.4047045409679413\n",
            "CLF+DIST Loss: 0.45060044527053833\n",
            "CLF+DIST Loss: 0.36009320616722107\n",
            "CLF+DIST Loss: 0.3346131145954132\n",
            "CLF+DIST Loss: 0.3633396625518799\n",
            "CLF+DIST Loss: 0.34501734375953674\n",
            "CLF+DIST Loss: 0.3309718072414398\n",
            "CLF+DIST Loss: 0.34905752539634705\n",
            "CLF+DIST Loss: 0.3471786677837372\n",
            "CLF+DIST Loss: 0.3276425004005432\n",
            "CLF+DIST Loss: 0.33385607600212097\n",
            "CLF+DIST Loss: 0.33863186836242676\n",
            "CLF+DIST Loss: 0.32752808928489685\n",
            "CLF+DIST Loss: 0.3265652358531952\n",
            "CLF+DIST Loss: 0.3376058340072632\n",
            "CLF+DIST Loss: 0.3389246165752411\n",
            "CLF+DIST Loss: 0.33895713090896606\n",
            "CLF+DIST Loss: 0.33010777831077576\n",
            "CLF+DIST Loss: 0.3286171853542328\n",
            "CLF+DIST Loss: 0.32970282435417175\n",
            "CLF+DIST Loss: 0.3267810344696045\n",
            "CLF+DIST Loss: 0.3290857970714569\n",
            "CLF+DIST Loss: 0.3274064064025879\n",
            "CLF+DIST Loss: 0.3269389271736145\n",
            "CLF+DIST Loss: 0.32941100001335144\n",
            "CLF+DIST Loss: 0.3271080553531647\n",
            "CLF+DIST Loss: 0.3263600468635559\n",
            "CLF+DIST Loss: 0.32682231068611145\n",
            "CLF+DIST Loss: 0.32384735345840454\n",
            "CLF+DIST Loss: 0.3290214240550995\n",
            "CLF+DIST Loss: 0.3222201466560364\n",
            "CLF+DIST Loss: 0.325611412525177\n",
            "CLF+DIST Loss: 0.32469063997268677\n",
            "CLF+DIST Loss: 0.325144499540329\n",
            "CLF+DIST Loss: 0.3240595757961273\n",
            "CLF+DIST Loss: 0.3219122886657715\n",
            "CLF+DIST Loss: 0.32225343585014343\n",
            "CLF+DIST Loss: 0.32423147559165955\n",
            "CLF+DIST Loss: 0.3214603066444397\n",
            "CLF+DIST Loss: 0.3226654529571533\n",
            "CLF+DIST Loss: 0.32410651445388794\n",
            "CLF+DIST Loss: 0.3225519359111786\n",
            "CLF+DIST Loss: 0.3368019759654999\n",
            "CLF+DIST Loss: 0.31879493594169617\n",
            "CLF+DIST Loss: 0.3289317786693573\n",
            "CLF+DIST Loss: 0.32512375712394714\n",
            "CLF+DIST Loss: 0.32466357946395874\n",
            "CLF+DIST Loss: 0.3198153078556061\n",
            "CLF+DIST Loss: 0.32699188590049744\n",
            "CLF+DIST Loss: 0.33283594250679016\n",
            "CLF+DIST Loss: 0.325839102268219\n",
            "CLF+DIST Loss: 0.3204440176486969\n",
            "CLF+DIST Loss: 0.3261829912662506\n",
            "CLF+DIST Loss: 0.3177834451198578\n",
            "CLF+DIST Loss: 0.32195958495140076\n",
            "CLF+DIST Loss: 0.3203926086425781\n",
            "CLF+DIST Loss: 0.32786664366722107\n",
            "CLF+DIST Loss: 0.3272121548652649\n",
            "CLF+DIST Loss: 0.3231188952922821\n",
            "CLF+DIST Loss: 0.3214147090911865\n",
            "CLF+DIST Loss: 0.3208519518375397\n",
            "Train loss: 0.37588590468679156, Train accuracy: 0.11763392857142857\n",
            "Validation loss: 0.32456580655915396, Validation accuracy: 0.13392857142857142\n",
            "Target number of exemplars per class: 200\n",
            "Target total number of exemplars: 2000\n",
            "Extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 200 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 200 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... done\n",
            "Test accuracy (iCaRL): 0.143 (exemplars and training data)\n",
            "## Split 1 of run 0 ##\n",
            "Length of exemplars set: 2000\n",
            "Epoch: 1, LR: [1]\n",
            "labels index: [tensor(5), tensor(4), tensor(1), tensor(2), tensor(6), tensor(6), tensor(0), tensor(3), tensor(4), tensor(1), tensor(0), tensor(4), tensor(8), tensor(9), tensor(8), tensor(9), tensor(8), tensor(0), tensor(2), tensor(5), tensor(9)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: 1.1398067474365234\n",
            "labels index: [tensor(3), tensor(9), tensor(2), tensor(7), tensor(1), tensor(9), tensor(3), tensor(7), tensor(9), tensor(6), tensor(6), tensor(3), tensor(4), tensor(6), tensor(2), tensor(9), tensor(6), tensor(3), tensor(2), tensor(8), tensor(1)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: 52.14015197753906\n",
            "labels index: [tensor(1), tensor(5), tensor(4), tensor(1), tensor(3), tensor(7), tensor(8), tensor(5), tensor(1), tensor(6), tensor(2), tensor(8), tensor(1), tensor(9), tensor(8)]\n",
            "Targets wished size: torch.Size([15])\n",
            "Targets actual size: torch.Size([15, 64])\n",
            "Target coeffs size: torch.Size([15, 64])\n",
            "CLF+DIST Loss: 46.67966079711914\n",
            "labels index: [tensor(1), tensor(4), tensor(0), tensor(5), tensor(9), tensor(9), tensor(1), tensor(2), tensor(2), tensor(1), tensor(4), tensor(7), tensor(2), tensor(4), tensor(8)]\n",
            "Targets wished size: torch.Size([15])\n",
            "Targets actual size: torch.Size([15, 64])\n",
            "Target coeffs size: torch.Size([15, 64])\n",
            "CLF+DIST Loss: 45.50152587890625\n",
            "labels index: [tensor(1), tensor(5), tensor(7), tensor(6), tensor(2), tensor(2), tensor(9), tensor(5), tensor(2), tensor(0), tensor(6), tensor(0), tensor(6), tensor(6), tensor(1), tensor(9), tensor(7), tensor(3), tensor(2), tensor(5)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: 52.18987274169922\n",
            "labels index: [tensor(8), tensor(9), tensor(3), tensor(3), tensor(8), tensor(9), tensor(1), tensor(1), tensor(3), tensor(9), tensor(5), tensor(2), tensor(8), tensor(9), tensor(6), tensor(2), tensor(9), tensor(6), tensor(9), tensor(0), tensor(7), tensor(1), tensor(7), tensor(1)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: 47.65311050415039\n",
            "labels index: [tensor(8), tensor(2), tensor(2), tensor(7), tensor(2), tensor(0), tensor(2), tensor(5), tensor(2), tensor(6), tensor(2), tensor(4), tensor(9), tensor(1), tensor(6), tensor(5), tensor(2), tensor(1), tensor(5), tensor(8), tensor(8), tensor(7), tensor(7)]\n",
            "Targets wished size: torch.Size([23])\n",
            "Targets actual size: torch.Size([23, 64])\n",
            "Target coeffs size: torch.Size([23, 64])\n",
            "CLF+DIST Loss: 54.279022216796875\n",
            "labels index: [tensor(6), tensor(0), tensor(3), tensor(8), tensor(9), tensor(8), tensor(8), tensor(9), tensor(4), tensor(3), tensor(2), tensor(9), tensor(8), tensor(3), tensor(9), tensor(4)]\n",
            "Targets wished size: torch.Size([16])\n",
            "Targets actual size: torch.Size([16, 64])\n",
            "Target coeffs size: torch.Size([16, 64])\n",
            "CLF+DIST Loss: 51.94248962402344\n",
            "labels index: [tensor(3), tensor(3), tensor(1), tensor(8), tensor(9), tensor(8), tensor(1), tensor(4), tensor(0), tensor(7), tensor(2), tensor(7), tensor(4), tensor(7), tensor(9), tensor(1), tensor(5), tensor(1), tensor(2)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: 63586.2578125\n",
            "labels index: [tensor(6), tensor(3), tensor(9), tensor(8), tensor(7), tensor(2), tensor(1), tensor(0), tensor(4), tensor(1), tensor(3), tensor(9), tensor(2), tensor(7), tensor(7), tensor(7), tensor(9), tensor(2), tensor(4)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: inf\n",
            "labels index: [tensor(0), tensor(0), tensor(6), tensor(6), tensor(9), tensor(4), tensor(2), tensor(6), tensor(4), tensor(1), tensor(0), tensor(7), tensor(6), tensor(6), tensor(3), tensor(9), tensor(4), tensor(2), tensor(2), tensor(4), tensor(7), tensor(8), tensor(6), tensor(4), tensor(6)]\n",
            "Targets wished size: torch.Size([25])\n",
            "Targets actual size: torch.Size([25, 64])\n",
            "Target coeffs size: torch.Size([25, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(2), tensor(1), tensor(5), tensor(2), tensor(9), tensor(3), tensor(9), tensor(9), tensor(0), tensor(1), tensor(5), tensor(7), tensor(8), tensor(9), tensor(9)]\n",
            "Targets wished size: torch.Size([16])\n",
            "Targets actual size: torch.Size([16, 64])\n",
            "Target coeffs size: torch.Size([16, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(5), tensor(1), tensor(1), tensor(9), tensor(5), tensor(3), tensor(6), tensor(0), tensor(7), tensor(3), tensor(9), tensor(2), tensor(3)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(8), tensor(1), tensor(1), tensor(2), tensor(8), tensor(2), tensor(6), tensor(3), tensor(6), tensor(1), tensor(1), tensor(4), tensor(1), tensor(9), tensor(2), tensor(8), tensor(8), tensor(1), tensor(5), tensor(1), tensor(8), tensor(1)]\n",
            "Targets wished size: torch.Size([23])\n",
            "Targets actual size: torch.Size([23, 64])\n",
            "Target coeffs size: torch.Size([23, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(1), tensor(0), tensor(6), tensor(9), tensor(9), tensor(7), tensor(1), tensor(7), tensor(8), tensor(4), tensor(2), tensor(3), tensor(2), tensor(0), tensor(7), tensor(3), tensor(0), tensor(9)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(3), tensor(1), tensor(5), tensor(3), tensor(6), tensor(8), tensor(0), tensor(2), tensor(0), tensor(3), tensor(3), tensor(7), tensor(7), tensor(7), tensor(2), tensor(1), tensor(1)]\n",
            "Targets wished size: torch.Size([17])\n",
            "Targets actual size: torch.Size([17, 64])\n",
            "Target coeffs size: torch.Size([17, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(3), tensor(3), tensor(2), tensor(5), tensor(1), tensor(8), tensor(4), tensor(7), tensor(8), tensor(4), tensor(6), tensor(6), tensor(6), tensor(7), tensor(9), tensor(5), tensor(5), tensor(5), tensor(3), tensor(6), tensor(9), tensor(6), tensor(1), tensor(9)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(7), tensor(9), tensor(1), tensor(1), tensor(9), tensor(6), tensor(4), tensor(8), tensor(8), tensor(3), tensor(8), tensor(8), tensor(1), tensor(8), tensor(7), tensor(6), tensor(5), tensor(1), tensor(1), tensor(2)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(0), tensor(6), tensor(9), tensor(9), tensor(9), tensor(2), tensor(0), tensor(5), tensor(1), tensor(5), tensor(7), tensor(9), tensor(6), tensor(8), tensor(3)]\n",
            "Targets wished size: torch.Size([15])\n",
            "Targets actual size: torch.Size([15, 64])\n",
            "Target coeffs size: torch.Size([15, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(7), tensor(3), tensor(6), tensor(6), tensor(5), tensor(9), tensor(0), tensor(6), tensor(6), tensor(8), tensor(4), tensor(6), tensor(8), tensor(0), tensor(3), tensor(3), tensor(7), tensor(2), tensor(9)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(6), tensor(4), tensor(9), tensor(3), tensor(8), tensor(7), tensor(9), tensor(4), tensor(0), tensor(1), tensor(5), tensor(5), tensor(8), tensor(7), tensor(0), tensor(0), tensor(1), tensor(4), tensor(6), tensor(5), tensor(0), tensor(2), tensor(8)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(9), tensor(1), tensor(7), tensor(3), tensor(6), tensor(8), tensor(0), tensor(2), tensor(1), tensor(6), tensor(8), tensor(3), tensor(4), tensor(5), tensor(6), tensor(4)]\n",
            "Targets wished size: torch.Size([17])\n",
            "Targets actual size: torch.Size([17, 64])\n",
            "Target coeffs size: torch.Size([17, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(3), tensor(3), tensor(1), tensor(8), tensor(2), tensor(2), tensor(5), tensor(4), tensor(3), tensor(4)]\n",
            "Targets wished size: torch.Size([11])\n",
            "Targets actual size: torch.Size([11, 64])\n",
            "Target coeffs size: torch.Size([11, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(7), tensor(7), tensor(2), tensor(2), tensor(9), tensor(7), tensor(6), tensor(1), tensor(7), tensor(1), tensor(2), tensor(0), tensor(6), tensor(1), tensor(9), tensor(7), tensor(0)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(0), tensor(4), tensor(7), tensor(4), tensor(2), tensor(4), tensor(7), tensor(0), tensor(2), tensor(6), tensor(8), tensor(6), tensor(0), tensor(7), tensor(3), tensor(6), tensor(6), tensor(9), tensor(1), tensor(1), tensor(7), tensor(4), tensor(2)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(0), tensor(7), tensor(2), tensor(2), tensor(8), tensor(1), tensor(0), tensor(3), tensor(3), tensor(8), tensor(0), tensor(0), tensor(5), tensor(4), tensor(2), tensor(9), tensor(9), tensor(1), tensor(3), tensor(0), tensor(0), tensor(3)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(3), tensor(5), tensor(1), tensor(3), tensor(6), tensor(4), tensor(3), tensor(8), tensor(5), tensor(6), tensor(0), tensor(2), tensor(6), tensor(1), tensor(2), tensor(3), tensor(9), tensor(1)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(1), tensor(5), tensor(2), tensor(8), tensor(3), tensor(6), tensor(8), tensor(0), tensor(3), tensor(8), tensor(0), tensor(0), tensor(3), tensor(3), tensor(7), tensor(8), tensor(4), tensor(9), tensor(5), tensor(3), tensor(4)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(3), tensor(0), tensor(2), tensor(8), tensor(4), tensor(1), tensor(3), tensor(7), tensor(0), tensor(0), tensor(5), tensor(4), tensor(2), tensor(9)]\n",
            "Targets wished size: torch.Size([15])\n",
            "Targets actual size: torch.Size([15, 64])\n",
            "Target coeffs size: torch.Size([15, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(4), tensor(8), tensor(2), tensor(9), tensor(0), tensor(5), tensor(0), tensor(3), tensor(7), tensor(3), tensor(7), tensor(5), tensor(0), tensor(5), tensor(8), tensor(2), tensor(2), tensor(6), tensor(3), tensor(6), tensor(3), tensor(4), tensor(2)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(6), tensor(0), tensor(8), tensor(1), tensor(3), tensor(9), tensor(8), tensor(5), tensor(8), tensor(1), tensor(5), tensor(3), tensor(0)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(1), tensor(7), tensor(0), tensor(8), tensor(8), tensor(7), tensor(5), tensor(7), tensor(2), tensor(5), tensor(6), tensor(6), tensor(2), tensor(8), tensor(8), tensor(4), tensor(5), tensor(3), tensor(6), tensor(5)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(4), tensor(9), tensor(5), tensor(8), tensor(8), tensor(8), tensor(0), tensor(9), tensor(4), tensor(3), tensor(9), tensor(1), tensor(3), tensor(2), tensor(6), tensor(6), tensor(5), tensor(8), tensor(3), tensor(7), tensor(8)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(0), tensor(0), tensor(5), tensor(9), tensor(7), tensor(6), tensor(2), tensor(3), tensor(9), tensor(7), tensor(1), tensor(5), tensor(8), tensor(2), tensor(3), tensor(4), tensor(0), tensor(8), tensor(2), tensor(1), tensor(5)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(5), tensor(6), tensor(4), tensor(0), tensor(9), tensor(7), tensor(3), tensor(7), tensor(2), tensor(8), tensor(3), tensor(6), tensor(8), tensor(7), tensor(9), tensor(3), tensor(4), tensor(5), tensor(9), tensor(9), tensor(5)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(7), tensor(8), tensor(7), tensor(0), tensor(4), tensor(8), tensor(5), tensor(5), tensor(9), tensor(4), tensor(4), tensor(1), tensor(5), tensor(9), tensor(0), tensor(4), tensor(8)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(3), tensor(3), tensor(3), tensor(1), tensor(1), tensor(7), tensor(1), tensor(8), tensor(6), tensor(4), tensor(7), tensor(2), tensor(5), tensor(1), tensor(6), tensor(8), tensor(8), tensor(1), tensor(2), tensor(3), tensor(7)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(7), tensor(9), tensor(4), tensor(2), tensor(1), tensor(6), tensor(4), tensor(9), tensor(2), tensor(4), tensor(0), tensor(1), tensor(6), tensor(1), tensor(8), tensor(6), tensor(5), tensor(3), tensor(1)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(2), tensor(5), tensor(4), tensor(3), tensor(9), tensor(9), tensor(6), tensor(3), tensor(9), tensor(3), tensor(4), tensor(6), tensor(9), tensor(0), tensor(2), tensor(8), tensor(2), tensor(5), tensor(2), tensor(5)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(6), tensor(6), tensor(9), tensor(0), tensor(3), tensor(6), tensor(5), tensor(6), tensor(7), tensor(6), tensor(6), tensor(0), tensor(5), tensor(7), tensor(9), tensor(2), tensor(6), tensor(0), tensor(1), tensor(9), tensor(1), tensor(6), tensor(7), tensor(4), tensor(9), tensor(0)]\n",
            "Targets wished size: torch.Size([27])\n",
            "Targets actual size: torch.Size([27, 64])\n",
            "Target coeffs size: torch.Size([27, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(0), tensor(7), tensor(3), tensor(2), tensor(3), tensor(7), tensor(1), tensor(4), tensor(5), tensor(8), tensor(5), tensor(9), tensor(0), tensor(5), tensor(5), tensor(9), tensor(1), tensor(8), tensor(6), tensor(5), tensor(9)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(6), tensor(2), tensor(7), tensor(4), tensor(5), tensor(1), tensor(8), tensor(8), tensor(6), tensor(5), tensor(0), tensor(8), tensor(3), tensor(9), tensor(3), tensor(3), tensor(8), tensor(7), tensor(2), tensor(2), tensor(0), tensor(4), tensor(2), tensor(2), tensor(2)]\n",
            "Targets wished size: torch.Size([26])\n",
            "Targets actual size: torch.Size([26, 64])\n",
            "Target coeffs size: torch.Size([26, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(0), tensor(5), tensor(5), tensor(2), tensor(9), tensor(4), tensor(6), tensor(9), tensor(6), tensor(5), tensor(2), tensor(1), tensor(1), tensor(1), tensor(8), tensor(2), tensor(9), tensor(5), tensor(0)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(6), tensor(2), tensor(9), tensor(8), tensor(4), tensor(1), tensor(7), tensor(1), tensor(1), tensor(8), tensor(8), tensor(2), tensor(9), tensor(1), tensor(9), tensor(0), tensor(7), tensor(9), tensor(9), tensor(5), tensor(2), tensor(6), tensor(0), tensor(0), tensor(9), tensor(5)]\n",
            "Targets wished size: torch.Size([27])\n",
            "Targets actual size: torch.Size([27, 64])\n",
            "Target coeffs size: torch.Size([27, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(6), tensor(5), tensor(3), tensor(6), tensor(7), tensor(3), tensor(0), tensor(8), tensor(3), tensor(6), tensor(7), tensor(3), tensor(6)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(2), tensor(8), tensor(9), tensor(9), tensor(6), tensor(7), tensor(7), tensor(9), tensor(7), tensor(3), tensor(6), tensor(9), tensor(0), tensor(8), tensor(7), tensor(7), tensor(0), tensor(8), tensor(5), tensor(1), tensor(9), tensor(4), tensor(7)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(3), tensor(2), tensor(1), tensor(1), tensor(3), tensor(6), tensor(7), tensor(6), tensor(6), tensor(6), tensor(1), tensor(5), tensor(0), tensor(3), tensor(8), tensor(4), tensor(9), tensor(5), tensor(0), tensor(7), tensor(0), tensor(3), tensor(3), tensor(3), tensor(7)]\n",
            "Targets wished size: torch.Size([26])\n",
            "Targets actual size: torch.Size([26, 64])\n",
            "Target coeffs size: torch.Size([26, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(9), tensor(5), tensor(7), tensor(8), tensor(5), tensor(4), tensor(4), tensor(3), tensor(0), tensor(1), tensor(1), tensor(4), tensor(3), tensor(5), tensor(0)]\n",
            "Targets wished size: torch.Size([16])\n",
            "Targets actual size: torch.Size([16, 64])\n",
            "Target coeffs size: torch.Size([16, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(3), tensor(1), tensor(8), tensor(3), tensor(1), tensor(4), tensor(8), tensor(2), tensor(1), tensor(2), tensor(2), tensor(9), tensor(9)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(4), tensor(1), tensor(9), tensor(5), tensor(8), tensor(9), tensor(0), tensor(5), tensor(9), tensor(0), tensor(7), tensor(8), tensor(7), tensor(7), tensor(7), tensor(5), tensor(0), tensor(5)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(4), tensor(3), tensor(7), tensor(3), tensor(9), tensor(4), tensor(3), tensor(0), tensor(3), tensor(3), tensor(7), tensor(9), tensor(6), tensor(5), tensor(7), tensor(3), tensor(4), tensor(4), tensor(0), tensor(8), tensor(5)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(2), tensor(5), tensor(6), tensor(1), tensor(2), tensor(8), tensor(9), tensor(6), tensor(9), tensor(9), tensor(9), tensor(5), tensor(5), tensor(8), tensor(4)]\n",
            "Targets wished size: torch.Size([16])\n",
            "Targets actual size: torch.Size([16, 64])\n",
            "Target coeffs size: torch.Size([16, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(0), tensor(5), tensor(3), tensor(3), tensor(8), tensor(1), tensor(7), tensor(2), tensor(7), tensor(0), tensor(3), tensor(8), tensor(0), tensor(7), tensor(8), tensor(8), tensor(4), tensor(4), tensor(0)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(5), tensor(7), tensor(6), tensor(3), tensor(8), tensor(7), tensor(8), tensor(8), tensor(7), tensor(8), tensor(5), tensor(0), tensor(8), tensor(9), tensor(6), tensor(5), tensor(7), tensor(2), tensor(1), tensor(6), tensor(4), tensor(4), tensor(1), tensor(6), tensor(1)]\n",
            "Targets wished size: torch.Size([26])\n",
            "Targets actual size: torch.Size([26, 64])\n",
            "Target coeffs size: torch.Size([26, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(7), tensor(0), tensor(2), tensor(1), tensor(5), tensor(9), tensor(5), tensor(4), tensor(2), tensor(1), tensor(0), tensor(0), tensor(0), tensor(2), tensor(1), tensor(0), tensor(4), tensor(5), tensor(7)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(0), tensor(4), tensor(7), tensor(8), tensor(0), tensor(0), tensor(6), tensor(4), tensor(2), tensor(4), tensor(7), tensor(0), tensor(0), tensor(8), tensor(3), tensor(5), tensor(9), tensor(2)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(0), tensor(1), tensor(5), tensor(2), tensor(8), tensor(7), tensor(9), tensor(3), tensor(8), tensor(6), tensor(4), tensor(2), tensor(8), tensor(4), tensor(3), tensor(0), tensor(6), tensor(1), tensor(2), tensor(5), tensor(7)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(9), tensor(2), tensor(9), tensor(3), tensor(3), tensor(9), tensor(4), tensor(9), tensor(3), tensor(4), tensor(5), tensor(2), tensor(0), tensor(9), tensor(3), tensor(5), tensor(0), tensor(5), tensor(4)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(1), tensor(2), tensor(0), tensor(7), tensor(1), tensor(0), tensor(6), tensor(6), tensor(3), tensor(5), tensor(6), tensor(5), tensor(3), tensor(7), tensor(4), tensor(4), tensor(0), tensor(1)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(2), tensor(1), tensor(2), tensor(7), tensor(5), tensor(7), tensor(3), tensor(2), tensor(8), tensor(7), tensor(0), tensor(6), tensor(3), tensor(0), tensor(7), tensor(3), tensor(7), tensor(6), tensor(7)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(6), tensor(9), tensor(2), tensor(2), tensor(3), tensor(8), tensor(4), tensor(8), tensor(2), tensor(4), tensor(4), tensor(0), tensor(3), tensor(7), tensor(2), tensor(7), tensor(6), tensor(5), tensor(6)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(0), tensor(4), tensor(1), tensor(3), tensor(7), tensor(7), tensor(0), tensor(5), tensor(8), tensor(7), tensor(5), tensor(3), tensor(3), tensor(8), tensor(8), tensor(4), tensor(5), tensor(4), tensor(3)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(4), tensor(5), tensor(9), tensor(7), tensor(6), tensor(9), tensor(4), tensor(2), tensor(7), tensor(4), tensor(1), tensor(9), tensor(1), tensor(1), tensor(4), tensor(8), tensor(7), tensor(7), tensor(2), tensor(2), tensor(4), tensor(8), tensor(4), tensor(5)]\n",
            "Targets wished size: torch.Size([25])\n",
            "Targets actual size: torch.Size([25, 64])\n",
            "Target coeffs size: torch.Size([25, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(2), tensor(8), tensor(2), tensor(4), tensor(9), tensor(6), tensor(6), tensor(3), tensor(2), tensor(3)]\n",
            "Targets wished size: torch.Size([11])\n",
            "Targets actual size: torch.Size([11, 64])\n",
            "Target coeffs size: torch.Size([11, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(9), tensor(1), tensor(3), tensor(7), tensor(5), tensor(5), tensor(5), tensor(8), tensor(1), tensor(7), tensor(9), tensor(6), tensor(4), tensor(5), tensor(8), tensor(3), tensor(3)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(9), tensor(7), tensor(8), tensor(6), tensor(3), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(5), tensor(5), tensor(4), tensor(7), tensor(9), tensor(3), tensor(2)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(1), tensor(6), tensor(4), tensor(9), tensor(9), tensor(7), tensor(1), tensor(0), tensor(6), tensor(3), tensor(2), tensor(7), tensor(0), tensor(5), tensor(4), tensor(3), tensor(1), tensor(5), tensor(6)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(4), tensor(9), tensor(6), tensor(4), tensor(5), tensor(5), tensor(6), tensor(4), tensor(1), tensor(5), tensor(0), tensor(8), tensor(3), tensor(1), tensor(6), tensor(9), tensor(1), tensor(3), tensor(4), tensor(2), tensor(5)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(4), tensor(9), tensor(2), tensor(1), tensor(2), tensor(6), tensor(5), tensor(0), tensor(2), tensor(2), tensor(2), tensor(0), tensor(6), tensor(9), tensor(5), tensor(3), tensor(4)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(3), tensor(7), tensor(7), tensor(3), tensor(5), tensor(3), tensor(7), tensor(4), tensor(9), tensor(8), tensor(7), tensor(3), tensor(2)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(6), tensor(0), tensor(0), tensor(2), tensor(3), tensor(7), tensor(1), tensor(5), tensor(3), tensor(2), tensor(0), tensor(8), tensor(9), tensor(9), tensor(2), tensor(6), tensor(3), tensor(9), tensor(4)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(5), tensor(6), tensor(1), tensor(8), tensor(6), tensor(6), tensor(0), tensor(8), tensor(4), tensor(7), tensor(2), tensor(4), tensor(0), tensor(3), tensor(4), tensor(0), tensor(1), tensor(3), tensor(2)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(3), tensor(7), tensor(3), tensor(4), tensor(4), tensor(0), tensor(5), tensor(8), tensor(9), tensor(4), tensor(9), tensor(4), tensor(7), tensor(8), tensor(9), tensor(8), tensor(8), tensor(4)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(3), tensor(4), tensor(0), tensor(0), tensor(5), tensor(0), tensor(0), tensor(5), tensor(1), tensor(5), tensor(4), tensor(0), tensor(2), tensor(6), tensor(0), tensor(7), tensor(9), tensor(3), tensor(6)]\n",
            "Targets wished size: torch.Size([19])\n",
            "Targets actual size: torch.Size([19, 64])\n",
            "Target coeffs size: torch.Size([19, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(7), tensor(6), tensor(8), tensor(6), tensor(4), tensor(6), tensor(0), tensor(4), tensor(2), tensor(2), tensor(3), tensor(1), tensor(1), tensor(4), tensor(3), tensor(4), tensor(9), tensor(7), tensor(5), tensor(1), tensor(6)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(6), tensor(1), tensor(2), tensor(4), tensor(8), tensor(5), tensor(0), tensor(6), tensor(5), tensor(3), tensor(6), tensor(4), tensor(9), tensor(8), tensor(6), tensor(3), tensor(8)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(5), tensor(6), tensor(0), tensor(7), tensor(7), tensor(9), tensor(3), tensor(1), tensor(9), tensor(0), tensor(3), tensor(8), tensor(7), tensor(5), tensor(8), tensor(1), tensor(2), tensor(5), tensor(8), tensor(1), tensor(7)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(8), tensor(3), tensor(0), tensor(4), tensor(5), tensor(4), tensor(6), tensor(0), tensor(8), tensor(4), tensor(7), tensor(4), tensor(8), tensor(1), tensor(9), tensor(0), tensor(8), tensor(4), tensor(0)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(7), tensor(5), tensor(6), tensor(1), tensor(5), tensor(4), tensor(3), tensor(7), tensor(1), tensor(6)]\n",
            "Targets wished size: torch.Size([11])\n",
            "Targets actual size: torch.Size([11, 64])\n",
            "Target coeffs size: torch.Size([11, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(4), tensor(7), tensor(0), tensor(4), tensor(6), tensor(5), tensor(2), tensor(0), tensor(6), tensor(1), tensor(5), tensor(3), tensor(7), tensor(9), tensor(4), tensor(2), tensor(8), tensor(8), tensor(4), tensor(2), tensor(9), tensor(0), tensor(6), tensor(7), tensor(7), tensor(4)]\n",
            "Targets wished size: torch.Size([27])\n",
            "Targets actual size: torch.Size([27, 64])\n",
            "Target coeffs size: torch.Size([27, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(7), tensor(8), tensor(7), tensor(0), tensor(4), tensor(4), tensor(3), tensor(6), tensor(0), tensor(6), tensor(5), tensor(2), tensor(9), tensor(2), tensor(9), tensor(3), tensor(0), tensor(3), tensor(4)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(6), tensor(8), tensor(6), tensor(4), tensor(9), tensor(3), tensor(8), tensor(5), tensor(6), tensor(9), tensor(6), tensor(5), tensor(5), tensor(4), tensor(7), tensor(3), tensor(5)]\n",
            "Targets wished size: torch.Size([17])\n",
            "Targets actual size: torch.Size([17, 64])\n",
            "Target coeffs size: torch.Size([17, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(0), tensor(9), tensor(4), tensor(9), tensor(6), tensor(5), tensor(7), tensor(5), tensor(1), tensor(0), tensor(7), tensor(1), tensor(7), tensor(0), tensor(0), tensor(8), tensor(7), tensor(8), tensor(3), tensor(4)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(8), tensor(4), tensor(7), tensor(0), tensor(0), tensor(5), tensor(5), tensor(0), tensor(4), tensor(1), tensor(5), tensor(7), tensor(1), tensor(6), tensor(1), tensor(2), tensor(6), tensor(0), tensor(7), tensor(1), tensor(3)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(0), tensor(9), tensor(8), tensor(4), tensor(4), tensor(1), tensor(2), tensor(4), tensor(4), tensor(1), tensor(8), tensor(2), tensor(2), tensor(2), tensor(7), tensor(7), tensor(9), tensor(4), tensor(3)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(6), tensor(8), tensor(6), tensor(1), tensor(8), tensor(0), tensor(5), tensor(1), tensor(8), tensor(4), tensor(1), tensor(1), tensor(4), tensor(9), tensor(7)]\n",
            "Targets wished size: torch.Size([16])\n",
            "Targets actual size: torch.Size([16, 64])\n",
            "Target coeffs size: torch.Size([16, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(8), tensor(2), tensor(0), tensor(8), tensor(8), tensor(6), tensor(6), tensor(9), tensor(5), tensor(3), tensor(1), tensor(7), tensor(6), tensor(3), tensor(0), tensor(6)]\n",
            "Targets wished size: torch.Size([17])\n",
            "Targets actual size: torch.Size([17, 64])\n",
            "Target coeffs size: torch.Size([17, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(8), tensor(1), tensor(7), tensor(3), tensor(1), tensor(2), tensor(5), tensor(1), tensor(9), tensor(4), tensor(3), tensor(0), tensor(5), tensor(4), tensor(5), tensor(0), tensor(4), tensor(1), tensor(0), tensor(4), tensor(6)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(5), tensor(8), tensor(9), tensor(0), tensor(7), tensor(7), tensor(0), tensor(2), tensor(3), tensor(8), tensor(8), tensor(4), tensor(1), tensor(7), tensor(5), tensor(0), tensor(9)]\n",
            "Targets wished size: torch.Size([17])\n",
            "Targets actual size: torch.Size([17, 64])\n",
            "Target coeffs size: torch.Size([17, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(4), tensor(0), tensor(4), tensor(8), tensor(4), tensor(6), tensor(5), tensor(3), tensor(9), tensor(8), tensor(0), tensor(4), tensor(5), tensor(8), tensor(8), tensor(5), tensor(5), tensor(1), tensor(9), tensor(3), tensor(5), tensor(2), tensor(0), tensor(5), tensor(8)]\n",
            "Targets wished size: torch.Size([26])\n",
            "Targets actual size: torch.Size([26, 64])\n",
            "Target coeffs size: torch.Size([26, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(1), tensor(4), tensor(0), tensor(1), tensor(1), tensor(2), tensor(7), tensor(2), tensor(8), tensor(7), tensor(8), tensor(9), tensor(3), tensor(7), tensor(1), tensor(9), tensor(7), tensor(3), tensor(4)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(8), tensor(4), tensor(5), tensor(0), tensor(1), tensor(2), tensor(1), tensor(8), tensor(1), tensor(4), tensor(7), tensor(2), tensor(4), tensor(2), tensor(0)]\n",
            "Targets wished size: torch.Size([16])\n",
            "Targets actual size: torch.Size([16, 64])\n",
            "Target coeffs size: torch.Size([16, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(2), tensor(3), tensor(1), tensor(0), tensor(3), tensor(0), tensor(0), tensor(5), tensor(0), tensor(0), tensor(4), tensor(8), tensor(7), tensor(2), tensor(1), tensor(3), tensor(0)]\n",
            "Targets wished size: torch.Size([18])\n",
            "Targets actual size: torch.Size([18, 64])\n",
            "Target coeffs size: torch.Size([18, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(4), tensor(4), tensor(6), tensor(4), tensor(0), tensor(0), tensor(0), tensor(9), tensor(5), tensor(1), tensor(2), tensor(2), tensor(7), tensor(4), tensor(4), tensor(0), tensor(2), tensor(3), tensor(3), tensor(6)]\n",
            "Targets wished size: torch.Size([20])\n",
            "Targets actual size: torch.Size([20, 64])\n",
            "Target coeffs size: torch.Size([20, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(7), tensor(3), tensor(9), tensor(4), tensor(7), tensor(4), tensor(8), tensor(0), tensor(9), tensor(6), tensor(6)]\n",
            "Targets wished size: torch.Size([11])\n",
            "Targets actual size: torch.Size([11, 64])\n",
            "Target coeffs size: torch.Size([11, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(6), tensor(5), tensor(0), tensor(2), tensor(6), tensor(5), tensor(0), tensor(8), tensor(0), tensor(1), tensor(7), tensor(5), tensor(9), tensor(8), tensor(0), tensor(6), tensor(3), tensor(4), tensor(0), tensor(0), tensor(0), tensor(8), tensor(1)]\n",
            "Targets wished size: torch.Size([24])\n",
            "Targets actual size: torch.Size([24, 64])\n",
            "Target coeffs size: torch.Size([24, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(1), tensor(0), tensor(3), tensor(9), tensor(9), tensor(2), tensor(7), tensor(5), tensor(9), tensor(3), tensor(5), tensor(2), tensor(2), tensor(7)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(2), tensor(2), tensor(5), tensor(9), tensor(0), tensor(0), tensor(9), tensor(9), tensor(0), tensor(5), tensor(3), tensor(8), tensor(3), tensor(3), tensor(2), tensor(6), tensor(2), tensor(7), tensor(4), tensor(5), tensor(6), tensor(8), tensor(6)]\n",
            "Targets wished size: torch.Size([23])\n",
            "Targets actual size: torch.Size([23, 64])\n",
            "Target coeffs size: torch.Size([23, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(5), tensor(1), tensor(9), tensor(2), tensor(4), tensor(2), tensor(8), tensor(6), tensor(5), tensor(2), tensor(5), tensor(2), tensor(5), tensor(2), tensor(2), tensor(7), tensor(4), tensor(3), tensor(2), tensor(5)]\n",
            "Targets wished size: torch.Size([21])\n",
            "Targets actual size: torch.Size([21, 64])\n",
            "Target coeffs size: torch.Size([21, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(3), tensor(3), tensor(9), tensor(0), tensor(3), tensor(2), tensor(4), tensor(1), tensor(6), tensor(7), tensor(1), tensor(6), tensor(4), tensor(3)]\n",
            "Targets wished size: torch.Size([14])\n",
            "Targets actual size: torch.Size([14, 64])\n",
            "Target coeffs size: torch.Size([14, 64])\n",
            "CLF+DIST Loss: nan\n",
            "labels index: [tensor(9), tensor(7), tensor(1), tensor(6), tensor(5), tensor(7), tensor(1), tensor(9), tensor(5), tensor(5), tensor(7), tensor(4), tensor(2), tensor(6), tensor(9), tensor(2), tensor(3), tensor(5), tensor(5), tensor(1), tensor(5), tensor(0)]\n",
            "Targets wished size: torch.Size([22])\n",
            "Targets actual size: torch.Size([22, 64])\n",
            "Target coeffs size: torch.Size([22, 64])\n",
            "CLF+DIST Loss: nan\n",
            "Train loss: nan, Train accuracy: 0.03186881188118812\n",
            "Validation loss: nan, Validation accuracy: 0.0\n",
            "Target number of exemplars per class: 100\n",
            "Target total number of exemplars: 2000\n",
            "Extracting exemplars from class 0 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 1 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 2 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 3 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 4 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 5 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 6 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 7 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 8 of current split... Extracted 100 exemplars.\n",
            "Extracting exemplars from class 9 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 0 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 1 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 2 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 3 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 4 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 5 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 6 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 7 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 8 of current split... Extracted 100 exemplars.\n",
            "Randomly extracting exemplars from class 9 of current split... Extracted 100 exemplars.\n",
            "Random:\n",
            "Computing mean of exemplars... "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UdGY1g1bT5m",
        "colab_type": "text"
      },
      "source": [
        "### Representation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_7qZD0eCjst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b6aeac6-38ca-4012-c4ce-e1213e2dd827"
      },
      "source": [
        "icarl.prototypes[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5.0445e-05, 3.8152e-01, 3.1539e-03, 1.5910e-04, 2.0014e-05, 5.0570e-04,\n",
              "         0.0000e+00, 2.1256e-06, 0.0000e+00, 0.0000e+00, 9.1653e-03, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7418e-01, 0.0000e+00,\n",
              "         0.0000e+00, 4.4615e-01, 1.0522e-01, 5.4034e-03, 0.0000e+00, 5.5331e-05,\n",
              "         0.0000e+00, 6.0695e-04, 0.0000e+00, 1.1492e-06, 0.0000e+00, 5.2616e-02,\n",
              "         0.0000e+00, 0.0000e+00, 5.5894e-03, 0.0000e+00, 0.0000e+00, 4.2033e-01,\n",
              "         2.5670e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         7.0851e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0343e-03, 1.5376e-05,\n",
              "         9.1762e-03, 9.0095e-06, 4.7246e-04, 0.0000e+00, 3.9589e-01, 3.4641e-01,\n",
              "         0.0000e+00, 4.6070e-03, 0.0000e+00, 3.0157e-06, 2.3931e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 1.5184e-04, 3.3571e-01],\n",
              "        [3.5812e-04, 4.0317e-01, 1.8222e-03, 6.8645e-05, 8.8625e-06, 3.3485e-04,\n",
              "         0.0000e+00, 1.0720e-06, 0.0000e+00, 0.0000e+00, 8.6401e-03, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9001e-01, 3.2178e-06,\n",
              "         0.0000e+00, 4.6973e-01, 1.0126e-01, 2.3846e-03, 0.0000e+00, 1.0129e-04,\n",
              "         0.0000e+00, 5.1460e-04, 0.0000e+00, 5.3354e-06, 0.0000e+00, 2.4128e-02,\n",
              "         0.0000e+00, 0.0000e+00, 2.3502e-03, 0.0000e+00, 0.0000e+00, 4.3415e-01,\n",
              "         4.5265e-05, 0.0000e+00, 1.2037e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         3.8209e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6966e-04, 1.2372e-04,\n",
              "         9.8822e-03, 7.3337e-06, 6.5733e-04, 0.0000e+00, 2.8960e-01, 3.4891e-01,\n",
              "         0.0000e+00, 2.7239e-03, 0.0000e+00, 2.2689e-05, 1.6379e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 4.0074e-04, 3.5717e-01],\n",
              "        [4.6452e-05, 2.9054e-01, 3.5900e-03, 4.6813e-04, 3.9832e-05, 7.6028e-04,\n",
              "         0.0000e+00, 2.6050e-05, 0.0000e+00, 0.0000e+00, 2.1964e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0903e-01, 0.0000e+00,\n",
              "         0.0000e+00, 3.4530e-01, 1.7524e-01, 1.0325e-02, 0.0000e+00, 3.5047e-05,\n",
              "         0.0000e+00, 1.2638e-03, 0.0000e+00, 1.7434e-06, 0.0000e+00, 9.5162e-02,\n",
              "         0.0000e+00, 0.0000e+00, 1.4907e-02, 0.0000e+00, 0.0000e+00, 3.2514e-01,\n",
              "         4.5764e-06, 0.0000e+00, 3.5192e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         2.2279e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3391e-03, 1.6057e-05,\n",
              "         1.2967e-02, 1.5270e-06, 9.2558e-04, 0.0000e+00, 6.7175e-01, 3.0313e-01,\n",
              "         0.0000e+00, 7.4090e-03, 0.0000e+00, 5.6458e-06, 7.9706e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 1.3435e-04, 2.5097e-01],\n",
              "        [1.2158e-03, 2.6509e-01, 4.9413e-03, 2.5719e-04, 1.8162e-05, 3.6530e-04,\n",
              "         0.0000e+00, 7.4490e-07, 0.0000e+00, 0.0000e+00, 1.7454e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 7.2732e-07, 0.0000e+00, 1.9214e-01, 0.0000e+00,\n",
              "         0.0000e+00, 3.1465e-01, 1.8242e-01, 6.5796e-03, 0.0000e+00, 2.4031e-04,\n",
              "         0.0000e+00, 4.4816e-04, 0.0000e+00, 1.2357e-05, 0.0000e+00, 9.6671e-02,\n",
              "         0.0000e+00, 0.0000e+00, 8.2881e-03, 0.0000e+00, 0.0000e+00, 2.9887e-01,\n",
              "         6.2852e-04, 0.0000e+00, 1.7875e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         3.1345e-05, 0.0000e+00, 0.0000e+00, 1.7392e-06, 4.7265e-03, 3.1158e-04,\n",
              "         9.5523e-03, 6.8775e-06, 9.4857e-04, 0.0000e+00, 7.3061e-01, 2.7186e-01,\n",
              "         0.0000e+00, 9.7230e-03, 0.0000e+00, 1.5272e-04, 5.5916e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 1.1618e-03, 2.3132e-01],\n",
              "        [1.3206e-04, 3.2491e-01, 4.7149e-03, 1.1271e-04, 3.1008e-05, 5.0655e-04,\n",
              "         0.0000e+00, 1.4651e-05, 0.0000e+00, 0.0000e+00, 1.6024e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3442e-01, 4.5038e-07,\n",
              "         0.0000e+00, 3.7944e-01, 1.6400e-01, 5.9649e-03, 0.0000e+00, 2.9563e-05,\n",
              "         0.0000e+00, 6.4632e-04, 0.0000e+00, 1.3720e-06, 0.0000e+00, 1.0029e-01,\n",
              "         0.0000e+00, 0.0000e+00, 7.6118e-03, 0.0000e+00, 0.0000e+00, 3.5751e-01,\n",
              "         6.8991e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         3.7967e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.3712e-03, 3.6632e-05,\n",
              "         1.2882e-02, 1.7031e-05, 1.0957e-03, 0.0000e+00, 6.0048e-01, 2.9581e-01,\n",
              "         0.0000e+00, 1.1571e-02, 0.0000e+00, 3.9745e-05, 2.6565e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 3.2585e-04, 2.8634e-01],\n",
              "        [1.6032e-06, 1.9045e-01, 7.2212e-03, 6.6007e-05, 2.5479e-05, 5.3584e-05,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7908e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3962e-01, 0.0000e+00,\n",
              "         0.0000e+00, 2.2187e-01, 2.0101e-01, 7.3308e-03, 0.0000e+00, 9.7553e-06,\n",
              "         0.0000e+00, 6.3774e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6368e-01,\n",
              "         0.0000e+00, 0.0000e+00, 1.1822e-02, 0.0000e+00, 0.0000e+00, 2.0816e-01,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         8.7590e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.8367e-03, 0.0000e+00,\n",
              "         9.2275e-03, 0.0000e+00, 2.6144e-03, 0.0000e+00, 8.5082e-01, 1.7443e-01,\n",
              "         0.0000e+00, 1.9472e-02, 0.0000e+00, 1.3721e-05, 1.0259e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 4.5693e-05, 1.6928e-01],\n",
              "        [1.3061e-04, 3.9041e-01, 4.1794e-03, 2.3555e-04, 2.5260e-05, 3.3689e-04,\n",
              "         0.0000e+00, 5.4778e-05, 0.0000e+00, 0.0000e+00, 1.0914e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7761e-01, 0.0000e+00,\n",
              "         0.0000e+00, 4.5255e-01, 1.0250e-01, 3.6186e-03, 0.0000e+00, 3.0293e-04,\n",
              "         0.0000e+00, 4.0153e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.2217e-02,\n",
              "         0.0000e+00, 0.0000e+00, 9.9597e-03, 0.0000e+00, 0.0000e+00, 4.2953e-01,\n",
              "         5.5216e-05, 0.0000e+00, 7.7222e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         1.6861e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.8385e-04, 4.0135e-06,\n",
              "         1.0133e-02, 8.2814e-06, 9.2687e-04, 0.0000e+00, 3.5627e-01, 3.4955e-01,\n",
              "         0.0000e+00, 8.6308e-03, 0.0000e+00, 5.6766e-06, 1.7512e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 2.7203e-04, 3.4248e-01],\n",
              "        [5.2710e-05, 4.0994e-01, 2.6291e-03, 2.3644e-04, 3.3217e-06, 6.1756e-04,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.9122e-03, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9556e-01, 0.0000e+00,\n",
              "         0.0000e+00, 4.7688e-01, 7.9113e-02, 3.4077e-03, 0.0000e+00, 7.7888e-05,\n",
              "         0.0000e+00, 7.7461e-04, 0.0000e+00, 1.0893e-05, 0.0000e+00, 2.4805e-02,\n",
              "         0.0000e+00, 0.0000e+00, 3.7836e-03, 0.0000e+00, 0.0000e+00, 4.4056e-01,\n",
              "         3.9891e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         1.1510e-04, 0.0000e+00, 0.0000e+00, 3.5139e-06, 5.5495e-04, 2.2154e-05,\n",
              "         7.9517e-03, 8.1262e-06, 5.0617e-04, 0.0000e+00, 2.4035e-01, 3.5552e-01,\n",
              "         0.0000e+00, 2.5830e-03, 0.0000e+00, 2.3228e-06, 1.7381e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 1.7787e-04, 3.6311e-01],\n",
              "        [3.5388e-05, 3.7097e-01, 2.3374e-03, 9.5204e-05, 1.1179e-06, 1.5018e-04,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.3377e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.6653e-01, 2.0567e-06,\n",
              "         0.0000e+00, 4.3042e-01, 1.2918e-01, 4.6450e-03, 0.0000e+00, 7.1044e-05,\n",
              "         0.0000e+00, 3.2484e-04, 0.0000e+00, 3.4617e-06, 0.0000e+00, 4.9615e-02,\n",
              "         0.0000e+00, 0.0000e+00, 6.4363e-03, 0.0000e+00, 0.0000e+00, 4.0330e-01,\n",
              "         2.4361e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         2.1689e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9773e-04, 3.1046e-05,\n",
              "         1.1880e-02, 1.8539e-05, 9.3978e-04, 0.0000e+00, 4.5557e-01, 3.3018e-01,\n",
              "         0.0000e+00, 5.6941e-03, 0.0000e+00, 0.0000e+00, 2.1329e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 1.1032e-04, 3.2756e-01],\n",
              "        [4.1100e-05, 4.0219e-01, 1.8282e-03, 9.0389e-05, 0.0000e+00, 2.7291e-04,\n",
              "         0.0000e+00, 9.8440e-06, 0.0000e+00, 0.0000e+00, 1.0924e-02, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9064e-01, 0.0000e+00,\n",
              "         0.0000e+00, 4.6822e-01, 1.0230e-01, 4.0564e-03, 0.0000e+00, 6.0292e-05,\n",
              "         0.0000e+00, 4.3541e-04, 0.0000e+00, 3.5608e-06, 0.0000e+00, 2.1960e-02,\n",
              "         0.0000e+00, 0.0000e+00, 3.4156e-03, 0.0000e+00, 0.0000e+00, 4.3155e-01,\n",
              "         2.7953e-05, 0.0000e+00, 5.2217e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
              "         1.0174e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0958e-04, 1.6105e-05,\n",
              "         9.7127e-03, 1.4125e-06, 7.9520e-04, 0.0000e+00, 3.0100e-01, 3.4500e-01,\n",
              "         0.0000e+00, 2.2967e-03, 0.0000e+00, 0.0000e+00, 1.5072e-04, 0.0000e+00,\n",
              "         0.0000e+00, 0.0000e+00, 1.1570e-04, 3.5704e-01]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyQt2ZjQCm_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "c05830de-4571-4bf5-ea94-d2dedbf9d240"
      },
      "source": [
        "print(icarl.prototypes[0][0])\n",
        "torch.sum(icarl.prototypes[0][0])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.0445e-05, 3.8152e-01, 3.1539e-03, 1.5910e-04, 2.0014e-05, 5.0570e-04,\n",
            "        0.0000e+00, 2.1256e-06, 0.0000e+00, 0.0000e+00, 9.1653e-03, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7418e-01, 0.0000e+00,\n",
            "        0.0000e+00, 4.4615e-01, 1.0522e-01, 5.4034e-03, 0.0000e+00, 5.5331e-05,\n",
            "        0.0000e+00, 6.0695e-04, 0.0000e+00, 1.1492e-06, 0.0000e+00, 5.2616e-02,\n",
            "        0.0000e+00, 0.0000e+00, 5.5894e-03, 0.0000e+00, 0.0000e+00, 4.2033e-01,\n",
            "        2.5670e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "        7.0851e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0343e-03, 1.5376e-05,\n",
            "        9.1762e-03, 9.0095e-06, 4.7246e-04, 0.0000e+00, 3.9589e-01, 3.4641e-01,\n",
            "        0.0000e+00, 4.6070e-03, 0.0000e+00, 3.0157e-06, 2.3931e-04, 0.0000e+00,\n",
            "        0.0000e+00, 0.0000e+00, 1.5184e-04, 3.3571e-01], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.7985, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JW4cn-PjVHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "icarl.features_coeffs # MSE, alpha = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxZG0xLKm2Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "icarl.rep_distance  # MSE, alpha = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkawPkrgYmWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "d828fdfa-6452-4351-855f-bbf0637d368c"
      },
      "source": [
        "icarl.rep_distance  # cosine embeddings, weighted but without alpha"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {0: tensor(0.0003, device='cuda:0'),\n",
              "  1: tensor(0.0001, device='cuda:0'),\n",
              "  2: tensor(0.0003, device='cuda:0'),\n",
              "  3: tensor(0.0003, device='cuda:0'),\n",
              "  4: tensor(7.2254e-05, device='cuda:0'),\n",
              "  5: tensor(0.0002, device='cuda:0'),\n",
              "  6: tensor(0.0006, device='cuda:0'),\n",
              "  7: tensor(0.0005, device='cuda:0'),\n",
              "  8: tensor(0.0002, device='cuda:0'),\n",
              "  9: tensor(0.0003, device='cuda:0')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LAhOmDuTBoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "33c77711-035a-4dcf-e622-a0369dcc7b37"
      },
      "source": [
        "icarl.rep_distance # weighted by importance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {0: tensor(0.1515, device='cuda:0'),\n",
              "  1: tensor(0.0844, device='cuda:0'),\n",
              "  2: tensor(0.1231, device='cuda:0'),\n",
              "  3: tensor(0.1016, device='cuda:0'),\n",
              "  4: tensor(0.1191, device='cuda:0'),\n",
              "  5: tensor(0.2006, device='cuda:0'),\n",
              "  6: tensor(0.1563, device='cuda:0'),\n",
              "  7: tensor(0.1776, device='cuda:0'),\n",
              "  8: tensor(0.0996, device='cuda:0'),\n",
              "  9: tensor(0.2898, device='cuda:0')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X97EdXtizewi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "076217c1-115f-4b80-a43e-bf92b3a27de4"
      },
      "source": [
        "len(torch.tensor([1, 2,4]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I2grMIIbdAE",
        "colab_type": "text"
      },
      "source": [
        "### Analisys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuDM6Ydokv05",
        "colab_type": "text"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3LN94fQfozr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_analisys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHhsZ71cghfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_analisys_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEn9P6usbwYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = [[logs_icarl[run_i][i]['train_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "train_accuracy = [[logs_icarl[run_i][i]['train_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_loss = [[logs_icarl[run_i][i]['val_loss'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "val_accuracy = [[logs_icarl[run_i][i]['val_accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "test_accuracy = [[logs_icarl[run_i][i]['accuracy'] for i in range(10)] for run_i in range(NUM_RUNS)]\n",
        "\n",
        "train_loss = np.array(train_loss)\n",
        "train_accuracy = np.array(train_accuracy)\n",
        "val_loss = np.array(val_loss)\n",
        "val_accuracy = np.array(val_accuracy)\n",
        "test_accuracy = np.array(test_accuracy)\n",
        "\n",
        "train_loss_stats = np.array([train_loss.mean(0), train_loss.std(0)]).transpose()\n",
        "train_accuracy_stats = np.array([train_accuracy.mean(0), train_accuracy.std(0)]).transpose()\n",
        "val_loss_stats = np.array([val_loss.mean(0), val_loss.std(0)]).transpose()\n",
        "val_accuracy_stats = np.array([val_accuracy.mean(0), val_accuracy.std(0)]).transpose()\n",
        "test_accuracy_stats = np.array([test_accuracy.mean(0), test_accuracy.std(0)]).transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GmiU4BfhVht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.train_val_scores(train_loss_stats, train_accuracy_stats, val_loss_stats, val_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAWPfif3j7pK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot.test_scores(test_accuracy_stats)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}