\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{lipsum}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

\title{Incremental Learning in Image Classification}

\author{Manuele Macchia\\
% Institution1\\
% Institution1 address\\
{\tt\small s277309@studenti.polito.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Francesco Montagna\\
{\tt\small s277596@studenti.polito.it}
\and
Giacomo Zema\\
{\tt\small s269614@studenti.polito.it}
}

\maketitle
%\thispagestyle{empty}

\begin{abstract}
    Extending the knowledge of a model is an open problem in deep learning. A central issue in incremental learning is catastrophic forgetting, resulting in degradation of previous knowledge when gradually learning new information.

    The scope of the described implementation is to reproduce some existing baselines that address the difficulties posed by incremental learning, to propose variations to the existing frameworks in order to gain a deeper knowledge of their components and in-depth insights, and finally to define new approaches to overcome existing limitations.
\end{abstract}

\section{Introduction}
Incremental learning is a paradigm that allows extending the knowledge of an existing model, gradually incorporating new information, as opposed to training the model on a complete dataset. Training a system on a dataset that includes all classes is unfeasible in many real-world scenarios, \eg, where data comes in a stream or where previously collected data is not available anymore. Therefore, the possibility of incrementally learning new data while maintaining existing knowledge has great importance for training more flexible and dynamic systems.

A central issue in incremental learning is \emph{catastrophic forgetting} or \emph{catastrophic interference} \cite{parisi:2019}, \ie, training a model with new data interferes with previously acquired knowledge. This leads to a performance decrease or, in the worst case, to the old knowledge being overwritten.

Considerable research has been devoted to limiting the effects of catastrophic forgetting. We mainly consider two incremental learning baselines in computer vision, Learning without Forgetting \cite{li:2016} and iCaRL \cite{rebuffi:2017}. These methods mitigate catastrophic forgetting by adopting techniques such as knowledge distillation and prototype rehearsal, which we further explore in our study.

In this work, we implement the existing incremental learning baselines of Learning without Forgetting and iCaRL to gain a deeper knowledge of the problem and the proposed techniques. We explore the effect of each component of the baselines on the performance of the model, and study the effectiveness of alternative components, such as different loss functions and classifiers. Finally, we propose novel approaches to overcome existing limitations.

%%%%%%%%%%%%%%%%
\section{Method}
%%%%%%%%%%%%%%%%
In this section we describe the implementation details of our work, the underlying convolutional neural network and the dataset partitioning for incremental learning.

\subsection{Network}
We carry out all experiments using a 32-layers ResNet \cite{he:2016}, a deep convolutional neural network. Residual networks address the vanishing gradient problem by allowing shortcut connections between layers, allowing for deeper architectures. From now on, we treat the  network up to the second to last layer as a feature extractor $\varphi: \chi \to \mathbb{R}^{d}$ and the last fully connected layer as a classifier. This choice is in line with the implementation of iCaRL.

In our incremental setting, we initialize the last fully connected layer to ten output nodes, and gradually increment this number as the network learns new classes. This approach is coherent with the assumption that the total number of classes is not known a priori. We use the Xavier normal initializer \cite{glorot:2010} to initialize the network weights. In our experiments, we find that this approach leads to better accuracy.

\subsection{Data}
We execute our experiments on the CIFAR-100 dataset \cite{krizhevsky:2009}. It is a popular dataset for incremental learning, and both our baselines \cite{li:2016, rebuffi:2017} use it to test their strategies. It comprises 60000 tiny images belonging to 100 classes.

CIFAR-100 provides 500 training images and 100 testing images per class. We hold out 50 images per class from the training set to form a validation set. We use this data to validate our models in some of our studies and to monitor the performance of the network during training.

We artificially increase the cardinality of the dataset through data augmentation. This technique may help in improving the results and avoiding overfitting. We apply the same data preprocessing that the iCaRL authors use in the official code\footnote{\url{https://github.com/srebuffi/iCaRL}}, which consists in applying a random horizontal flip with 50\% probability and a $32$ by $32$ random crop with $4$ pixels of padding to the training set. These transformations are actually effective, increasing the accuracy of the model on the first incremental step by 10\%.

All three RGB channels are normalized with mean $0.5$ and standard deviation $0.5$. Using the true mean and standard deviation of the dataset would violate the incremental learning assumptions.

\subsection{Incremental approach}
Our incremental approach is consistent with the benchmark protocol of iCaRL. The dataset is split into ten batches of ten classes each, with classes randomly assigned to a batch. At each learning step, the network is trained on a batch of data and evaluated on a test set that contains all known classes up to that point. To obtain statistically significant results, we fix three class orders and run all experiments three times, as different class splits lead to very different accuracy scores. We present the results in terms of mean and standard deviation of classification accuracies after each batch of classes, using an error bar plot or reporting the average of the mean accuracies, called \emph{average incremental accuracy}.

This approach does not eliminate all sources of randomness, which are still present in the initialization of the network weights. We do not explore this aspect due to time and computational resources limitations.

\subsection{Settings}
We train the model using stochastic gradient descent with momentum as optimization algorithm, with mini-batch size set to $64$ and weight decay equal to $0.00001$. The initial learning rate is set to $2.0$ and is divided by $5$ after $49$ and $63$ epochs. For prototype rehearsal, we store a maximum of $K=2000$ exemplars. These are the same hyper-parameters used by the authors of iCaRL. The only exception is the size of the mini-batches, which we lower to $64$ to obtain results comparable to the ones reported in their study.

This is the default hyper-parameter set-up. All further variations that we apply to these settings are reported in the description of the experiments.

\subsection{Code}
For our experiments we rely on the \emph{PyTorch} framework. Our source code is publicly available at
\url{https://github.com/manuelemacchia/incremental-learning-image-classification}.

%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%
\subsection{Joint Training}

\subsection{Finetuning}

\subsection{Learning without Forgetting}

\subsection{iCaRL}

{\small
\bibliographystyle{ieee}
\bibliography{bibliography}
}

\end{document}
