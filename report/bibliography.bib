
@article{parisi:2019,
	title = {Continual lifelong learning with neural networks: {A} review},
	volume = {113},
	issn = {08936080},
	shorttitle = {Continual lifelong learning with neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608019300231},
	doi = {10.1016/j.neunet.2019.01.012},
	language = {en},
	urldate = {2020-06-26},
	journal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	month = may,
	year = {2019},
	pages = {54--71},
	file = {Full Text:C\:\\Users\\macch\\Zotero\\storage\\RVSNCG8F\\Parisi et al. - 2019 - Continual lifelong learning with neural networks .pdf:application/pdf}
}

@article{li:2016,
	title = {Learning without {Forgetting}},
	journal = {European Conference on Computer Vision (ECCV)},
	author = {Li, Zhizhong and Hoiem, Derek},
	year = {2016},
	file = {Submitted Version:C\:\\Users\\macch\\Zotero\\storage\\ZUYJSERU\\Li e Hoiem - 2018 - Learning without Forgetting.pdf:application/pdf}
}

@inproceedings{rebuffi:2017,
	address = {Honolulu, HI},
	title = {{iCaRL}: {Incremental} {Classifier} and {Representation} {Learning}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {{iCaRL}},
	url = {http://ieeexplore.ieee.org/document/8100070/},
	doi = {10.1109/CVPR.2017.587},
	urldate = {2020-06-26},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
	month = jul,
	year = {2017},
	pages = {5533--5542}
}

@inproceedings{he:2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2020-06-26},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {Submitted Version:C\:\\Users\\macch\\Zotero\\storage\\IQIQER78\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@inproceedings{glorot:2010,
	title = {Understanding the difﬁculty of training deep feedforward neural networks},
	abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
	language = {en},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = {2010},
	pages = {249--256},
	file = {Glorot e Bengio - Understanding the difﬁculty of training deep feedf.pdf:C\:\\Users\\macch\\Zotero\\storage\\DLTU7YJN\\Glorot e Bengio - Understanding the difﬁculty of training deep feedf.pdf:application/pdf}
}

@article{krizhevsky:2009,
	title = {Learning {Multiple} {Layers} of {Features} from {Tiny} {Images}},
	language = {en},
	author = {Krizhevsky, Alex},
	year = {2009},
	pages = {60},
	file = {Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:C\:\\Users\\macch\\Zotero\\storage\\NE8SE7KG\\Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf:application/pdf}
}

@article{hinton:2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2020-06-29},
	journal = {arXiv:1503.02531},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02531},
	keywords = {Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\macch\\Zotero\\storage\\FWVUXVFQ\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\macch\\Zotero\\storage\\M2Z7UWAR\\1503.html:text/html}
}

@article{pedregosa:2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simpliﬁed BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	number = {85},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	year = {2011},
	pages = {2825--2830},
	file = {Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:C\:\\Users\\macch\\Zotero\\storage\\J333RH62\\Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:application/pdf}
}

@article{breiman:2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {08856125},
	doi = {10.1023/A:1010933404324},
	number = {1},
	urldate = {2020-06-30},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	pages = {5--32},
	file = {Full Text:C\:\\Users\\macch\\Zotero\\storage\\NN2VYJ6F\\Breiman - 2001 - [No title found].pdf:application/pdf}
}

@inproceedings{hou:2019,
	address = {Long Beach, CA, USA},
	title = {Learning a {Unified} {Classifier} {Incrementally} via {Rebalancing}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953661/},
	doi = {10.1109/CVPR.2019.00092},
	urldate = {2020-06-30},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
	month = jun,
	year = {2019},
	pages = {831--839}
}